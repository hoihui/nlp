{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://radimrehurek.com/gensim/tutorial.html\">Tutorials</a><br/>\n",
    "<a href=\"https://radimrehurek.com/gensim/apiref.html\">API</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.4.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install gensim\n",
    "import gensim\n",
    "gensim.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'sentence', 'right']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "gensim.utils.simple_preprocess('This is a sentence, right?', deacc=True)# deacc=True removes punctuations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary[0]: computer\n",
      "token2id: {'computer': 0, 'human': 1, 'interface': 2, 'response': 3, 'survey': 4, 'system': 5, 'time': 6, 'user': 7, 'eps': 8, 'trees': 9, 'graph': 10, 'minors': 11}\n",
      "doc2bow: [(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(common_texts) #inputs iterator (list, file object, ...) of words\n",
    "print('dictionary[0]:', dictionary[0]) #word mapped to just id\n",
    "print('token2id:', dictionary.token2id) #word mapped to just id\n",
    "print('doc2bow:', dictionary.doc2bow(common_texts[1])) #tuple (id,freq), ignoring those with 0 freq\n",
    "# dictionary.save(pathtofile)\n",
    "# dictionary=corpora.Dictionary.load(pathtofile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corpus = list (iterator) of bow (list of tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1)], [(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)], [(2, 1), (5, 1), (7, 1), (8, 1)], [(1, 1), (5, 2), (8, 1)], [(3, 1), (6, 1), (7, 1)], [(9, 1)], [(9, 1), (10, 1)], [(9, 1), (10, 1), (11, 1)], [(4, 1), (10, 1), (11, 1)]]\n"
     ]
    }
   ],
   "source": [
    "corpus=[dictionary.doc2bow(linelist) for linelist in common_texts]\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tfidf (term frequencyâ€“inverse document frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "applied to one doc: [(0, 0.5773502691896257), (1, 0.5773502691896257), (2, 0.5773502691896257)]\n",
      "applied to whole: [(0, 0.5773502691896257), (1, 0.5773502691896257), (2, 0.5773502691896257)]\n",
      "[(0, 0.44424552527467476), (3, 0.44424552527467476), (4, 0.44424552527467476), (5, 0.3244870206138555), (6, 0.44424552527467476), (7, 0.3244870206138555)]\n",
      "[(2, 0.5710059809418182), (5, 0.4170757362022777), (7, 0.4170757362022777), (8, 0.5710059809418182)]\n",
      "[(1, 0.49182558987264147), (5, 0.7184811607083769), (8, 0.49182558987264147)]\n",
      "[(3, 0.6282580468670046), (6, 0.6282580468670046), (7, 0.45889394536615247)]\n",
      "[(9, 1.0)]\n",
      "[(9, 0.7071067811865475), (10, 0.7071067811865475)]\n",
      "[(9, 0.5080429008916749), (10, 0.5080429008916749), (11, 0.695546419520037)]\n",
      "[(4, 0.6282580468670046), (10, 0.45889394536615247), (11, 0.6282580468670046)]\n"
     ]
    }
   ],
   "source": [
    "tfidf = models.TfidfModel(corpus) #transforms from bow (id,freq) to (id,tf x idf)  -- has no params\n",
    "print('applied to one doc:', tfidf[corpus[0]])\n",
    "print('applied to whole:', '\\n'.join(map(str,tfidf[corpus])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a href=\"https://radimrehurek.com/gensim/models/lsimodel.html#module-gensim.models.lsimodel\">lsi (Latent Semantic Indexing)</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.703*\"trees\" + 0.538*\"graph\" + 0.402*\"minors\" + 0.187*\"survey\" + 0.061*\"system\" + 0.060*\"time\" + 0.060*\"response\" + 0.058*\"user\" + 0.049*\"computer\" + 0.035*\"interface\"'),\n",
       " (1,\n",
       "  '-0.460*\"system\" + -0.373*\"user\" + -0.332*\"eps\" + -0.328*\"interface\" + -0.320*\"time\" + -0.320*\"response\" + -0.293*\"computer\" + -0.280*\"human\" + -0.171*\"survey\" + 0.161*\"trees\"')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi = models.LsiModel(tfidf[corpus], id2word=dictionary, num_topics=2) #inputs either bow or tfidf model\n",
    "lsi.print_topics(2) #print words 'belonging' to each topic\n",
    "# lsi.save(pathtofile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a href=\"https://radimrehurek.com/gensim/models/ldamodel.html\">lda (Latent Dirichlet Allocation)</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.8963071), (1, 0.048678253), (2, 0.055014614)]\n",
      "0 [(0, 0.82634884), (1, 0.08513247), (2, 0.08851871)]\n",
      "1 [(0, 0.89627016), (1, 0.048678327), (2, 0.05505152)]\n",
      "2 [(0, 0.8572984), (1, 0.06803836), (2, 0.07466328)]\n",
      "3 [(0, 0.7829753), (1, 0.06865222), (2, 0.14837246)]\n",
      "4 [(0, 0.81499547), (1, 0.08530275), (2, 0.09970176)]\n",
      "5 [(0, 0.16784121), (1, 0.65781194), (2, 0.17434685)]\n",
      "6 [(0, 0.11177107), (1, 0.7646773), (2, 0.123551615)]\n",
      "7 [(0, 0.084061265), (1, 0.37287787), (2, 0.5430609)]\n",
      "8 [(0, 0.08876538), (1, 0.09079955), (2, 0.8204351)]\n"
     ]
    }
   ],
   "source": [
    "lda=models.LdaModel(corpus,id2word=dictionary,num_topics=3) #inputs bow\n",
    "print(lda[corpus[1]]) #distribution of topics\n",
    "for i,l in enumerate(lda[corpus]): #distribution of topics for all doc\n",
    "    print(i,l)\n",
    "lda.print_topics()\n",
    "lda.update(corpus) #update (online training) with more data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.185*\"user\" + 0.184*\"response\" + 0.184*\"time\" + 0.116*\"system\" + 0.101*\"survey\" + 0.100*\"computer\" + 0.032*\"trees\" + 0.023*\"eps\" + 0.023*\"human\" + 0.018*\"graph\"'),\n",
       " (1,\n",
       "  '0.284*\"graph\" + 0.268*\"trees\" + 0.195*\"minors\" + 0.106*\"survey\" + 0.019*\"system\" + 0.019*\"user\" + 0.018*\"eps\" + 0.018*\"human\" + 0.018*\"interface\" + 0.018*\"response\"'),\n",
       " (2,\n",
       "  '0.230*\"system\" + 0.167*\"interface\" + 0.162*\"human\" + 0.162*\"eps\" + 0.092*\"computer\" + 0.091*\"user\" + 0.017*\"trees\" + 0.016*\"time\" + 0.016*\"graph\" + 0.016*\"survey\"')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a href=\"https://radimrehurek.com/gensim/models/hdpmodel.html\">hdp (Hierarchical Dirichlet Process)</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.426*minors + 0.130*survey + 0.115*human + 0.070*interface + 0.063*computer + 0.052*time + 0.031*trees + 0.027*graph + 0.026*system + 0.025*user'),\n",
       " (1,\n",
       "  '0.292*system + 0.116*user + 0.115*minors + 0.099*graph + 0.098*eps + 0.081*time + 0.077*response + 0.060*computer + 0.028*human + 0.021*survey'),\n",
       " (2,\n",
       "  '0.383*eps + 0.215*interface + 0.061*human + 0.058*survey + 0.054*user + 0.040*trees + 0.036*response + 0.036*minors + 0.036*graph + 0.029*time')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdp = models.HdpModel(corpus, id2word=dictionary)\n",
    "hdp.print_topics(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: with a courpus and a query string, find the simiarities of each doc w.r.t. the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.4618210045327153), (1, -0.0700276652789999)]\n",
      "[ 0.998093    0.93748635  0.9984453   0.9865886   0.90755945 -0.12416792\n",
      " -0.10639259 -0.09879464  0.05004176]\n"
     ]
    }
   ],
   "source": [
    "from gensim import models\n",
    "from gensim import similarities\n",
    "\n",
    "#build lsi space from the corpus\n",
    "lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=2)\n",
    "\n",
    "#embed query in lsi space\n",
    "query = ['human','computer','interaction']\n",
    "query_vec = lsi[dictionary.doc2bow(query)]  \n",
    "print(query_vec)\n",
    "\n",
    "# transform corpus to LSI space and index it\n",
    "index = similarities.MatrixSimilarity(lsi[corpus]) #or use similarities.Similarity class for large corpus\n",
    "# index.save()\n",
    "# index=similarities.MatrixSimilarity.load()\n",
    "\n",
    "# perform a similarity query against the corpus\n",
    "sims = index[query_vec]\n",
    "print(sims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a href=\"https://radimrehurek.com/gensim/models/phrases.html\">Phrase Modelling</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cookie', 'Manager', ':', '\"', 'Don', \"'\", 't', 'allow', 'sites', 'that', 'set', 'removed', 'cookies', 'to', 'set', 'future', 'cookies', '\"', 'should', 'stay', 'checked', 'When', 'in', 'full', 'screen', 'mode', 'Pressing', 'Ctrl', '-', 'N', 'should', 'open', 'a', 'new', 'browser', 'when', 'only', 'download', 'dialog', 'is', 'left', 'open', 'add', 'icons', 'to', 'context', 'menu', 'So', 'called', '\"', 'tab', 'bar', '\"', 'should', 'be', 'made', 'a', 'proper', 'toolbar', 'or', 'given', 'the', 'ability', 'collapse', '/', 'expand', '.']\n",
      "['Cookie_Manager', ':', '\"', 'Don', \"'\", 't', 'allow', 'sites', 'that', 'set', 'removed', 'cookies', 'to', 'set', 'future', 'cookies', '\"', 'should', 'stay', 'checked', 'When', 'in', 'full_screen', 'mode', 'Pressing', 'Ctrl', '-', 'N', 'should', 'open', 'a', 'new', 'browser', 'when', 'only', 'download', 'dialog', 'is', 'left', 'open', 'add', 'icons', 'to', 'context_menu', 'So', 'called', '\"', 'tab', 'bar', '\"', 'should', 'be', 'made', 'a', 'proper', 'toolbar', 'or', 'given', 'the', 'ability', 'collapse', '/', 'expand', '.']\n",
      "['Cookie_Manager', ':', '\"', 'Don', \"'\", 't', 'allow', 'sites', 'that', 'set', 'removed', 'cookies', 'to', 'set', 'future', 'cookies', '\"', 'should', 'stay', 'checked', 'When', 'in', 'full_screen_mode', 'Pressing', 'Ctrl', '-', 'N', 'should', 'open', 'a', 'new', 'browser', 'when', 'only', 'download', 'dialog', 'is', 'left', 'open', 'add', 'icons', 'to', 'context_menu', 'So', 'called', '\"', 'tab', 'bar', '\"', 'should', 'be', 'made', 'a', 'proper', 'toolbar', 'or', 'given', 'the', 'ability', 'collapse', '/', 'expand', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk,gensim\n",
    "\n",
    "sentences = sum([list(nltk.corpus.webtext.sents(fileid)) for fileid in nltk.corpus.webtext.fileids()],[])\n",
    "\n",
    "bigram = gensim.models.Phrases(sentences, min_count=5, threshold=100)  # bigram model, higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[sentences], min_count=5, threshold=100)  # trigram model\n",
    "   \n",
    "bigram = gensim.models.phrases.Phraser(bigram)\n",
    "trigram = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "print(sentences[0])\n",
    "print(bigram[sentences[0]])\n",
    "print(trigram[bigram[sentences[0]]])\n",
    "# bigram[sentences] # can also apply to whole corpus (list of list)\n",
    "\n",
    "# phrases.add_vocab([[\"hello\", \"world\"], [\"meow\"]])  # update model with new sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import brown\n",
    "model = Word2Vec(brown.sents(),size=10) #training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.882792481859794\n",
      "15173\n",
      "[('membership', 0.9912745952606201), ('combination', 0.987086296081543), ('treatment', 0.9851009845733643)]\n",
      "dinner\n"
     ]
    }
   ],
   "source": [
    "print(model.wv['human'])  #get vector representation\n",
    "print(model.wv.similarity('university','school'))\n",
    "print(len(model.wv.vocab)) #vocab size\n",
    "print(model.wv.most_similar(positive=['university'], topn = 3)) #'add' university\n",
    "print(model.wv.doesnt_match('breakfast cereal dinner lunch'.split())) #Finding a word that is not in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Steel', 0.9899702072143555)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['woman','king'], negative=['man'], topn = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pretrained by nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from nltk.data import find\n",
    "word2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))\n",
    "modelnltk = gensim.models.KeyedVectors.load_word2vec_format(word2vec_sample, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5080746061254015\n",
      "43981\n",
      "[('universities', 0.7003918886184692), ('faculty', 0.6780906915664673), ('undergraduate', 0.6587096452713013)]\n",
      "cereal\n"
     ]
    }
   ],
   "source": [
    "# print(model.wv['human'])  #get vector representation\n",
    "print(modelnltk.similarity('university','school'))\n",
    "print(len(modelnltk.vocab)) #vocab size\n",
    "print(modelnltk.most_similar(positive=['university'], topn = 3))\n",
    "print(modelnltk.doesnt_match('breakfast cereal dinner lunch'.split())) #Finding a word that is not in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Japan', 0.7969787120819092)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelnltk.most_similar(positive=['woman','king'], negative=['man'], topn = 1) # king-man+woman\n",
    "modelnltk.most_similar(positive=['Tokyo','Germany'], negative=['Berlin'], topn = 1) # Germany-Berlin+Tokyo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pretrained by Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM\n",
    "# https://github.com/mmihaltz/word2vec-GoogleNews-vectors\n",
    "# http://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/\n",
    "modelgoog=gensim.models.KeyedVectors.load_word2vec_format('~/GoogleNews-vectors-negative300.bin', binary=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('China', 0.7755854725837708)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelgoog.most_similar(positive=['Beijing','Germany'], negative=['Berlin'], topn = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pretrained GloVe by Stanford"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request,zipfile\n",
    "import os,tempfile\n",
    "os.chdir(tempfile.gettempdir())\n",
    "urllib.request.urlretrieve('http://nlp.stanford.edu/data/glove.6B.zip','glove.6B.zip')\n",
    "zipfile.ZipFile('glove.6B.zip').extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim.scripts.glove2word2vec.glove2word2vec('glove.6B.100d.txt', 'glove2vec')\n",
    "glove=gensim.models.KeyedVectors.load_word2vec_format('glove2vec', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.7698541283607483)]\n",
      "[('japan', 0.8432861566543579)]\n"
     ]
    }
   ],
   "source": [
    "print(glove.most_similar(positive=['woman','king'], negative=['man'], topn = 1)) # king-man+woman\n",
    "print(glove.most_similar(positive=['tokyo','germany'], negative=['berlin'], topn = 1)) # Germany-Berlin+Tokyo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## self-trained Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/develop-word-embeddings-python-gensim/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=14, size=10, alpha=0.025)\n",
      "['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec', 'second', 'yet', 'another', 'one', 'more', 'and', 'final']\n",
      "[ 0.02409168 -0.02793393  0.03854181 -0.01606321  0.0270295   0.0039328\n",
      "  0.01337573 -0.04209328 -0.02356309  0.01950325]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "sentences = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n",
    "             ['this', 'is', 'the', 'second', 'sentence'],\n",
    "             ['yet', 'another', 'sentence'],\n",
    "             ['one', 'more', 'sentence'],\n",
    "             ['and', 'the', 'final', 'sentence']]\n",
    "\n",
    "model = Word2Vec(sentences, size=10, window=5, min_count=1, workers=4)# train\n",
    "print(model)# summarize\n",
    "\n",
    "print(list(model.wv.vocab)) # summarize vocabulary\n",
    "print(model.wv['sentence'])   # access vector for one word\n",
    "\n",
    "# model.save('model.bin')# save model\n",
    "# new_model = Word2Vec.load('model.bin')# load model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://radimrehurek.com/gensim/models/ldamodel.html\n",
    "\n",
    "https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Internal LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, gensim\n",
    "stopwords=nltk.corpus.stopwords.words('english')+['from', 'subject', 're', 'edu', 'use']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "corp = [nltk.corpus.webtext.raw(fileid) for fileid in nltk.corpus.webtext.fileids()] #list of strings (docs)\n",
    "corp = [gensim.utils.simple_preprocess(doc,deacc=True) for doc in corp] #list of list of words\n",
    "corp = [[w for w in d if w not in stopwords] for d in corp] #remove stopwords\n",
    "vocab = gensim.corpora.dictionary.Dictionary(corp) #Dictionary(~set) of unique words\n",
    "corpbow = [vocab.doc2bow(line) for line in corp] # list of list of tuples (wordid, number of occurences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "lda = gensim.models.LdaModel(corpbow, num_topics=10, id2word=vocab) #id2word necessary for printing topics in terms of words\n",
    "# lda.save(fn)\n",
    "# lda = gensim.models.LdaModel.load(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load unseen corpus \n",
    "corp2 = [nltk.corpus.nps_chat.raw(fileid) for fileid in nltk.corpus.nps_chat.fileids()]\n",
    "corp2 = [gensim.utils.simple_preprocess(doc,deacc=True) for doc in corp2]\n",
    "corp2 = [[w for w in d if w not in stopwords] for d in corp2]\n",
    "corp2bow = [vocab.doc2bow(line) for line in corp2]\n",
    "#incremental training\n",
    "lda.update(corp2bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-9.329627165212187\n",
      "0.4024744707366629\n"
     ]
    }
   ],
   "source": [
    "print(lda.log_perplexity(corpbow)) #lower better\n",
    "print(gensim.models.CoherenceModel(model=lda, texts=corp, dictionary=vocab, coherence='c_v').get_coherence()) #higher better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\uparrow$ Choose num_topics as the \"elbow\" in a plot of Coherence vs. num_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.011*\"word\" + 0.010*\"girl\" + 0.006*\"guy\" + 0.005*\"like\" + 0.004*\"post\" + 0.003*\"get\" + 0.003*\"yeah\" + 0.003*\"man\" + 0.003*\"woman\" + 0.003*\"know\"'), (1, '0.012*\"girl\" + 0.010*\"guy\" + 0.009*\"word\" + 0.005*\"like\" + 0.004*\"woman\" + 0.004*\"know\" + 0.004*\"post\" + 0.004*\"get\" + 0.003*\"yeah\" + 0.003*\"class\"'), (2, '0.284*\"word\" + 0.086*\"class\" + 0.058*\"post\" + 0.038*\"uh\" + 0.031*\"lol\" + 0.026*\"user\" + 0.021*\"emotion\" + 0.015*\"hi\" + 0.010*\"join\" + 0.010*\"part\"'), (3, '0.015*\"word\" + 0.008*\"girl\" + 0.004*\"like\" + 0.004*\"firefox\" + 0.004*\"guy\" + 0.003*\"post\" + 0.003*\"page\" + 0.003*\"class\" + 0.003*\"get\" + 0.003*\"bookmarks\"'), (4, '0.390*\"word\" + 0.081*\"class\" + 0.081*\"post\" + 0.047*\"uh\" + 0.035*\"user\" + 0.027*\"statement\" + 0.017*\"join\" + 0.014*\"system\" + 0.009*\"part\" + 0.008*\"hi\"'), (5, '0.009*\"girl\" + 0.006*\"guy\" + 0.006*\"page\" + 0.005*\"firefox\" + 0.004*\"window\" + 0.004*\"new\" + 0.004*\"bookmarks\" + 0.004*\"tab\" + 0.004*\"browser\" + 0.004*\"like\"'), (6, '0.023*\"word\" + 0.015*\"guy\" + 0.010*\"girl\" + 0.007*\"post\" + 0.007*\"class\" + 0.006*\"like\" + 0.004*\"man\" + 0.004*\"user\" + 0.004*\"yeah\" + 0.004*\"know\"'), (7, '0.245*\"word\" + 0.241*\"post\" + 0.126*\"user\" + 0.050*\"class\" + 0.026*\"system\" + 0.025*\"statement\" + 0.023*\"uh\" + 0.019*\"part\" + 0.014*\"join\" + 0.011*\"cc\"'), (8, '0.009*\"girl\" + 0.007*\"guy\" + 0.006*\"like\" + 0.003*\"know\" + 0.003*\"yeah\" + 0.003*\"man\" + 0.003*\"good\" + 0.003*\"get\" + 0.003*\"woman\" + 0.003*\"firefox\"'), (9, '0.051*\"word\" + 0.027*\"post\" + 0.012*\"user\" + 0.010*\"statement\" + 0.008*\"class\" + 0.004*\"like\" + 0.004*\"page\" + 0.003*\"guy\" + 0.003*\"girl\" + 0.003*\"good\"')]\n",
      "[(0, [('word', 0.0108956965), ('girl', 0.010255615), ('guy', 0.00581051), ('like', 0.004733584), ('post', 0.0038493802), ('get', 0.0034301255), ('yeah', 0.00331979), ('man', 0.0031848091), ('woman', 0.0031316835), ('know', 0.003037981)]), (1, [('girl', 0.012006573), ('guy', 0.009910867), ('word', 0.008603155), ('like', 0.0047959094), ('woman', 0.0043174787), ('know', 0.0038251188), ('post', 0.0036529603), ('get', 0.0035377152), ('yeah', 0.0029884134), ('class', 0.0026693011)]), (2, [('word', 0.28363082), ('class', 0.08586911), ('post', 0.057808105), ('uh', 0.037968367), ('lol', 0.030562587), ('user', 0.0261079), ('emotion', 0.02052284), ('hi', 0.015403209), ('join', 0.010129652), ('part', 0.010060182)]), (3, [('word', 0.01467951), ('girl', 0.00835293), ('like', 0.0040852935), ('firefox', 0.0039390763), ('guy', 0.0036032577), ('post', 0.0032370405), ('page', 0.0031103487), ('class', 0.0028926544), ('get', 0.0028734358), ('bookmarks', 0.002744846)]), (4, [('word', 0.38961187), ('class', 0.08110198), ('post', 0.08066746), ('uh', 0.047479372), ('user', 0.034903176), ('statement', 0.027350847), ('join', 0.016903305), ('system', 0.013642135), ('part', 0.008761118), ('hi', 0.0078590885)]), (5, [('girl', 0.009070868), ('guy', 0.0063266973), ('page', 0.005908715), ('firefox', 0.005027187), ('window', 0.0041708774), ('new', 0.0040426566), ('bookmarks', 0.0039059327), ('tab', 0.0038612264), ('browser', 0.003649393), ('like', 0.0035539162)]), (6, [('word', 0.022607928), ('guy', 0.014955575), ('girl', 0.009567888), ('post', 0.0074057407), ('class', 0.007387629), ('like', 0.0062375427), ('man', 0.0042389333), ('user', 0.0041480893), ('yeah', 0.00411861), ('know', 0.0038948625)]), (7, [('word', 0.24468389), ('post', 0.24090742), ('user', 0.12636755), ('class', 0.05029802), ('system', 0.026109468), ('statement', 0.025283713), ('uh', 0.022666551), ('part', 0.019332994), ('join', 0.0137779815), ('cc', 0.011263275)]), (8, [('girl', 0.008828083), ('guy', 0.007490785), ('like', 0.0060810735), ('know', 0.0031703862), ('yeah', 0.0031422575), ('man', 0.0030952152), ('good', 0.0029815328), ('get', 0.0029124499), ('woman', 0.0026817452), ('firefox', 0.002667044)]), (9, [('word', 0.050642803), ('post', 0.026842229), ('user', 0.012405186), ('statement', 0.01034415), ('class', 0.0080653895), ('like', 0.0039764717), ('page', 0.0039735353), ('guy', 0.0033878908), ('girl', 0.0033018312), ('good', 0.0032536113)])]\n",
      "[(0, 0.72209114), (2, 0.039314125), (3, 0.19951497), (4, 0.03834692)]\n"
     ]
    }
   ],
   "source": [
    "print(lda.print_topics()) # word distribution of a topic\n",
    "print(lda.show_topics(formatted=False)) # same as above as list of tuples\n",
    "print(lda[corpbow[3]]) # find topic distribution of a doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## umass Mallet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/private/var/folders/r7/n9dny1wj46q8njz2gds66kqr0000gp/T\n"
     ]
    }
   ],
   "source": [
    "import urllib.request,zipfile\n",
    "import os,tempfile\n",
    "os.chdir(tempfile.gettempdir())\n",
    "urllib.request.urlretrieve('http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip','mallet.zip')\n",
    "zipfile.ZipFile('mallet.zip').extractall()\n",
    "path=tempfile.gettempdir()+'/mallet-2.0.8/bin/mallet'\n",
    "os.chmod(path,0777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ldamallet = gensim.models.wrappers.LdaMallet(path, corpus=corpbow, num_topics=10, id2word=vocab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "177.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
