{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "R1g44JvFNDv6"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hoihui/pkgs/blob/master/RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9Lv9DZt6NDwA",
        "colab": {}
      },
      "source": [
        "# !pip3 install -U torch torchvision\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hHIsOZBjNDwL"
      },
      "source": [
        "# Sine Curve"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2y5M5iiDNDwM"
      },
      "source": [
        "## Torch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "D14tST-eNDwR",
        "outputId": "c2c6dd45-3698-4cc6-98bd-ae24792faf0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torch\n",
        "cuda=torch.cuda.is_available()\n",
        "torch.__version__, cuda"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('1.1.0', True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JGTqcfbvNDwd"
      },
      "source": [
        "https://github.com/udacity/deep-learning-v2-pytorch/blob/master/recurrent-neural-networks/time-series/Simple_RNN.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "80PpOnlcNDwg"
      },
      "source": [
        "### Generate Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5_FpO7kxNDwh",
        "colab": {}
      },
      "source": [
        "time_steps = np.linspace(0, np.pi, 21)\n",
        "data = np.sin(time_steps).reshape((21,1))  #last dim of 1 is the number of features at each time instance (just a number here)\n",
        "inp, tar = data[:-1], data[1:]\n",
        "\n",
        "# illustration\n",
        "plt.plot(time_steps[1:], inp, 'r.', label='input, x')\n",
        "plt.plot(time_steps[1:], tar, 'b.', label='target, y')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XSJRScA9NDwo"
      },
      "source": [
        "### Constructing 1-layer Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lfyY_fiONDwq"
      },
      "source": [
        "Recall: RNN unit:\n",
        "* input: previous hidden state (could have layers) and current input\n",
        "* output: next hidden state (could have layers) and current output features (to be fed to a dense layer to generate final output)\n",
        "\n",
        "https://pytorch.org/docs/stable/nn.html#rnn\n",
        "* **input_size** - # features at each time instance (=1, a number)\n",
        "* **output_size** - # output dim at each time instance (=1, a number)\n",
        "* **hidden_dim** - # dimension in the hidden state\n",
        "* **n_layers** - # layers making up the RNN, typically 1-3; > 1 means stacked RNN\n",
        "* **batch_first** - whether batch_size as the first dimension `(batch_size, seq_length, hidden_dim)`\n",
        "\n",
        "**nn.RNN is not a single-time instance**.  Fed in full sequence `(batch_size, seq_length, input_size)` with hidden states `(n_layers,batch_size,hidden_dim)`, outputs `(batch_size, seq_length, hidden_dim)` and next hidden state `(n_layers,batch_size,hidden_dim)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oq8q86gqNDwr",
        "colab": {}
      },
      "source": [
        "from torch import nn\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
        "        super(RNN, self).__init__()\n",
        "        \n",
        "        self.hidden_dim=hidden_dim\n",
        "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)\n",
        "        # in.shape =(batch_size, seq_length, input_size),(n_layers,batch_size,hidden_dim)\n",
        "        # out.shape=(batch_size, seq_length, hidden_dim),(n_layers,batch_size,hidden_dim)\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)  # in (batch_size*seq_length, hidden_dim), out (batch_size*seq_length, 1)\n",
        "\n",
        "    def forward(self, x, hidden):   # x.shape = (batch_size, seq_length, input_size)\n",
        "                                    # hidden.shape = (n_layers, batch_size, hidden_dim)\n",
        "                                    # r_out.shape = (batch_size, time_step, hidden_size)\n",
        "        batch_size = x.size(0)\n",
        "        \n",
        "        r_out, hidden = self.rnn(x, hidden)\n",
        "        r_out = r_out.view(-1, self.hidden_dim)   # reshape to pass to fc layer\n",
        "        \n",
        "        output = self.fc(r_out)     # out (batch_size*seq_length, 1)\n",
        "        \n",
        "        return output, hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jnjnDrBFNDwy",
        "outputId": "0f8587cf-2e8a-42a2-a272-47728b216dfd",
        "colab": {}
      },
      "source": [
        "# dimensions test\n",
        "rnn = RNN(input_size=1, output_size=1, hidden_dim=12, n_layers=2)\n",
        "\n",
        "test_input = torch.rand((1,22,1)) # batch_size of 1 as first dimension, feature dim of 1 as last dimension\n",
        "print('Input size: ', test_input.shape)\n",
        "\n",
        "pred, hidden = rnn(x=test_input, hidden=torch.rand((2,1,12)))\n",
        "print('Output size: ', pred.shape)\n",
        "print('Hidden state size: ', hidden.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input size:  torch.Size([1, 22, 1])\n",
            "Output size:  torch.Size([22, 1])\n",
            "Hidden state size:  torch.Size([2, 1, 12])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BlcflwmCNDw8"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b7zXkTtKNDw9",
        "outputId": "0fcb283b-0b5c-43eb-d75b-364e3927421e",
        "colab": {}
      },
      "source": [
        "seq_length=20\n",
        "input_size=1 \n",
        "output_size=1\n",
        "hidden_dim=32\n",
        "n_layers=1\n",
        "n_steps = 90\n",
        "print_every = 30\n",
        "\n",
        "rnn = RNN(input_size, output_size, hidden_dim, n_layers)\n",
        "print(rnn)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.01)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RNN(\n",
            "  (rnn): RNN(1, 32, batch_first=True)\n",
            "  (fc): Linear(in_features=32, out_features=1, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "74fWCfqJNDxK",
        "outputId": "899983d8-e5c9-4c34-8399-0acaa1c0582f",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "hidden = None\n",
        "for step in range(n_steps):\n",
        "    # training data = sine curve (continues from previous)\n",
        "    time_steps = np.linspace(step*np.pi, (step+1)*np.pi, seq_length + 1)\n",
        "    data = np.sin(time_steps)\n",
        "    X = torch.Tensor(data[:-1]).view(1,seq_length,1) # gives batch_size dim of 1\n",
        "    y = torch.Tensor(data[1:]).view(seq_length,1)\n",
        "    \n",
        "    prediction, hidden = rnn(X, hidden) # forward\n",
        "    hidden = hidden.detach()            # don't backpropagate through the entire history\n",
        "\n",
        "    loss = criterion(prediction, y)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if step%print_every == 0:        \n",
        "        print('Loss: ', loss.item())\n",
        "        plt.plot(time_steps[1:], X.numpy().flatten(), 'r.') # input\n",
        "        plt.plot(time_steps[1:], prediction.detach().numpy().flatten(), 'b.') # predictions\n",
        "        plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss:  0.0034269089810550213\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Loss:  4.2462361307116225e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Loss:  3.917669459951867e-07\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pzIJfJZ7NDxX"
      },
      "source": [
        "### Generate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dTVVPTA0NDxY",
        "outputId": "bccf0ef2-c628-4ca1-e4d8-d50a4cd8a2cc",
        "colab": {}
      },
      "source": [
        "y=[]\n",
        "for _ in range(100):\n",
        "    with torch.no_grad():\n",
        "        y+=X.numpy().flatten()[-1],\n",
        "        X=torch.Tensor([y[-1]]).view(1,1,1)  # batchsize, seqlen, feature dim\n",
        "        X, hidden = rnn(X, hidden)\n",
        "plt.plot(range(len(y)),y,'.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x12d8ea828>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hqKxv0cnNDxf"
      },
      "source": [
        "# char-rnn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Sr4bTm67NDxh"
      },
      "source": [
        "## Torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "z2Zw495VNDxj"
      },
      "source": [
        "https://github.com/udacity/deep-learning-v2-pytorch/blob/master/recurrent-neural-networks/char-rnn/Character_Level_RNN_Solution.ipynb\n",
        "\n",
        "https://classroom.udacity.com/courses/ud188/lessons/a8fc0724-37ed-40d9-a226-57175b8bb8cc/concepts/6538eb14-1ec2-4a25-bc73-5942a48b1141"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "H-2yJ3HINDxl",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "cuda=torch.cuda.is_available()\n",
        "torch.__version__, cuda\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "seqlen = 20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2l0WEEWTNDxs"
      },
      "source": [
        "### Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pS4CcFlLNDxt",
        "outputId": "e55f423e-ac79-47d1-f79c-1fbd4ce558aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os,urllib,tempfile\n",
        "os.chdir(tempfile.gettempdir())\n",
        "url='https://github.com/udacity/deep-learning-v2-pytorch/raw/master/recurrent-neural-networks/char-rnn/data/anna.txt'\n",
        "urllib.request.urlretrieve(url,'anna.txt')\n",
        "with open('anna.txt', 'r') as f:\n",
        "    text = f.read()\n",
        "text[:15]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Chapter 1\\n\\n\\nHap'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Gcux4g5-NDx0"
      },
      "source": [
        "#### Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vsFh5tEcNDx1",
        "outputId": "11360506-6bd9-4c95-a75c-3c2855921b3c",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Construct int mapping; will be built into Network Module\n",
        "chars=set(text)\n",
        "int2char = dict(enumerate(chars))\n",
        "char2int = {v: k for k,v in int2char.items()}\n",
        "encoded = np.array([char2int[ch] for ch in text])\n",
        "nchars = len(set(int2char))\n",
        "encoded[:15]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([81, 66, 17,  1, 64, 33, 13, 25, 48, 73, 73, 73,  9, 17,  1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQL8stikLQ-3",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/udacity/deep-learning-v2-pytorch/master/recurrent-neural-networks/char-rnn/assets/sequence_batching.png\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YXUOM5HZNDx-",
        "outputId": "2c98194d-21cf-411d-f778-4aacdad1ba44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "# batch loader: in (N,); out generates (batch_size, seqlen)x2   ##NON-OVERLAPPING\n",
        "def Batch_Loader(arr, batch_size, seqlen):    \n",
        "    batch_size_total = batch_size * seqlen\n",
        "    n_batches = len(arr)//batch_size_total     # number of batches\n",
        "    X_size_total = n_batches * batch_size_total\n",
        "    \n",
        "    if len(arr) > X_size_total:                # special treatment for the last batch's target so it doesn't wrap around; not important\n",
        "        arr_ = arr[:X_size_total].reshape((batch_size, -1))\n",
        "        arr  = np.hstack((arr_,np.vstack((arr_[1:,:1],[arr[X_size_total]]))))\n",
        "    else:\n",
        "        arr = arr[:X_size_total].reshape((batch_size, -1))\n",
        "    \n",
        "    for n in range(0, arr.shape[1], seqlen):\n",
        "        x = arr[:, n:n+seqlen]\n",
        "        y = np.zeros_like(x)\n",
        "        try:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seqlen]\n",
        "        except IndexError:\n",
        "            break\n",
        "#             y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
        "        yield x, y\n",
        "for x,y in Batch_Loader(np.array(range(31)),3,5):\n",
        "    print('x:',x)\n",
        "    print('y:',y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x: [[ 0  1  2  3  4]\n",
            " [10 11 12 13 14]\n",
            " [20 21 22 23 24]]\n",
            "y: [[ 1  2  3  4  5]\n",
            " [11 12 13 14 15]\n",
            " [21 22 23 24 25]]\n",
            "x: [[ 5  6  7  8  9]\n",
            " [15 16 17 18 19]\n",
            " [25 26 27 28 29]]\n",
            "y: [[ 6  7  8  9 10]\n",
            " [16 17 18 19 20]\n",
            " [26 27 28 29 30]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oIJuq30pNDyJ",
        "outputId": "c9aef7d4-4c8b-4747-8ade-4b7813536bda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "# One-Hot Encoding: in (batch_size, seqlen); out (batch_size, seqlen, nchars)\n",
        "def One_Hot_Encode(arr, nchars=nchars):\n",
        "    one_hot = np.zeros((arr.size, nchars), dtype=np.float32)\n",
        "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
        "    one_hot = one_hot.reshape((*arr.shape, nchars))\n",
        "    return one_hot\n",
        "One_Hot_Encode(np.array([[3,5,1],[4,2,7]]), 8)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "        [0., 1., 0., 0., 0., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., 0., 1., 0., 0., 0.],\n",
              "        [0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 1.]]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hCxIp_anNDyS"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3DhWh9VLQ_h",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/udacity/deep-learning-v2-pytorch/master/recurrent-neural-networks/char-rnn/assets/charRNN%400.5x.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Uh6M1zECNDyX"
      },
      "source": [
        "`nn.LSTM(input_size, n_hidden, n_layers, dropout=drop_prob, batch_first=True)`:\n",
        "* input_size: number of characters as sequential input,\n",
        "* n_hidden: number of units in the hidden layers\n",
        "* n_layers: Number of LSTM units in each time instance\n",
        "* dropout: dropout to the inputs or outputs and in-between the layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wCSMfzLQNDyY",
        "colab": {}
      },
      "source": [
        "from torch import nn\n",
        "class CharRNN(nn.Module):    \n",
        "    def __init__(self, tokens, n_hidden=512, n_layers=2, drop_prob=0.5):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        \n",
        "        # int mappings\n",
        "        self.chars = tokens\n",
        "        self.int2char = dict(enumerate(self.chars))\n",
        "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
        "        \n",
        "        # net structure\n",
        "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers,    \n",
        "                            dropout=drop_prob, batch_first=True)   # in: (batch_size, seqlen, nchars), out: (batch_size,seqlen,n_hidden), hidden\n",
        "        self.dropout = nn.Dropout(drop_prob)        \n",
        "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
        "      \n",
        "    \n",
        "    def forward(self, x, hidden):\n",
        "        r_output, hidden = self.lstm(x, hidden)\n",
        "        out = self.dropout(r_output)\n",
        "        \n",
        "        out = out.contiguous().view(-1, self.n_hidden) # contiguous: makes a copy of tensor so the order of elements would be same as if tensor of same shape created from scratch\n",
        "        out = self.fc(out)\n",
        "        \n",
        "        return out, hidden\n",
        "    \n",
        "    \n",
        "    def init_hidden(self, batch_size):  # produce hidden & cell state conforming with the LSTM (n_layers , batch_size , n_hidden), \n",
        "        weight = next(self.parameters()).data   #get the LSTM parameters (for its datatype)\n",
        "        hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device),\n",
        "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))        \n",
        "        return hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ihH89MLWNDyf",
        "outputId": "88a3b9d2-e4a2-4ef7-a62d-a6e751398d59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "net = CharRNN(chars)\n",
        "print(net)\n",
        "opt = torch.optim.Adam(net.parameters(), lr=.001)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CharRNN(\n",
            "  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5)\n",
            "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_fxWJFsgNDyl"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8-qCvBhVNDym",
        "colab": {}
      },
      "source": [
        "batch_size = 128\n",
        "seqlen = 100\n",
        "epochs = 20\n",
        "clip=5          #max gradient allowed\n",
        "val_frac=0.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tuNdMCKGNDyw"
      },
      "source": [
        "* use `clip_grad_norm_` to help prevent exploding gradients\n",
        "* detach hidden state to not backprop the entire history"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0GbbVrkdNDyy",
        "outputId": "1f50193a-cbc6-4ffc-d887-72c9f538ef5a",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "if cuda: net.cuda()\n",
        "\n",
        "# create train/val; each of them must be contiguous\n",
        "val_idx = int(len(encoded)*(1-val_frac))\n",
        "train, val = encoded[:val_idx], encoded[val_idx:]\n",
        "\n",
        "for e in range(epochs):\n",
        "    h = net.init_hidden(batch_size)    # initialize hidden state\n",
        "    \n",
        "    for i, (x, y) in enumerate(Batch_Loader(train, batch_size, seqlen)):\n",
        "        net.train()\n",
        "\n",
        "        # prepare training sample: 1Hot, to torch, to cuda\n",
        "        x = One_Hot_Encode(x)\n",
        "        x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "        if cuda: x, y = x.cuda(), y.cuda()\n",
        "\n",
        "        h = tuple([e.data for e in h]) #detach (to not backprop history)\n",
        "\n",
        "        net.zero_grad()\n",
        "        output, h = net(x, h)\n",
        "        loss = criterion(output, y.view(y.numel()).long())\n",
        "        loss.backward()        \n",
        "        nn.utils.clip_grad_norm_(net.parameters(), clip)# prevent exploding gradient\n",
        "        opt.step()\n",
        "\n",
        "        if i % 50 == 0:      #validation stats\n",
        "            val_h = net.init_hidden(batch_size)\n",
        "            val_losses = []\n",
        "            net.eval()\n",
        "            for x, y in Batch_Loader(val, batch_size, seqlen):\n",
        "                x = One_Hot_Encode(x)\n",
        "                x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "                if cuda: x, y = x.cuda(), y.cuda()\n",
        "\n",
        "                val_h = tuple([e.data for e in val_h])\n",
        "\n",
        "                output, val_h = net(x, val_h)\n",
        "                val_loss = criterion(output, y.view(y.numel()).long())\n",
        "                val_losses += val_loss.item(),\n",
        "\n",
        "            print(f\"\\rEpoch {e}\",\n",
        "                  f\"Step {i}:\",\n",
        "                  f\"Loss: {loss.item():.4f},\",\n",
        "                  f\"Val Loss: {np.mean(val_losses):.4f}\",end='')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 19 Step 100: Loss: 1.1761, Val Loss: 1.2611"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XwBe_p2rNDy3"
      },
      "source": [
        "### Prediction / Text generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XtlRmXYDNDy3",
        "colab": {}
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def predict(net, char, h):   # model, 1 char, hidden_state        \n",
        "    x = np.array([[net.char2int[char]]])\n",
        "    x = One_Hot_Encode(x)\n",
        "    x = torch.from_numpy(x)        \n",
        "    if cuda: x = x.cuda()\n",
        "\n",
        "    h = [e.data for e in h]\n",
        "    out, h = net(x, h)\n",
        "\n",
        "    p = F.softmax(out, dim=1).data    # probabilities\n",
        "    if cuda: p = p.cpu()\n",
        "\n",
        "    p = p.numpy().squeeze()  \n",
        "    char = np.random.choice(range(nchars), p=p/p.sum()) # select char according to p\n",
        "\n",
        "    return net.int2char[char], h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zTF7feFJNDy7",
        "outputId": "503835e5-1c2f-4281-f767-5acc2b3e164b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        }
      },
      "source": [
        "start = 'Human'\n",
        "size = 2000\n",
        "\n",
        "if cuda: net.cuda()\n",
        "else:    net.cpu()\n",
        "net.eval()\n",
        "\n",
        "s = start\n",
        "\n",
        "h = net.init_hidden(batch_size=1)\n",
        "for ch in start:       # feed the first word through without making predictions\n",
        "    char, h = predict(net, ch, h)\n",
        "s+=char\n",
        "\n",
        "for ii in range(size):\n",
        "    char, h = predict(net, s[-1], h)\n",
        "    s+=char\n",
        "\n",
        "print(s)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Humants\n",
            "kiffered by his. She arranged this affairs to him. She had the commission might\n",
            "gave love.\n",
            "\n",
            "\"Birch persapt is it?\" said Kitty, giving incaparitial catching his\n",
            "figure; in the great beginning to was the modest papes had never\n",
            "meet harrly dainer. He knew hes something with it was so could have not to\n",
            "go up, and was like thaising that seemed to the members of the\n",
            "morning sent to understand that he had come at done to watch to its brown back\n",
            "from his place.\n",
            "\n",
            "\"Who's I see that yesterday. Ah, with Ged at miserable sweep...\"\n",
            "\n",
            "\"She gares her the prica. That's not to think you it's to do,\" said he was\n",
            "looking with out of the troussea,\n",
            "of the table, his forehead, in a tiely tried to profly chain. The\n",
            "family other bard a moment failing to Darya Alexandrovna's hand on whom Levin\n",
            "began to grand the buckingrees heart, that not in these relations\n",
            "birachess.\n",
            "\n",
            "\"If I tell you--that's the reely.\"\n",
            "\n",
            "\"That's my hinllow business and your childness that is calagient for it.\n",
            "But \"Capitale, now di nothing in which say was worling\n",
            "that you imagined me doesn't; so ammorming.\"\n",
            "\n",
            "\"Dor you will tell you the master to resuse and denititely.\"\n",
            "\n",
            "\"When you can't very much not walk without you.\n",
            "I have not been used about a man breath, and then in\n",
            "fonger one. But made Karenins, a sense of it, against a Iwal most\n",
            "of the bosiciances as the first meal of man must tell you?\" he said, capital\n",
            "yinging vitwory and meaning, still hearing the architect out at her.\n",
            "\n",
            "\"No, sleep at the bodilies,\" she thought, siding in the saming into the\n",
            "hall), suddenly improved like out which was now now to broke firmless for her truth,\n",
            "that Stepan Arkadyevitch always misted her.\n",
            "\n",
            "\"Come, I suppose it's berous, and I could say. I've told you for\n",
            "indevery of Kitty.\" Ask Anna quickly ceated a long agoted and a past in conviction and loseet\n",
            "in this booking back, and passing her hand right. The longs mounned he made\n",
            "came into his prosen-back, and writing his head.\n",
            "\n",
            "\"Here is more...\"\n",
            "\n",
            "Sergey Ivanovitch came into the corner, and uttent\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HUdyw1jUNDzG"
      },
      "source": [
        "## Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Wgd1R5bSNDzP"
      },
      "source": [
        "* https://github.com/mineshmathew/char_rnn_karpathy_keras\n",
        "* https://www.tensorflow.org/alpha/tutorials/text/text_generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6dh-TgWUNDzP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ff1bc8dd-03e7-4ce4-b1e8-db17dbe43b23"
      },
      "source": [
        "# !pip install -q tensorflow-gpu==2.0.0-alpha0\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "tf.__version__, keras.__version__"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('2.0.0-alpha0', '2.2.4-tf')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bOgl-PMoNDzU"
      },
      "source": [
        "### Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "j1bktPRXNDzV",
        "outputId": "cfc3102b-560a-4a87-8b0a-c4387e5f0163",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os, tempfile\n",
        "os.chdir(tempfile.gettempdir())\n",
        "# path = tf.keras.utils.get_file('nietzsche.txt', origin=\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\")\n",
        "path = tf.keras.utils.get_file('shakespeare.txt', origin=\"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\")\n",
        "text = open(path).read().lower()\n",
        "print (f'Length of text: {len(text)} characters')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 1115394 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6camTAKbbyuV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2ffcb154-7e5f-4281-adf6-2a295a4fc64f"
      },
      "source": [
        "vocab = sorted(list(set(text)))\n",
        "print('total chars:', len(vocab))\n",
        "char2int = dict((c, i) for i, c in enumerate(vocab))\n",
        "int2char = np.array(vocab)  #dict((i, c) for i, c in enumerate(vocab))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total chars: 39\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alyp7_5Mb7PK",
        "colab_type": "text"
      },
      "source": [
        "#### Building training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2UvQoZuJNDzh",
        "outputId": "066ed20c-9ce4-413c-9286-7b166365b726",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# (1) explicitly build batches [a,b,c,d] --> [b,c,d], [b,c,d]--->[c,d,e]....and so on\n",
        "seqlen, step = 40, 1\n",
        "sentences = []\n",
        "next_chars = []\n",
        "for i in range(0, len(text) - seqlen+1, step):\n",
        "    sentences.append(text[i: i + seqlen])\n",
        "    next_chars.append(text[i+1:i +1+ seqlen])\n",
        "print('number of sequences:', len(sentences))\n",
        "\n",
        "import numpy as np\n",
        "X = np.zeros((len(sentences),seqlen,len(vocab)),dtype=np.bool)#dataSize x seqLength x vocabSize\n",
        "y = np.zeros((len(sentences),seqlen,len(vocab)),dtype=np.bool)\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for t, char in enumerate(sentence):\n",
        "        X[i, t, char2int[char]] = 1\n",
        "\n",
        "for i, sentence in enumerate(next_chars):\n",
        "    for t, char in enumerate(sentence):\n",
        "        y[i, t, char2int[char]] = 1"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total chars: 57\n",
            "number of sequences: 600854\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGSS48vBb4XG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "db72f2fa-ad10-406c-db44-096ee953175b"
      },
      "source": [
        "# (2) generator\n",
        "seqlen = 100\n",
        "examples_per_epoch = len(text)//seqlen\n",
        "text_as_int = [char2int[c] for c in text]\n",
        "\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)  # individual chars\n",
        "for i in char_dataset.take(25):\n",
        "  print(int2char[i.numpy()],end='')\n",
        "\n",
        "sequences = char_dataset.batch(seqlen+1, drop_remainder=True) # sequences\n",
        "for seq in sequences.take(5):\n",
        "  print(repr(''.join(int2char[seq.numpy()])))\n",
        "  \n",
        "Xy = sequences.map(lambda chunk:(chunk[:-1],chunk[1:]) )  # X,y (both are sequences)\n",
        "for X, y in  Xy.take(2):   # not shifted by step=1\n",
        "  print('X: ', repr(''.join(int2char[X.numpy()])) )\n",
        "  print('y: ', repr(''.join(int2char[y.numpy()])) )\n",
        "  \n",
        "Xy = Xy.shuffle(buffer_size=10000).batch(batch_size=64, drop_remainder=True) #shuffle and batch\n",
        "Xy"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "first citizen:\n",
            "before we 'first citizen:\\nbefore we proceed any further, hear me speak.\\n\\nall:\\nspeak, speak.\\n\\nfirst citizen:\\nyou '\n",
            "'are all resolved rather to die than to famish?\\n\\nall:\\nresolved. resolved.\\n\\nfirst citizen:\\nfirst, you k'\n",
            "\"now caius marcius is chief enemy to the people.\\n\\nall:\\nwe know't, we know't.\\n\\nfirst citizen:\\nlet us ki\"\n",
            "\"ll him, and we'll have corn at our own price.\\nis't a verdict?\\n\\nall:\\nno more talking on't; let it be d\"\n",
            "'one: away, away!\\n\\nsecond citizen:\\none word, good citizens.\\n\\nfirst citizen:\\nwe are accounted poor citi'\n",
            "X:  'first citizen:\\nbefore we proceed any further, hear me speak.\\n\\nall:\\nspeak, speak.\\n\\nfirst citizen:\\nyou'\n",
            "y:  'irst citizen:\\nbefore we proceed any further, hear me speak.\\n\\nall:\\nspeak, speak.\\n\\nfirst citizen:\\nyou '\n",
            "X:  'are all resolved rather to die than to famish?\\n\\nall:\\nresolved. resolved.\\n\\nfirst citizen:\\nfirst, you '\n",
            "y:  're all resolved rather to die than to famish?\\n\\nall:\\nresolved. resolved.\\n\\nfirst citizen:\\nfirst, you k'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int32, tf.int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JtghwnP_NDzk"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fgA-E-FNNDzl",
        "outputId": "49613b52-16a5-460e-8801-06b2349c6d08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        }
      },
      "source": [
        "# (1) no embedding; LSTM is NOT stateful (training corpus have huge repetitions with step=1)\n",
        "hiddenSizes = [512,512]\n",
        "dropoutProb = 0.2\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.LSTM(hiddenSizes[0], return_sequences=True, input_shape=(seqlen, len(vocab))),\n",
        "    keras.layers.LSTM(hiddenSizes[1], return_sequences=True),\n",
        "    keras.layers.Dropout(dropoutProb),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(len(vocab),activation='softmax')),\n",
        "])\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
        "model.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0526 22:51:18.541334 140072895281024 tf_logging.py:161] <tensorflow.python.keras.layers.recurrent.UnifiedLSTM object at 0x7f64c0207860>: Note that this layer is not optimized for performance. Please use tf.keras.layers.CuDNNLSTM for better performance on GPU.\n",
            "W0526 22:51:18.545431 140072895281024 tf_logging.py:161] <tensorflow.python.keras.layers.recurrent.UnifiedLSTM object at 0x7f64c0207dd8>: Note that this layer is not optimized for performance. Please use tf.keras.layers.CuDNNLSTM for better performance on GPU.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "unified_lstm_2 (UnifiedLSTM) (None, 40, 512)           1167360   \n",
            "_________________________________________________________________\n",
            "unified_lstm_3 (UnifiedLSTM) (None, 40, 512)           2099200   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 40, 512)           0         \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 40, 57)            29241     \n",
            "=================================================================\n",
            "Total params: 3,295,801\n",
            "Trainable params: 3,295,801\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MEa7-dme8Pm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "e9ce1ea4-0533-4bc3-fe9e-5c508ffe239a"
      },
      "source": [
        "# (2) with embedding; LSTM is stateful, requires batchsize input. corpus has no repetition\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024\n",
        "batch_size = 64\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),  #batchsize input\n",
        "    tf.keras.layers.LSTM(rnn_units,\n",
        "                         return_sequences=True,\n",
        "                         stateful=True,\n",
        "                         recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size, activation='softmax')\n",
        "  ])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "model.summary()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0526 23:43:51.554454 140072895281024 tf_logging.py:161] <tensorflow.python.keras.layers.recurrent.UnifiedLSTM object at 0x7f641e795e48>: Note that this layer is not optimized for performance. Please use tf.keras.layers.CuDNNLSTM for better performance on GPU.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (64, None, 256)           9984      \n",
            "_________________________________________________________________\n",
            "unified_lstm_8 (UnifiedLSTM) (64, None, 1024)          5246976   \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (64, None, 39)            39975     \n",
            "=================================================================\n",
            "Total params: 5,296,935\n",
            "Trainable params: 5,296,935\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZwTlFoC8NDzp"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YRHVgaqONDzp",
        "outputId": "2a4d0eef-ee48-453e-871e-c1c7cea579e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "# (1)\n",
        "history=model.fit(X, y, batch_size=128, epochs=3,verbose=1)\n",
        "print ('loss is',history.history['loss'][0])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "600854/600854 [==============================] - 189s 315us/sample - loss: 1.1020\n",
            "Epoch 2/3\n",
            "600854/600854 [==============================] - 189s 315us/sample - loss: 0.6808\n",
            "Epoch 3/3\n",
            "600854/600854 [==============================] - 190s 316us/sample - loss: 0.5654\n",
            "loss is 1.1020105174576489\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lv71_RiJfukP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "4815792f-c60d-4c4b-d461-d345eaf24d76"
      },
      "source": [
        "# (2)\n",
        "history = model.fit(Xy, epochs=3)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "172/172 [==============================] - 16s 93ms/step - loss: 2.4768\n",
            "Epoch 2/3\n",
            "172/172 [==============================] - 15s 88ms/step - loss: 1.8201\n",
            "Epoch 3/3\n",
            "172/172 [==============================] - 14s 82ms/step - loss: 1.5714\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZZlkOWTVNDzt"
      },
      "source": [
        "### Generate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NMdC0W7nNDzu",
        "colab": {}
      },
      "source": [
        "# (1)\n",
        "seed=\"brutus: this aint right. you should be going\"\n",
        "print(seed,end='')\n",
        "for i in range(1000):\n",
        "    x=np.zeros((1, seqlen, len(vocab)))\n",
        "    for t, char in enumerate(seed[-seqlen:]):\n",
        "        x[0, t, char2int[char]] = 1.\n",
        "    preds = model.predict(x, verbose=0)[0]\n",
        "    next_index = np.random.choice(range(len(vocab)),p=preds[-1])  #np.argmax(preds[-1])\n",
        "    next_char = int2char[next_index]\n",
        "    seed = seed + next_char\n",
        "    print(next_char,end='')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZy4E5wogGnA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "outputId": "caf4e186-4cb9-423b-944d-eb9f5ea262cd"
      },
      "source": [
        "# (2)\n",
        "seed=\"brutus: this aint right. you should be going\"\n",
        "print(seed,end='')\n",
        "# model.reset_states()\n",
        "# model.build(tf.TensorShape([1, None]))  # reset batchsize=1 here\n",
        "\n",
        "x = [char2int[s] for s in seed]\n",
        "x = tf.expand_dims(x, 0)\n",
        "model(x)\n",
        "# for i in range(100):\n",
        "#     predictions = model(input_eval)\n",
        "#     predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "#     predictions = predictions / temperature\n",
        "#     predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "#     # along with the previous hidden state\n",
        "#     input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "#     text_generated.append(idx2char[predicted_id])\n"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "brutus: this aint right. you should be going"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-da678a848d7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mchar2int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m# for i in range(100):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#     predictions = model(input_eval)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    658\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    659\u001b[0m               input_list, self._mixed_precision_policy.should_cast_variables):\n\u001b[0;32m--> 660\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    661\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    240\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m  \u001b[0;31m# handle the corner case where self.layers is empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    868\u001b[0m                                 ' implement a `call` method.')\n\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 870\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_internal_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m           \u001b[0;31m# Compute outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1011\u001b[0;31m           \u001b[0moutput_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomputed_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m           \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 658\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m     \u001b[0;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    658\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    659\u001b[0m               input_list, self._mixed_precision_policy.should_cast_variables):\n\u001b[0;32m--> 660\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    661\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, mask, training, initial_state)\u001b[0m\n\u001b[1;32m   3264\u001b[0m           last_output, outputs, new_h, new_c, runtime = cudnn_lstm(\n\u001b[1;32m   3265\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3266\u001b[0;31m               self.cell.recurrent_kernel, self.cell.bias, self.time_major)\n\u001b[0m\u001b[1;32m   3267\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3268\u001b[0m           last_output, outputs, new_h, new_c, runtime = standard_lstm(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36mcudnn_lstm\u001b[0;34m(inputs, input_h, input_c, kernel, recurrent_kernel, bias, time_major)\u001b[0m\n\u001b[1;32m   3436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3437\u001b[0m   outputs, h, c, _ = gen_cudnn_rnn_ops.cudnn_rnn(\n\u001b[0;32m-> 3438\u001b[0;31m       inputs, input_h=input_h, input_c=input_c, params=params, is_training=True)\n\u001b[0m\u001b[1;32m   3439\u001b[0m   \u001b[0mlast_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3440\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtime_major\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_cudnn_rnn_ops.py\u001b[0m in \u001b[0;36mcudnn_rnn\u001b[0;34m(input, input_h, input_c, params, rnn_mode, input_mode, direction, dropout, seed, seed2, is_training, name)\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0minput_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_training\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m             ctx=_ctx)\n\u001b[0m\u001b[1;32m    110\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_cudnn_rnn_ops.py\u001b[0m in \u001b[0;36mcudnn_rnn_eager_fallback\u001b[0;34m(input, input_h, input_c, params, rnn_mode, input_mode, direction, dropout, seed, seed2, is_training, name, ctx)\u001b[0m\n\u001b[1;32m    195\u001b[0m   \"is_training\", is_training)\n\u001b[1;32m    196\u001b[0m   _result = _execute.execute(b\"CudnnRNN\", 4, inputs=_inputs_flat,\n\u001b[0;32m--> 197\u001b[0;31m                              attrs=_attrs, ctx=_ctx, name=name)\n\u001b[0m\u001b[1;32m    198\u001b[0m   _execute.record_gradient(\n\u001b[1;32m    199\u001b[0m       \"CudnnRNN\", _inputs_flat, _attrs, _result, name)\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_keras_symbolic_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Invalid input_h shape: [1,64,1024] [1,1,1024] [Op:CudnnRNN]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jgt-FdbLRBH",
        "colab_type": "text"
      },
      "source": [
        "# IMDB Sentiment by RNN with LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQTymv17LRBK",
        "colab_type": "text"
      },
      "source": [
        "https://github.com/udacity/deep-learning-v2-pytorch/blob/master/sentiment-rnn/Sentiment_RNN_Solution.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWjuru71LRBL",
        "colab_type": "code",
        "outputId": "ff404a46-e1eb-48f4-cfce-3281371a6834",
        "colab": {}
      },
      "source": [
        "# !pip3 install -U torch torchvision\n",
        "import torch\n",
        "cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.__version__, cuda"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('1.1.0', False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pC1OLIHBLRBZ",
        "colab_type": "text"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "RcrWMDWlLRBZ",
        "colab_type": "code",
        "outputId": "50c6594f-7883-4ab0-98a3-c31eb33b0a4a",
        "colab": {}
      },
      "source": [
        "# import os,urllib,tempfile\n",
        "# os.chdir(tempfile.gettempdir())\n",
        "# urllib.request.urlretrieve('https://github.com/udacity/deep-learning-v2-pytorch/raw/master/sentiment-rnn/data/reviews.txt','reviews.txt')\n",
        "# urllib.request.urlretrieve('https://github.com/udacity/deep-learning-v2-pytorch/raw/master/sentiment-rnn/data/labels.txt','labels.txt')\n",
        "with open('reviews.txt', 'r') as f: reviews = f.read().strip()\n",
        "with open('labels.txt', 'r') as f:  labels = f.read().strip()\n",
        "print(reviews[:1000])\n",
        "print(labels[:20])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers  . the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students . when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled . . . . . . . . . at . . . . . . . . . . high . a classic line inspector i  m here to sack one of your teachers . student welcome to bromwell high . i expect that many adults of my age think that bromwell high is far fetched . what a pity that it isn  t   \n",
            "story of a man who has unnatural feelings for a pig . starts out with a opening scene that is a terrific example of absurd comedy . a formal orchestra audience is turn\n",
            "positive\n",
            "negative\n",
            "po\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgX94ecsLRBf",
        "colab_type": "text"
      },
      "source": [
        "### Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugpXuW3KLRBi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# encode labels\n",
        "labels = np.array([int(l[0]=='p') for l in labels.split('\\n')]) #now list of int"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JSt_n5ZLRBo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tokenize\n",
        "from string import punctuation\n",
        "reviews = ''.join(c for c in reviews if c not in punctuation)\n",
        "reviews = reviews.lower().split('\\n')  #now list of strings\n",
        "\n",
        "words = [w for sent in reviews for w in sent.split()]\n",
        "Nw = len(words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3t-oRDSLRBs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# encode reviews\n",
        "from collections import Counter\n",
        "\n",
        "counts = Counter(words)\n",
        "vocab = sorted(counts, key=counts.get, reverse=True)\n",
        "vocab2int = {word: ii+1 for ii, word in enumerate(vocab)}\n",
        "Nv = len(vocab2int)\n",
        "\n",
        "reviews = [[vocab2int[word] for word in r.split()] for r in reviews]  #now list of list of int"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4j7TmrVkLRBw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pad/truncate reviews to constant length\n",
        "seqlen =  200\n",
        "reviews = np.asarray([([0]*(seqlen-len(r))+r)[:200] for r in reviews])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EP1F80sqLRBy",
        "colab_type": "text"
      },
      "source": [
        "### train/val/test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdmeQYqCLRBz",
        "colab_type": "code",
        "outputId": "3b0376d8-e86c-4725-b1b7-5a5339b2ab09",
        "colab": {}
      },
      "source": [
        "train_frac = 0.8\n",
        "val_frac = 0.1\n",
        "\n",
        "val_start  = int(len(reviews)*train_frac)\n",
        "test_start = val_start + int(len(reviews)*val_frac)\n",
        "train_x, val_x, test_x = reviews[:val_start], reviews[val_start:test_start], reviews[test_start:]\n",
        "train_y, val_y, test_y = labels[:val_start],  labels[val_start:test_start],  labels[test_start:]\n",
        "\n",
        "print(\"Feature Shapes:\\n\",\n",
        "      train_x.shape, \n",
        "      val_x.shape,\n",
        "      test_x.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Feature Shapes:\n",
            " (20000, 200) (2500, 200) (2500, 200)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdGU4fCCLRB5",
        "colab_type": "text"
      },
      "source": [
        "### Batch Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJznX3U3LRB5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
        "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
        "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
        "\n",
        "batch_size = 50\n",
        "\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XdBJnvvLRB8",
        "colab_type": "code",
        "outputId": "f66ee831-04a2-4aee-be0d-74c0bacf4c9f",
        "colab": {}
      },
      "source": [
        "sample_x, sample_y = next(iter(train_loader))\n",
        "\n",
        "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
        "print('Sample input: \\n', sample_x)\n",
        "print()\n",
        "print('Sample label size: ', sample_y.size()) # batch_size\n",
        "print('Sample label: \\n', sample_y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample input size:  torch.Size([50, 200])\n",
            "Sample input: \n",
            " tensor([[   40,    26,   106,  ...,   674,   332,    32],\n",
            "        [    8,   552,    71,  ...,    42,    24,   142],\n",
            "        [   10,   139,   329,  ...,  2771,  5770,   282],\n",
            "        ...,\n",
            "        [   10,    14,  9088,  ...,    64,  2443,     4],\n",
            "        [    0,     0,     0,  ...,     7,     7, 54291],\n",
            "        [    0,     0,     0,  ...,     5,    75,     8]])\n",
            "\n",
            "Sample label size:  torch.Size([50])\n",
            "Sample label: \n",
            " tensor([1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,\n",
            "        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
            "        0, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzfUV_-0LRCC",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjI1p_PQLRCD",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/udacity/deep-learning-v2-pytorch/master/sentiment-rnn/assets/network_diagram.png\" width=\"200px\"/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OB3CYtm8LRCE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr=0.001\n",
        "epochs = 4 # 3-4 is approx where I noticed the validation loss stop decreasing\n",
        "print_every = 100\n",
        "clip=5 # gradient clipping"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E98LTGplLRCI",
        "colab_type": "code",
        "outputId": "a81f0bfd-945d-42b9-f23e-d603def0abd0",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SentimentRNN(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
        "        super(SentimentRNN, self).__init__()\n",
        "\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
        "                            dropout=drop_prob, batch_first=True)        \n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "        self.sig = nn.Sigmoid()\n",
        "        \n",
        "    def forward(self, x, hidden):\n",
        "        batch_size = x.size(0)\n",
        "        x = x.long()\n",
        "        embeds = self.embedding(x)\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "    \n",
        "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
        "        out = self.dropout(lstm_out)\n",
        "        out = self.fc(out)\n",
        "        sig_out = self.sig(out)\n",
        "        \n",
        "        sig_out = sig_out.view(batch_size, -1)\n",
        "        sig_out = sig_out[:, -1]\n",
        "        return sig_out, hidden\n",
        "    \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        # Helper init hidden & cell states of size (n_layers, batch_size, hidden_dim)\n",
        "        weight = next(self.parameters()).data\n",
        "        \n",
        "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
        "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
        "        \n",
        "        return hidden\n",
        "    \n",
        "vocab_size = Nv + 1 # for 0 padding\n",
        "output_size = 1\n",
        "embedding_dim = 400\n",
        "hidden_dim = 256\n",
        "n_layers = 2\n",
        "net = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
        "print(net)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SentimentRNN(\n",
            "  (embedding): Embedding(74073, 400)\n",
            "  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.3)\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (sig): Sigmoid()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2x8fb1hRLRCM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-T2X9-w6LRCP",
        "colab_type": "text"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITKL8tkVLRCQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if cuda: net.cuda()\n",
        "net.train()\n",
        "for e in range(epochs):\n",
        "    h = net.init_hidden(batch_size)\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "\n",
        "        if cuda: inputs, labels = inputs.cuda(), labels.cuda()\n",
        "        h = tuple([each.data for each in h])  # so as not to backprop to past\n",
        "\n",
        "        net.zero_grad()\n",
        "        output, h = net(inputs, h)\n",
        "        loss = criterion(output.squeeze(), labels.float())\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % print_every == 0:\n",
        "            val_h = net.init_hidden(batch_size)\n",
        "            val_losses = []\n",
        "            net.eval()\n",
        "            for inputs, labels in valid_loader:\n",
        "                if cuda: inputs, labels = inputs.cuda(), labels.cuda()\n",
        "                val_h = tuple([each.data for each in val_h]) # so as not to backprop to past\n",
        "\n",
        "                output, val_h = net(inputs, val_h)\n",
        "                val_loss = criterion(output.squeeze(), labels.float())\n",
        "                val_losses.append(val_loss.item())\n",
        "\n",
        "            net.train()\n",
        "            print(f\"Epoch: {e+1}/{epochs}...\",\n",
        "                  f\"Step: {counter}...\",\n",
        "                  f\"Loss: {loss.item():.6f}...\",\n",
        "                  f\"Val Loss: {np.mean(val_losses):.6f}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JdiyjKBLRCa",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7al49hwwLRCb",
        "colab_type": "text"
      },
      "source": [
        "### on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Nbp5iNtLRCc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_losses = [] # track loss\n",
        "num_correct = 0\n",
        "\n",
        "h = net.init_hidden(batch_size)\n",
        "net.eval()\n",
        "for inputs, labels in test_loader:\n",
        "    h = tuple([each.data for each in h])\n",
        "\n",
        "    if cuda:   inputs, labels = inputs.cuda(), labels.cuda()\n",
        "    \n",
        "    output, h = net(inputs, h)\n",
        "    test_loss = criterion(output.squeeze(), labels.float())\n",
        "    test_losses.append(test_loss.item())\n",
        "    pred = torch.round(output.squeeze())   # to either 0 or 1\n",
        "    \n",
        "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
        "    if cuda: correct_tensor=correct_tensor.cpu()\n",
        "    correct = np.squeeze(correct_tensor.numpy())\n",
        "    num_correct += np.sum(correct)\n",
        "\n",
        "\n",
        "print(f\"Test loss: {np.mean(test_losses):.3f}\")\n",
        "print(f\"Test accuracy: {num_correct/len(test_loader.dataset):.3f}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zULoelyLRCf",
        "colab_type": "text"
      },
      "source": [
        "### on new data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXJObFXcLRCg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOcKnr3SLRCi",
        "colab_type": "text"
      },
      "source": [
        "# IMDB large sentiment by Bidirectional LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yevQUOE1LRCi",
        "colab_type": "text"
      },
      "source": [
        "https://www.tensorflow.org/alpha/tutorials/text/text_classification_rnn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I99kcC4ZLRCj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q tensorflow-gpu==2.0.0-alpha0\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yG-8ipamLRCr",
        "colab_type": "text"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Frx1deGLRCs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset, info = tfds.load('imdb_reviews/subwords8k', with_info=True,\n",
        "                          as_supervised=True)\n",
        "train_dataset, test_dataset = dataset['train'], dataset['test']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "phw-x7-TLRC1",
        "colab_type": "code",
        "outputId": "d0e71c48-2121-4c0b-8e13-2b16b7a73030",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "tokenizer = info.features['text'].encoder\n",
        "print(tokenizer.encode('sample string'))\n",
        "print(tokenizer.decode(tokenizer.encode('sample string')))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[460, 4568, 49, 5875]\n",
            "sample string\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJXcYcMILRDD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = 10000\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
        "train_dataset = train_dataset.padded_batch(BATCH_SIZE, train_dataset.output_shapes)\n",
        "test_dataset = test_dataset.padded_batch(BATCH_SIZE, test_dataset.output_shapes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WL5FVPdLRDH",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMTDtkGWLRDJ",
        "colab_type": "code",
        "outputId": "de454b41-cd8c-4560-b88e-dfd2c61410b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "embed_dim=64\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Embedding(tokenizer.vocab_size, embed_dim),\n",
        "    keras.layers.Bidirectional(tf.keras.layers.LSTM(embed_dim)),\n",
        "    keras.layers.Dense(64, activation='relu'),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0525 13:38:09.723803 140406638167936 tf_logging.py:161] <tensorflow.python.keras.layers.recurrent.UnifiedLSTM object at 0x7fb2704078d0>: Note that this layer is not optimized for performance. Please use tf.keras.layers.CuDNNLSTM for better performance on GPU.\n",
            "W0525 13:38:09.729490 140406638167936 tf_logging.py:161] <tensorflow.python.keras.layers.recurrent.UnifiedLSTM object at 0x7fb270310978>: Note that this layer is not optimized for performance. Please use tf.keras.layers.CuDNNLSTM for better performance on GPU.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vB3LFxgbTKlX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 2 layers\n",
        "embed_dim=64\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(tokenizer.vocab_size, embed_dim),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embed_dim, return_sequences=True)),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),  #???\n",
        "    tf.keras.layers.Dense(embed_dim, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTSwHswOLRDO",
        "colab_type": "text"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CL7UyYuELRDP",
        "colab_type": "code",
        "outputId": "ff83d4fa-2aec-4a8f-d687-a450ffca0dbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "source": [
        "history = model.fit(train_dataset, epochs=10,\n",
        "                    validation_data=test_dataset)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "391/391 [==============================] - 73s 187ms/step - loss: 0.5591 - accuracy: 0.7080 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/10\n",
            "391/391 [==============================] - 68s 173ms/step - loss: 0.4943 - accuracy: 0.7756 - val_loss: 0.5828 - val_accuracy: 0.7039\n",
            "Epoch 3/10\n",
            "391/391 [==============================] - 68s 173ms/step - loss: 0.4360 - accuracy: 0.8133 - val_loss: 0.4864 - val_accuracy: 0.7809\n",
            "Epoch 4/10\n",
            "391/391 [==============================] - 67s 172ms/step - loss: 0.3825 - accuracy: 0.8386 - val_loss: 0.4843 - val_accuracy: 0.8057\n",
            "Epoch 5/10\n",
            "391/391 [==============================] - 67s 172ms/step - loss: 0.2997 - accuracy: 0.8796 - val_loss: 0.4464 - val_accuracy: 0.7953\n",
            "Epoch 6/10\n",
            "391/391 [==============================] - 68s 174ms/step - loss: 0.2444 - accuracy: 0.9071 - val_loss: 0.4224 - val_accuracy: 0.8240\n",
            "Epoch 7/10\n",
            "391/391 [==============================] - 67s 172ms/step - loss: 0.2104 - accuracy: 0.9230 - val_loss: 0.4450 - val_accuracy: 0.8179\n",
            "Epoch 8/10\n",
            "391/391 [==============================] - 67s 171ms/step - loss: 0.1728 - accuracy: 0.9380 - val_loss: 0.4583 - val_accuracy: 0.8360\n",
            "Epoch 9/10\n",
            "391/391 [==============================] - 67s 172ms/step - loss: 0.1360 - accuracy: 0.9538 - val_loss: 0.5049 - val_accuracy: 0.8322\n",
            "Epoch 10/10\n",
            "391/391 [==============================] - 68s 173ms/step - loss: 0.1175 - accuracy: 0.9600 - val_loss: 0.5491 - val_accuracy: 0.8158\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smheVKwLQ5w6",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtoY3WiCQ5K3",
        "colab_type": "code",
        "outputId": "9b2dfbb3-f8e2-4d0c-cef4-79777c7b9b51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "test_loss, test_acc = model.evaluate(test_dataset)\n",
        "\n",
        "print(f'Test Loss: {test_loss}')\n",
        "print(f'Test Accuracy: {test_acc}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    391/Unknown - 23s 58ms/step - loss: 0.5491 - accuracy: 0.8158Test Loss: 0.5490736904199166\n",
            "Test Accuracy: 0.8158000111579895\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qS8mp5AASP0h",
        "colab_type": "code",
        "outputId": "38a9d661-a6e4-4e09-c2f0-9c1396df834f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# New data\n",
        "s = \"I used to hate his move. This is out of expectation\"\n",
        "tokenized = tokenizer.encode(s)\n",
        "model.predict(tf.expand_dims(tokenized,0))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.3861948]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "collapsed": true,
        "id": "CutEUFmoNDzx"
      },
      "source": [
        "# [HRNN](https://github.com/fchollet/keras/blob/master/examples/mnist_hierarchical_rnn.py)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HHoSJsT7NDzy",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Input, Dense, TimeDistributed\n",
        "from keras.layers import LSTM\n",
        "batch_size = 32\n",
        "nb_epochs = 5\n",
        "row_hidden = 128\n",
        "col_hidden = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BKSs95M1NDz1"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cXODV9YjNDz2",
        "colab": {}
      },
      "source": [
        "X_train_ = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
        "X_test_ = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
        "row, col, pixel = X_train_.shape[1:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Z0TObzULNDz6"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cztvwuvzNDz7",
        "colab": {}
      },
      "source": [
        "x = Input(shape=(row, col, pixel))\n",
        "encoded_rows = TimeDistributed(LSTM(output_dim=row_hidden))(x)\n",
        "encoded_columns = LSTM(col_hidden)(encoded_rows)\n",
        "prediction = Dense(10, activation='softmax')(encoded_columns)\n",
        "model = Model(input=x, output=prediction)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AyBjnd2GND0E",
        "colab": {}
      },
      "source": [
        "model.fit(X_train_, Y_train, batch_size=batch_size, nb_epoch=nb_epochs,verbose=1,validation_data=(X_test_, Y_test))\n",
        "\n",
        "scores = model.evaluate(X_test_, Y_test, verbose=0)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkmggX_SLRD-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}