{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13.0 2.6.0 1.13.0-rc2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym, stable_baselines, tensorflow\n",
    "print(gym.__version__,stable_baselines.__version__,tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Name\\action space**   | **Refactored** to `BaseRLModel` | **Recurrent**      | `Box`          | `Discrete`     | `MultiDiscrete` | `MultiBinary`  | **Multi Processing**              |\n",
    "| ---------- | ---------------------------- | ------------------ | ------------------ | ------------------ | ------------------- | ------------------ | --------------------------------- |\n",
    "| A2C        | Yes | Yes | Yes | Yes | Yes  | Yes | Yes                |\n",
    "| ACER       | Yes           | Yes | No | Yes | No                 | No                | Yes                |\n",
    "| ACKTR      | Yes           | Yes | No | Yes | No                 | No                | Yes                |\n",
    "| DDPG       | Yes           | No                | Yes | No                | No                 | No                | Yes (MPI)|\n",
    "| DQN        | Yes           | No                | No                | Yes | No                 | No                | No                               |\n",
    "| GAIL (only for TRPO)  | Yes           | No                | Yes |Yes| No                 | No                | Yes (MPI) |\n",
    "| HER        | Yes | No                | Yes | Yes | No                 | Yes| No                               |\n",
    "| PPO1       | Yes           | No                | Yes | Yes | Yes  | Yes | Yes (MPI) |\n",
    "| PPO2       | Yes           | Yes | Yes | Yes | Yes  | Yes | Yes                |\n",
    "| SAC        | Yes           | No                | Yes | No                | No                 | No                | No                               |\n",
    "| TRPO       | Yes           | No                | Yes | Yes | Yes  | Yes | Yes (MPI) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `model = DQN(policy_name, env, learning_rate, verbose=1, tensorboard_log=\"fn\", ..)`; `env` could be a string if it is registered\n",
    "* `model.learn(total_timesteps)`: training\n",
    "* `action, _states = model.predict(observed_state)`: predict; second return `_states` for recurrent policies\n",
    "* `model.save(filename)`,\n",
    "  `model = DQN.load(\"dqn_lunar\")`: save and load model\n",
    "  \n",
    "  `load` function re-creates model from scratch. if you need to evaluate same model with multiple different sets of parameters, consider using `load_parameters` instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines import ACKTR\n",
    "from stable_baselines.common.vec_env import SubprocVecEnv\n",
    "from stable_baselines.common import set_global_seeds\n",
    "\n",
    "def make_env(env_id, thread_id, seed=0):\n",
    "    def _init():\n",
    "        env = gym.make(env_id)\n",
    "        env.seed(seed + thread_id)\n",
    "        return env\n",
    "    set_global_seeds(seed)\n",
    "    return _init\n",
    "\n",
    "cpus = 4\n",
    "env = SubprocVecEnv([make_env(\"CartPole-v1\", i) for i in range(cpus)])\n",
    "\n",
    "model = ACKTR('MlpPolicy', env, verbose=1)\n",
    "model.learn(total_timesteps=50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## callbacks/Tensorboard\n",
    "https://stable-baselines.readthedocs.io/en/master/guide/examples.html#using-callback-monitoring-training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Atari Game interface](https://stable-baselines.readthedocs.io/en/master/guide/examples.html#id1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines.common.cmd_util import make_atari_env\n",
    "from stable_baselines.common.vec_env import VecFrameStack\n",
    "from stable_baselines import ACER\n",
    "env = make_atari_env('PongNoFrameskip-v4', num_env=4, seed=0)\n",
    "env = VecFrameStack(env, n_stack=4)\n",
    "model = ACER('CnnPolicy', env, verbose=0)\n",
    "model.learn(total_timesteps=25000)\n",
    "\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    print(rewards,end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize input (env wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines.common.vec_env import DummyVecEnv, VecNormalize\n",
    "from stable_baselines import PPO2\n",
    "\n",
    "env = DummyVecEnv([lambda: gym.make(\"Reacher-v2\")])\n",
    "env = VecNormalize(env, norm_obs=True, norm_reward=False, clip_obs=10.)\n",
    "\n",
    "model = PPO2(MlpPolicy, env)\n",
    "model.learn(total_timesteps=2000)\n",
    "model.save('model')\n",
    "env.save_running_average('path/to/dir/b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Custom \"policy\" network](https://stable-baselines.readthedocs.io/en/master/guide/custom_policy.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_kwargs = dict(act_fun=tf.nn.tanh, net_arch=[32, 32])# Custom MLP policy of two layers of size 32 each with tanh activation function\n",
    "model = PPO2(\"MlpPolicy\", \"CartPole-v1\", policy_kwargs=policy_kwargs, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "from stable_baselines import A2C\n",
    "\n",
    "from stable_baselines.common.policies import FeedForwardPolicy, register_policy\n",
    "# MLP A2C policy of three layers of size 128 each (with two branches out for actor & critic)\n",
    "class CustomPolicyA2C(FeedForwardPolicy):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(CustomPolicyA2C, self).__init__(*args, **kwargs,\n",
    "                                           net_arch=[dict(pi=[128, 128, 128],   #actor-critic policy network\n",
    "                                                          vf=[128, 128, 128])],\n",
    "                                           feature_extraction=\"mlp\")\n",
    "\n",
    "model = A2C(CustomPolicyA2C, env)      \n",
    "# register_policy('CustomPolicyA2C', CustomPolicyA2C); model = A2C(policy='CustomPolicy', env) #equiv to above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines.deepq.policies import FeedForwardPolicy\n",
    "# MLP DQN policy of two layers of size 32 each\n",
    "class CustomPolicyDQN(FeedForwardPolicy):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(CustomPolicyDQN, self).__init__(*args, **kwargs,\n",
    "                                           layers=[32, 32], #size of the Neural network for the policy (if None, default to [64, 64])\n",
    "                                           layer_norm=False,\n",
    "                                           feature_extraction=\"mlp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Proximal Policy Optimization](https://stable-baselines.readthedocs.io/en/master/modules/ppo2.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* PPO2 is the implementation of OpenAI made for GPU.\n",
    "* For multiprocessing, it uses vectorized environments compared to PPO1 which uses MPI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "from stable_baselines import PPO2\n",
    "\n",
    "################# EITHER ###################\n",
    "env = gym.make('CartPole-v0')\n",
    "env = DummyVecEnv([lambda: env])  # requires a vectorized environment\n",
    "model = PPO2(MlpPolicy, env, verbose=0)\n",
    "model.learn(total_timesteps=10000)\n",
    "################### OR #####################\n",
    "# model = PPO2('MlpPolicy', 'CartPole-v1').learn(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode   20 | Average R  132.4 | Median R 144.0\n",
      "Episode   40 | Average R  140.7 | Median R 161.5\n",
      "Episode   60 | Average R  137.3 | Median R 159.5\n",
      "Episode   80 | Average R  139.6 | Median R 154.0\n",
      "Episode  100 | Average R    140 | Median R 152.0\n",
      "Episode  120 | Average R  141.9 | Median R 157.5\n",
      "Episode  140 | Average R  142.3 | Median R 152.0\n",
      "Episode  160 | Average R  142.1 | Median R 152.0\n",
      "Episode  180 | Average R  142.5 | Median R 154.0\n",
      "Episode  200 | Average R  142.3 | Median R 153.5\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "scores=[]\n",
    "episodes=200\n",
    "\n",
    "for e in range(1,episodes+1):\n",
    "    state,done = env.reset(),False\n",
    "    R = 0\n",
    "    while not done:\n",
    "        action, _states = model.predict(state)        # <- use of trained model\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        R += reward\n",
    "    scores+=R,\n",
    "    if e%(episodes//10) == 0:\n",
    "        print(f'Episode {e:4d} | Average R {np.mean(scores):6.4g} | Median R {np.median(scores)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Deep Q Network](https://stable-baselines.readthedocs.io/en/master/modules/dqn.html)\n",
    "\n",
    "* \"policy\" network is actually the action-value network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode   20 | Average R   9.35 | Median R 9.0\n",
      "Episode   40 | Average R   9.45 | Median R 9.0\n",
      "Episode   60 | Average R  9.483 | Median R 9.5\n",
      "Episode   80 | Average R  9.438 | Median R 9.0\n",
      "Episode  100 | Average R   9.37 | Median R 9.0\n",
      "Episode  120 | Average R  9.333 | Median R 9.0\n",
      "Episode  140 | Average R    9.3 | Median R 9.0\n",
      "Episode  160 | Average R  9.331 | Median R 9.0\n",
      "Episode  180 | Average R  9.356 | Median R 9.0\n",
      "Episode  200 | Average R  9.365 | Median R 9.0\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines import DQN\n",
    "from stable_baselines.deepq.policies import FeedForwardPolicy\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "env = DummyVecEnv([lambda: env]) \n",
    "\n",
    "class CustomDQNet(FeedForwardPolicy): # MLP value network\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(CustomDQNet, self).__init__(*args, **kwargs,\n",
    "                                           layers=[96, 48], #size of the Neural network for the policy (if None, default to [64, 64])\n",
    "                                           layer_norm=False,\n",
    "                                           feature_extraction=\"mlp\")\n",
    "model = DQN(CustomDQNet,env, learning_rate=1e-3, prioritized_replay=True,\n",
    "                             learning_starts=0, verbose=0)\n",
    "model.learn(total_timesteps=1000)\n",
    "\n",
    "scores=[]\n",
    "episodes=200\n",
    "\n",
    "for e in range(1,episodes+1):\n",
    "    state,done = env.reset(),False\n",
    "    R = 0\n",
    "    while not done:\n",
    "        action, _states = model.predict(state)        # <- use of trained model\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        R += reward\n",
    "    scores+=R,\n",
    "    if e%show_every == 0:\n",
    "        print(f'Episode {e:4d} | Average R {np.mean(scores):6.4g} | Median R {np.median(scores)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
