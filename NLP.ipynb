{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression on imdb for sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/sentiment-analysis-with-python-part-1-5ce197074184"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,tempfile\n",
    "os.chdir(tempfile.gettempdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request,tarfile\n",
    "urllib.request.urlretrieve('http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz','aclImdb_v1.tar.gz')\n",
    "tarfile.open('aclImdb_v1.tar.gz','r:gz').extractall()\n",
    "os.chdir('aclImdb')\n",
    "cmd='for split in train test; do for sentiment in pos neg; do for file in $split/$sentiment/*; do cat $file >> full_${split}.txt; echo >> full_${split}.txt; done; done; done;'\n",
    "os.sysmtem(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, string\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "reviews_train=[' '.join(wnl.lemmatize(w) for w in nltk.word_tokenize(l) if w.isalpha())\n",
    "               for l in open('full_train.txt')]\n",
    "reviews_test=[' '.join(wnl.lemmatize(w) for w in nltk.word_tokenize(l) if w.isalpha())\n",
    "              for l in open('full_test.txt')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOW - 1-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "features = 1/0 * vocab size (exist/not exist)\n",
    "\n",
    "target = 1/0 (positive/negative)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(binary=True)\n",
    "cv.fit(reviews_train)\n",
    "X = cv.transform(reviews_train)\n",
    "y = [1 if i < 12500 else 0 for i in range(25000)]\n",
    "\n",
    "X_test = cv.transform(reviews_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.8612399999999999\n",
      "Accuracy for C=0.03: 0.86492\n",
      "Accuracy for C=0.1: 0.8626000000000001\n",
      "Accuracy for C=0.3: 0.85856\n",
      "Accuracy for C=1: 0.8500400000000001\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "for c in [0.01, 0.03, 0.1, 0.3, 1]:  \n",
    "    print(\"Accuracy for C={}: {}\".format(c,\n",
    "                                         np.mean(cross_val_score(LogisticRegression(C=c,solver='liblinear'), X,y, cv=5, scoring='accuracy'))\n",
    "                                        )\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 0.87772\n",
      "('excellent', 0.8593489932761398)\n",
      "('perfect', 0.6801029769414313)\n",
      "('great', 0.6545590135908305)\n",
      "('amazing', 0.5924591149829764)\n",
      "('favorite', 0.5651963748851384)\n",
      "('worst', -1.2600324008053674)\n",
      "('waste', -1.0270444543800354)\n",
      "('awful', -0.8978979413808582)\n",
      "('boring', -0.7401303362319944)\n",
      "('poorly', -0.717995970049884)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "model=LogisticRegression(C=0.03).fit(X,y)\n",
    "print (\"Final Accuracy: %s\" % accuracy_score(target, model.predict(X_test)))\n",
    "feature_to_coef = {\n",
    "    word: coef for word, coef in zip( cv.get_feature_names(), model.coef_[0] )\n",
    "}\n",
    "for best_positive in sorted( feature_to_coef.items(),  key=lambda x: x[1],  reverse=True)[:5]:\n",
    "    print (best_positive)    \n",
    "for best_negative in sorted( feature_to_coef.items(),  key=lambda x: x[1])[:5]:\n",
    "    print (best_negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-gram?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification by Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compared with n-gram (counting, then computing MLE).\n",
    "\n",
    "Now representing words by vec, use CNN along time for look for pattern (larger size = longer-grams)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/babakgohardani/spam-detection-with-deep-learning-methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language modelling by MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence generation/tagging by RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
