{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/hoihui/pkgs/blob/master/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression on imdb for sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/sentiment-analysis-with-python-part-1-5ce197074184"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request,tarfile\n",
    "import os,tempfile\n",
    "os.chdir(tempfile.gettempdir())\n",
    "urllib.request.urlretrieve('http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz','aclImdb_v1.tar.gz')\n",
    "tarfile.open('aclImdb_v1.tar.gz','r:gz').extractall()\n",
    "os.chdir('aclImdb')\n",
    "cmd='for split in train test; do for sentiment in pos neg; do for file in $split/$sentiment/*; do cat $file >> full_${split}.txt; echo >> full_${split}.txt; done; done; done;'\n",
    "os.sysmtem(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, string\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "reviews_train=[' '.join(wnl.lemmatize(w) for w in nltk.word_tokenize(l) if w.isalpha())\n",
    "               for l in open('full_train.txt')]\n",
    "reviews_test=[' '.join(wnl.lemmatize(w) for w in nltk.word_tokenize(l) if w.isalpha())\n",
    "              for l in open('full_test.txt')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOW - 1-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "features = 1/0 * vocab size (exist/not exist)\n",
    "\n",
    "target = 1/0 (positive/negative)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(binary=True)\n",
    "cv.fit(reviews_train)\n",
    "X = cv.transform(reviews_train)\n",
    "y = [1 if i < 12500 else 0 for i in range(25000)]\n",
    "\n",
    "X_test = cv.transform(reviews_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.8612399999999999\n",
      "Accuracy for C=0.03: 0.86492\n",
      "Accuracy for C=0.1: 0.8626000000000001\n",
      "Accuracy for C=0.3: 0.85856\n",
      "Accuracy for C=1: 0.8500400000000001\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "for c in [0.01, 0.03, 0.1, 0.3, 1]:  \n",
    "    print(\"Accuracy for C={}: {}\".format(c,\n",
    "                                         np.mean(cross_val_score(LogisticRegression(C=c,solver='liblinear'), X,y, cv=5, scoring='accuracy'))\n",
    "                                        )\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 0.87772\n",
      "('excellent', 0.8593489932761398)\n",
      "('perfect', 0.6801029769414313)\n",
      "('great', 0.6545590135908305)\n",
      "('amazing', 0.5924591149829764)\n",
      "('favorite', 0.5651963748851384)\n",
      "('worst', -1.2600324008053674)\n",
      "('waste', -1.0270444543800354)\n",
      "('awful', -0.8978979413808582)\n",
      "('boring', -0.7401303362319944)\n",
      "('poorly', -0.717995970049884)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "model=LogisticRegression(C=0.03).fit(X,y)\n",
    "print (\"Final Accuracy: %s\" % accuracy_score(target, model.predict(X_test)))\n",
    "feature_to_coef = {\n",
    "    word: coef for word, coef in zip( cv.get_feature_names(), model.coef_[0] )\n",
    "}\n",
    "for best_positive in sorted( feature_to_coef.items(),  key=lambda x: x[1],  reverse=True)[:5]:\n",
    "    print (best_positive)    \n",
    "for best_negative in sorted( feature_to_coef.items(),  key=lambda x: x[1])[:5]:\n",
    "    print (best_negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-gram?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/udacity/deep-learning-v2-pytorch/tree/master/word2vec-embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " anarchism originated as a term of abuse first used against early working class radicals including t\n"
     ]
    }
   ],
   "source": [
    "url='https://s3.amazonaws.com/video.udacity-data.com/topher/2018/October/5bbe6499_text8/text8.zip'\n",
    "import urllib.request,zipfile\n",
    "import os,tempfile\n",
    "os.chdir(tempfile.gettempdir())\n",
    "urllib.request.urlretrieve(url,'text.zip')\n",
    "zipfile.ZipFile('text.zip').extractall()\n",
    "with open('text8') as f:\n",
    "    text = f.read()\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including', 'the', 'diggers', 'of', 'the', 'english', 'revolution', 'and', 'the', 'sans', 'culottes', 'of', 'the', 'french', 'revolution', 'whilst']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "for p in punctuation:\n",
    "    text=text.replace(p,f' {p} ')\n",
    "\n",
    "words = text.split()\n",
    "cnt = Counter(words)\n",
    "words=[w for w in words if cnt[w]>5]\n",
    "print(words[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16680599 words, 63641 vocabs\n"
     ]
    }
   ],
   "source": [
    "# Lookup table with descending frequency\n",
    "cnt = Counter(words)\n",
    "sorted_vocab = sorted(cnt, key=cnt.get, reverse=True)\n",
    "int2vocab = {ii: word for ii, word in enumerate(sorted_vocab)}\n",
    "vocab2int = {word: ii for ii, word in int2vocab.items()}\n",
    "integers = [vocab2int[w] for w in words]\n",
    "Nw = len(words)\n",
    "Nv = len(cnt)\n",
    "print(f\"{Nw} words, {Nv} vocabs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mikolov subsampling**: Discard $i^{\\rm th}$ word with frequency $$P_\\text{discard}(w_i)=1-\\sqrt{\\frac{\\rm thres}{\\text{total number of $w_i$ / total  number words}}},$$\n",
    "So that frequent words `the`, `of`, etc. are downsampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5233, 3133, 10571, 27349, 15067, 58112, 10712, 1324, 454, 2731]\n"
     ]
    }
   ],
   "source": [
    "# Mikolov subsampling\n",
    "import random, numpy as np\n",
    "thres = 1e-5\n",
    "cnt =  Counter(integers)\n",
    "P_dis = {i:1-np.sqrt(thres/(n/Nw)) for i,n in cnt.items()}\n",
    "train = [i for i in integers if random.random()>P_dis[i]]\n",
    "print(train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantage of random window size: closer words are included more frequently than distant words => higher importance to close words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 6, 7, 8]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def get_surround(li, idx, minsize=1, maxsize=5): # list of neighboring elements randomly of size 2*(minsize,maxsize)\n",
    "    R = np.random.randint(minsize, maxsize+1)\n",
    "    start = idx - R if (idx - R) > 0 else 0\n",
    "    stop = idx + R\n",
    "    win = li[start:idx] + li[idx+1:stop+1]    \n",
    "    return list(win)\n",
    "get_surround(list(range(10)), idx=5, minsize=3, maxsize=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3], [1, 2, 3, 0, 2, 3, 0, 1, 3, 0, 1, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_batches(words, batch_size, maxsize=5): #generates (list of centre, list of surround), with duplicated element in c\n",
    "    n_batches = len(words)//batch_size\n",
    "    words = words[:n_batches*batch_size]    \n",
    "    for idx in range(0, len(words), batch_size):\n",
    "        c, y = [], []\n",
    "        batch = words[idx:idx+batch_size]\n",
    "        for ii in range(len(batch)):\n",
    "            batch_c = batch[ii]\n",
    "            batch_y = get_surround(batch, ii, maxsize)\n",
    "            y.extend(batch_y)\n",
    "            c.extend([batch_c]*len(batch_y))\n",
    "        yield c, y\n",
    "        \n",
    "next(get_batches(list(range(10)), batch_size=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SkipGram using Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.1.0', False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "cuda=torch.cuda.is_available()\n",
    "device = 'cuda' if cuda else 'cpu'\n",
    "torch.__version__, cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim=300\n",
    "print_every = 500\n",
    "steps = 0\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SkipGram: predict surrounding words from centre word\n",
    "* `nn.Embedding`: converts word to vec\n",
    "* `nn.Linear`: dense embed_dim $\\rightarrow$ n_vocab (the prediction)\n",
    "* `nn.LogSoftmax`: converts to probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, n_vocab, n_embed):\n",
    "        super().__init__()        \n",
    "        self.embed = nn.Embedding(n_vocab, n_embed)\n",
    "        self.output = nn.Linear(n_embed, n_vocab)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        scores = self.output(x)\n",
    "        log_ps = self.log_softmax(scores)        \n",
    "        return log_ps\n",
    "    \n",
    "model = SkipGram(Nv, embedding_dim).to(device)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training, want some words examples that the model considers \"similar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar_examples(embedding, sample_size=10, topk=5, device='cpu'):\n",
    "    \"\"\" Returns list of exampleidx, list of list of idx similar to the corresponding exampleidx \"\"\"\n",
    "        \n",
    "    embed_vectors = embedding.weight  # shape (Nv, embedding_dim)\n",
    "    magnitudes = embed_vectors.pow(2).sum(dim=1).sqrt().unsqueeze(0) # = |b|\n",
    "    \n",
    "    # pick N vocab id from ranges (0,100) and (1000,1100)  (corr. frequent & infrequent words)\n",
    "    valid_examples = np.array(random.sample(range(100), sample_size//2))\n",
    "    valid_examples = np.append(valid_examples,\n",
    "                               random.sample(range(1000,1100), sample_size//2))\n",
    "    valid_examples = torch.LongTensor(valid_examples).to(device)\n",
    "    \n",
    "    valid_vectors = embedding(valid_examples) # shape (sample_size, embedding_dim)\n",
    "    similarities = torch.mm(valid_vectors, embed_vectors.t())/magnitudes # sim=(a.b)/|a||b|  (common denom |a| discarded)\n",
    "    \n",
    "    _, closest_idxs = similarities.topk(topk+1) # highest similarities (includes itself)\n",
    "    \n",
    "    return valid_examples.to('cpu'), closest_idxs[:,1:].to('cpu') #exclude itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0\n",
    "for _ in range(epochs):\n",
    "    \n",
    "    for centre, surround in get_batches(train, 256):\n",
    "        step+=1\n",
    "        inputs  = torch.LongTensor(centre).to(device)\n",
    "        targets = torch.LongTensor(surround).to(device)\n",
    "        \n",
    "        log_ps = model(inputs)\n",
    "        loss = criterion(log_ps, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step%print_every: continue\n",
    "        # [OPTIONAL] getting examples and similarities\n",
    "        from IPython.display import clear_output\n",
    "        clear_output()\n",
    "        for exampleidx, closeidxs in zip(*similar_examples(model.embed, device=device)):\n",
    "            print(int2vocab[exampleidx.item()] + \" ~ \" + ', '.join(int2vocab[idx.item()] for idx in closeidxs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SkipGram + Negative Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/udacity/deep-learning-v2-pytorch/blob/master/word2vec-embeddings/Negative_Sampling_Solution.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "embeddings = model.embed.weight.to('cpu').data.numpy()\n",
    "viz_words = 60\n",
    "tsne = TSNE()\n",
    "embed_tsne = tsne.fit_transform(embeddings[:viz_words, :])\n",
    "fig, ax = plt.subplots(figsize=(16, 16))\n",
    "for idx in range(viz_words):\n",
    "    plt.scatter(*embed_tsne[idx, :], color='steelblue')\n",
    "    plt.annotate(int2vocab[idx], (embed_tsne[idx, 0], embed_tsne[idx, 1]), alpha=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification by CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compared with n-gram (counting, then computing MLE).\n",
    "\n",
    "Now representing words by vec, use CNN along time for look for pattern (larger size = longer-grams)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/babakgohardani/spam-detection-with-deep-learning-methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language modelling by MLP"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
