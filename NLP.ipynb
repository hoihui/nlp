{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression on imdb for sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/sentiment-analysis-with-python-part-1-5ce197074184"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request,tarfile\n",
    "import os,tempfile\n",
    "os.chdir(tempfile.gettempdir())\n",
    "urllib.request.urlretrieve('http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz','aclImdb_v1.tar.gz')\n",
    "tarfile.open('aclImdb_v1.tar.gz','r:gz').extractall()\n",
    "os.chdir('aclImdb')\n",
    "cmd='for split in train test; do for sentiment in pos neg; do for file in $split/$sentiment/*; do cat $file >> full_${split}.txt; echo >> full_${split}.txt; done; done; done;'\n",
    "os.sysmtem(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, string\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "reviews_train=[' '.join(wnl.lemmatize(w) for w in nltk.word_tokenize(l) if w.isalpha())\n",
    "               for l in open('full_train.txt')]\n",
    "reviews_test=[' '.join(wnl.lemmatize(w) for w in nltk.word_tokenize(l) if w.isalpha())\n",
    "              for l in open('full_test.txt')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOW - 1-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "features = 1/0 * vocab size (exist/not exist)\n",
    "\n",
    "target = 1/0 (positive/negative)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(binary=True)\n",
    "cv.fit(reviews_train)\n",
    "X = cv.transform(reviews_train)\n",
    "y = [1 if i < 12500 else 0 for i in range(25000)]\n",
    "\n",
    "X_test = cv.transform(reviews_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.8612399999999999\n",
      "Accuracy for C=0.03: 0.86492\n",
      "Accuracy for C=0.1: 0.8626000000000001\n",
      "Accuracy for C=0.3: 0.85856\n",
      "Accuracy for C=1: 0.8500400000000001\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "for c in [0.01, 0.03, 0.1, 0.3, 1]:  \n",
    "    print(\"Accuracy for C={}: {}\".format(c,\n",
    "                                         np.mean(cross_val_score(LogisticRegression(C=c,solver='liblinear'), X,y, cv=5, scoring='accuracy'))\n",
    "                                        )\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 0.87772\n",
      "('excellent', 0.8593489932761398)\n",
      "('perfect', 0.6801029769414313)\n",
      "('great', 0.6545590135908305)\n",
      "('amazing', 0.5924591149829764)\n",
      "('favorite', 0.5651963748851384)\n",
      "('worst', -1.2600324008053674)\n",
      "('waste', -1.0270444543800354)\n",
      "('awful', -0.8978979413808582)\n",
      "('boring', -0.7401303362319944)\n",
      "('poorly', -0.717995970049884)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "model=LogisticRegression(C=0.03).fit(X,y)\n",
    "print (\"Final Accuracy: %s\" % accuracy_score(target, model.predict(X_test)))\n",
    "feature_to_coef = {\n",
    "    word: coef for word, coef in zip( cv.get_feature_names(), model.coef_[0] )\n",
    "}\n",
    "for best_positive in sorted( feature_to_coef.items(),  key=lambda x: x[1],  reverse=True)[:5]:\n",
    "    print (best_positive)    \n",
    "for best_negative in sorted( feature_to_coef.items(),  key=lambda x: x[1])[:5]:\n",
    "    print (best_negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-gram?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/udacity/deep-learning-v2-pytorch/tree/master/word2vec-embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " anarchism originated as a term of abuse first used against early working class radicals including t\n"
     ]
    }
   ],
   "source": [
    "url='https://s3.amazonaws.com/video.udacity-data.com/topher/2018/October/5bbe6499_text8/text8.zip'\n",
    "import urllib.request,zipfile\n",
    "import os,tempfile\n",
    "os.chdir(tempfile.gettempdir())\n",
    "urllib.request.urlretrieve(url,'text.zip')\n",
    "zipfile.ZipFile('text.zip').extractall()\n",
    "with open('text8') as f:\n",
    "    text = f.read()\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including', 'the', 'diggers', 'of', 'the', 'english', 'revolution', 'and', 'the', 'sans', 'culottes', 'of', 'the', 'french', 'revolution', 'whilst']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "for p in punctuation:\n",
    "    text=text.replace(p,f' {p} ')\n",
    "\n",
    "words = text.split()\n",
    "cnt = Counter(words)\n",
    "words=[w for w in words if cnt[w]>5]\n",
    "print(words[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16680599 words, 63641 vocabs\n"
     ]
    }
   ],
   "source": [
    "# Lookup table with descending frequency\n",
    "cnt = Counter(words)\n",
    "sorted_vocab = sorted(cnt, key=cnt.get, reverse=True)\n",
    "int2vocab = {ii: word for ii, word in enumerate(sorted_vocab)}\n",
    "vocab2int = {word: ii for ii, word in int2vocab.items()}\n",
    "integers = [vocab2int[w] for w in words]\n",
    "Nw = len(words)\n",
    "Nv = len(cnt)\n",
    "print(f\"{Nw} words, {Nv} vocabs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mikolov subsampling**: Discard $i^{\\rm th}$ word with frequency $$P_\\text{discard}(w_i)=1-\\sqrt{\\frac{\\rm thres}{\\text{total number of $w_i$ / total  number words}}},$$\n",
    "So that frequent words `the`, `of`, etc. are downsampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5233, 3133, 10571, 27349, 15067, 58112, 10712, 1324, 454, 2731]\n"
     ]
    }
   ],
   "source": [
    "# Mikolov subsampling\n",
    "import random, numpy as np\n",
    "thres = 1e-5\n",
    "cnt =  Counter(integers)\n",
    "P_dis = {i:1-np.sqrt(thres/(n/Nw)) for i,n in cnt.items()}\n",
    "train = [i for i in integers if random.random()>P_dis[i]]\n",
    "print(train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantage of random window size: closer words are included more frequently than distant words => higher importance to close words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 6, 7, 8]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def get_surround(li, idx, minsize=1, maxsize=5): # list of neighboring elements randomly of size 2*(minsize,maxsize)\n",
    "    R = np.random.randint(minsize, maxsize+1)\n",
    "    start = idx - R if (idx - R) > 0 else 0\n",
    "    stop = idx + R\n",
    "    win = li[start:idx] + li[idx+1:stop+1]    \n",
    "    return list(win)\n",
    "get_surround(list(range(10)), idx=5, minsize=3, maxsize=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3], [1, 2, 3, 0, 2, 3, 0, 1, 3, 0, 1, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_batches(words, batch_size, maxsize=5): #generates (list of centre, list of surround), with duplicated element in c\n",
    "    n_batches = len(words)//batch_size\n",
    "    words = words[:n_batches*batch_size]    \n",
    "    for idx in range(0, len(words), batch_size):\n",
    "        c, y = [], []\n",
    "        batch = words[idx:idx+batch_size]\n",
    "        for ii in range(len(batch)):\n",
    "            batch_c = batch[ii]\n",
    "            batch_y = get_surround(batch, ii, maxsize)\n",
    "            y.extend(batch_y)\n",
    "            c.extend([batch_c]*len(batch_y))\n",
    "        yield c, y\n",
    "        \n",
    "next(get_batches(list(range(10)), batch_size=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SkipGram using Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.1.0', False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "cuda=torch.cuda.is_available()\n",
    "device = 'cuda' if cuda else 'cpu'\n",
    "torch.__version__, cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim=300\n",
    "print_every = 500\n",
    "steps = 0\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SkipGram: predict surrounding words from centre word\n",
    "* `nn.Embedding`: converts word to vec\n",
    "* `nn.Linear`: dense embed_dim $\\rightarrow$ n_vocab (the prediction)\n",
    "* `nn.LogSoftmax`: converts to probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, n_vocab, n_embed):\n",
    "        super().__init__()        \n",
    "        self.embed = nn.Embedding(n_vocab, n_embed)\n",
    "        self.output = nn.Linear(n_embed, n_vocab)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        scores = self.output(x)\n",
    "        log_ps = self.log_softmax(scores)        \n",
    "        return log_ps\n",
    "    \n",
    "model = SkipGram(Nv, embedding_dim).to(device)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training, want some words examples that the model considers \"similar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar_examples(embedding, sample_size=10, topk=5, device='cpu'):\n",
    "    \"\"\" Returns list of exampleidx, list of list of idx similar to the corresponding exampleidx \"\"\"\n",
    "        \n",
    "    embed_vectors = embedding.weight  # shape (Nv, embedding_dim)\n",
    "    magnitudes = embed_vectors.pow(2).sum(dim=1).sqrt().unsqueeze(0) # = |b|\n",
    "    \n",
    "    # pick N vocab id from ranges (0,100) and (1000,1100)  (corr. frequent & infrequent words)\n",
    "    valid_examples = np.array(random.sample(range(100), sample_size//2))\n",
    "    valid_examples = np.append(valid_examples,\n",
    "                               random.sample(range(1000,1100), sample_size//2))\n",
    "    valid_examples = torch.LongTensor(valid_examples).to(device)\n",
    "    \n",
    "    valid_vectors = embedding(valid_examples) # shape (sample_size, embedding_dim)\n",
    "    similarities = torch.mm(valid_vectors, embed_vectors.t())/magnitudes # sim=(a.b)/|a||b|  (common denom |a| discarded)\n",
    "    \n",
    "    _, closest_idxs = similarities.topk(topk+1) # highest similarities (includes itself)\n",
    "    \n",
    "    return valid_examples.to('cpu'), closest_idxs[:,1:].to('cpu') #exclude itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0\n",
    "for _ in range(epochs):\n",
    "    \n",
    "    for centre, surround in get_batches(train, 256):\n",
    "        step+=1\n",
    "        inputs  = torch.LongTensor(centre).to(device)\n",
    "        targets = torch.LongTensor(surround).to(device)\n",
    "        \n",
    "        log_ps = model(inputs)\n",
    "        loss = criterion(log_ps, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step%print_every: continue\n",
    "        # [OPTIONAL] getting examples and similarities\n",
    "        from IPython.display import clear_output\n",
    "        clear_output()\n",
    "        for exampleidx, closeidxs in zip(*similar_examples(model.embed, device=device)):\n",
    "            print(int2vocab[exampleidx.item()] + \" ~ \" + ', '.join(int2vocab[idx.item()] for idx in closeidxs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SkipGram + Negative Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/udacity/deep-learning-v2-pytorch/blob/master/word2vec-embeddings/Negative_Sampling_Solution.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "embeddings = model.embed.weight.to('cpu').data.numpy()\n",
    "viz_words = 60\n",
    "tsne = TSNE()\n",
    "embed_tsne = tsne.fit_transform(embeddings[:viz_words, :])\n",
    "fig, ax = plt.subplots(figsize=(16, 16))\n",
    "for idx in range(viz_words):\n",
    "    plt.scatter(*embed_tsne[idx, :], color='steelblue')\n",
    "    plt.annotate(int2vocab[idx], (embed_tsne[idx, 0], embed_tsne[idx, 1]), alpha=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding + sentiment of imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/alpha/tutorials/text/word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,) (25000,) (25000,) (25000,)\n",
      "[1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10000\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.imdb.load_data(num_words=vocab_size) # discard infrequent words\n",
    "# https://stackoverflow.com/questions/55890813/how-to-fix-object-arrays-cannot-be-loaded-when-allow-pickle-false-for-imdb-loa\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "print(X_train[1][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<START> big hair big boobs bad music and a giant safety pin these are the words to best describe this terrible movie i love cheesy horror movies and i've seen hundreds but this had got to be on of the worst ever made the plot is paper thin and ridiculous the acting is an abomination the script is completely laughable the best is the end showdown with the cop and how he worked out who the killer is it's just so damn terribly written the clothes are sickening and funny in equal <UNK> the hair is big lots of boobs <UNK> men wear those cut <UNK> shirts that show off their <UNK> sickening that men actually wore them and the music is just <UNK> trash that plays over and over again in almost every scene there is trashy music boobs and <UNK> taking away bodies and the gym still doesn't close for <UNK> all joking aside this is a truly bad film whose only charm is to look back on the disaster that was the 80's and have a good old laugh at how bad everything was back then\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# int2words function\n",
    "word_index = keras.datasets.imdb.get_word_index()\n",
    "# reserve first 4 indices\n",
    "word_index = {k:(v+3) for k,v in word_index.items()}\n",
    "word_index[\"<PAD>\"] = 0\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2\n",
    "word_index[\"<UNUSED>\"] = 3\n",
    "reverse_word_index = {value: key for (key, value) in word_index.items()}\n",
    "\n",
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
    "\n",
    "decode_review(X_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{500}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pad/truncate the same length\n",
    "maxlen = 500\n",
    "X_train = keras.preprocessing.sequence.pad_sequences(X_train,\n",
    "                                                     value=word_index[\"<PAD>\"],\n",
    "                                                     padding='post',\n",
    "                                                     maxlen=maxlen)\n",
    "X_test = keras.preprocessing.sequence.pad_sequences(X_test,\n",
    "                                                    value=word_index[\"<PAD>\"],\n",
    "                                                    padding='post',\n",
    "                                                    maxlen=maxlen)\n",
    "set(map(len,X_train))|set(map(len,X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 16)           160000    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 160,289\n",
      "Trainable params: 160,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_dim=16\n",
    "\n",
    "model = keras.Sequential([\n",
    "  keras.layers.Embedding(vocab_size, embedding_dim, input_length=maxlen),\n",
    "  keras.layers.GlobalAveragePooling1D(),\n",
    "  keras.layers.Dense(16, activation='relu'),\n",
    "  keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/300\n",
      "20000/20000 [==============================] - 1s 69us/sample - loss: 0.6692 - accuracy: 0.6573 - val_loss: 0.5949 - val_accuracy: 0.7776\n",
      "Epoch 2/300\n",
      "20000/20000 [==============================] - 1s 58us/sample - loss: 0.4666 - accuracy: 0.8402 - val_loss: 0.3800 - val_accuracy: 0.8662\n",
      "Epoch 3/300\n",
      "20000/20000 [==============================] - 1s 62us/sample - loss: 0.3168 - accuracy: 0.8841 - val_loss: 0.3168 - val_accuracy: 0.8794: 0.3280 \n",
      "Epoch 4/300\n",
      "20000/20000 [==============================] - 1s 60us/sample - loss: 0.2587 - accuracy: 0.9040 - val_loss: 0.2962 - val_accuracy: 0.8828\n",
      "Epoch 5/300\n",
      "20000/20000 [==============================] - 1s 61us/sample - loss: 0.2233 - accuracy: 0.9192 - val_loss: 0.2818 - val_accuracy: 0.8902\n",
      "Epoch 6/300\n",
      "20000/20000 [==============================] - 1s 59us/sample - loss: 0.1989 - accuracy: 0.9284 - val_loss: 0.2775 - val_accuracy: 0.8920\n",
      "Epoch 7/300\n",
      "20000/20000 [==============================] - 1s 59us/sample - loss: 0.1783 - accuracy: 0.9362 - val_loss: 0.2774 - val_accuracy: 0.8946\n",
      "Epoch 8/300\n",
      "20000/20000 [==============================] - 1s 59us/sample - loss: 0.1609 - accuracy: 0.9436 - val_loss: 0.2813 - val_accuracy: 0.8944\n",
      "Epoch 9/300\n",
      "20000/20000 [==============================] - 1s 60us/sample - loss: 0.1469 - accuracy: 0.9499 - val_loss: 0.2863 - val_accuracy: 0.8954\n",
      "Epoch 10/300\n",
      "20000/20000 [==============================] - 1s 63us/sample - loss: 0.1344 - accuracy: 0.9546 - val_loss: 0.2909 - val_accuracy: 0.8968\n",
      "Epoch 11/300\n",
      "20000/20000 [==============================] - 1s 60us/sample - loss: 0.1243 - accuracy: 0.9577 - val_loss: 0.3017 - val_accuracy: 0.8936\n",
      "Epoch 12/300\n",
      "20000/20000 [==============================] - 1s 61us/sample - loss: 0.1139 - accuracy: 0.9627 - val_loss: 0.3083 - val_accuracy: 0.8944\n",
      "Epoch 00012: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0xb3e42ccc0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tempfile\n",
    "_,weightsfile=tempfile.mkstemp()\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "es=EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "mc=ModelCheckpoint(weightsfile,monitor='val_loss',save_best_only=True,save_weights_only=True,verbose=0)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=300, batch_size=64,\n",
    "                    callbacks=[es,mc],\n",
    "                    validation_split=0.2)\n",
    "model.load_weights(weightsfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Validation loss history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.figure(figsize=(12,9))\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,9))\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylim((0.5,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve learned embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 16)\n"
     ]
    }
   ],
   "source": [
    "weights = model.layers[0].get_weights()[0]          # embedding layer weights\n",
    "print(weights.shape)                                # vocab size -> embedding dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 0s 19us/sample - loss: 0.2825 - accuracy: 0.8866\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.28253252049446104, 0.88656]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample/new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.17863826]\n",
      " [0.99984455]\n",
      " [0.8195051 ]]\n",
      "please give this one a miss br br <UNK> <UNK> and the rest of the cast rendered terrible performances the show is flat flat flat br br i don't know how michael madison could have allowed this one on his plate he almost seemed to know this wasn't going to work out and his performance was quite <UNK> so all you madison fans give this a miss\n",
      "this film requires a lot of patience because it focuses on mood and character development the plot is very simple and many of the scenes take place on the same set in frances <UNK> the sandy dennis character apartment but the film builds to a disturbing climax br br the characters create an atmosphere <UNK> with sexual tension and psychological <UNK> it's very interesting that robert altman directed this considering the style and structure of his other films still the trademark altman audio style is evident here and there i think what really makes this film work is the brilliant performance by sandy dennis it's definitely one of her darker characters but she plays it so perfectly and convincingly that it's scary michael burns does a good job as the mute young man regular altman player michael murphy has a small part the <UNK> moody set fits the content of the story very well in short this movie is a powerful study of loneliness sexual <UNK> and desperation be patient <UNK> up the atmosphere and pay attention to the wonderfully written script br br i praise robert altman this is one of his many films that deals with unconventional fascinating subject matter this film is disturbing but it's sincere and it's sure to <UNK> a strong emotional response from the viewer if you want to see an unusual film some might even say bizarre this is worth the time br br unfortunately it's very difficult to find in video stores you may have to buy it off the internet\n",
      "at a time when motion picture animation of all sorts was in its <UNK> br br the political <UNK> of the russian revolution caused <UNK> to move to paris where one of his first productions <UNK> was a dark political satire <UNK> known as <UNK> or the <UNK> who wanted a king a strain of black comedy can be found in almost all of films but here it is very dark indeed aimed more at grown ups who can appreciate the satirical aspects than children who would most likely find the climax <UNK> i'm middle aged and found it pretty <UNK> myself and indeed <UNK> of the film intended for english speaking viewers of the 1920s were given title cards filled with <UNK> and <UNK> in order to help <UNK> the sharp <UNK> of the finale br br our tale is set in a swamp the <UNK> <UNK> where the citizens are unhappy with their government and have called a special session to see what they can do to improve matters they decide to <UNK> <UNK> for a king the crowds are <UNK> animated in this opening sequence it couldn't have been easy to make so many frog puppets look alive simultaneously while <UNK> for his part is depicted as a <UNK> white <UNK> guy in the clouds who looks like he'd rather be taking a <UNK> when <UNK> sends them a tree like god who regards them the <UNK> decide that this is no improvement and demand a different king irritated <UNK> sends them a <UNK> br br delighted with this <UNK> looking new king who towers above them the <UNK> welcome him with a <UNK> of <UNK> dressed <UNK> the mayor steps forward to hand him the key to the <UNK> as <UNK> cameras record the event to everyone's horror the <UNK> promptly eats the mayor and then goes on a merry rampage <UNK> citizens at random a title card <UNK> reads news of the king's <UNK> throughout the kingdom when the now terrified <UNK> once more <UNK> <UNK> for help he loses his temper and <UNK> their community with lightning <UNK> the moral of our story delivered by a hapless frog just before he is eaten is let well enough alone br br considering the time period when this startling little film was made and considering the fact that it was made by a russian <UNK> at the height of that <UNK> country's civil war it would be easy to see this as a <UNK> about those events <UNK> may or may not have had <UNK> turmoil in mind when he made <UNK> but whatever <UNK> his choice of material the film stands as a <UNK> tale of universal <UNK> <UNK> could be the soviet union italy germany or japan in the 1930s or any country of any era that lets its guard down and is overwhelmed by <UNK> it's a fascinating film even a charming one in its macabre way but its message is no joke\n"
     ]
    }
   ],
   "source": [
    "print(model(X_test[:3]).numpy())\n",
    "for i in range(3):\n",
    "    print(decode_review(X_test[i]).strip('<PAD>STR '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.51735705]], dtype=float32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "sent='used to like his movies this is an exception'\n",
    "X=[word_index[w] for w in sent.split()]\n",
    "X+=[word_index[\"<PAD>\"]]*(maxlen-len(X))\n",
    "model(np.array([X])).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification by CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compared with n-gram (counting, then computing MLE).\n",
    "\n",
    "Now representing words by vec, use CNN along time for look for pattern (larger size = longer-grams)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/babakgohardani/spam-detection-with-deep-learning-methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language modelling by MLP"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
