{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0-beta1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Catch](https://gist.github.com/EderSantana/c7222daa328f0e885093) MLP on raw pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "network input: `state (raw pixels)` output: `Q(a|state)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Catch(object): # 1 game is 1 fruit dropped from top to bottom\n",
    "    def __init__(self, grid_size=10):\n",
    "        self.grid_size = grid_size\n",
    "        self.basketSize = 0 # actually this is 2*basket_size+1\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        n = np.random.randint(0, self.grid_size-1)  # starting fruit_col\n",
    "        m = np.random.randint(1, self.grid_size-2)  # starting basket col\n",
    "        self.state = np.asarray([0, n, m])          # [fruit_row, fruit_col, basket]\n",
    "        return self.observe()\n",
    "    \n",
    "    def _get_reward(self):   # inc/dec score only if fruit has dropped to bottom\n",
    "        fruit_row, fruit_col, basket = self.state\n",
    "        if fruit_row == self.grid_size-1:\n",
    "            return 1 if abs(fruit_col - basket) <= self.basketSize else -1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def _is_over(self):    # game over if fruit dropped to bottom\n",
    "        return (self.state[0] == self.grid_size-1)\n",
    "    \n",
    "    def observe(self):\n",
    "        im_size = (self.grid_size, self.grid_size)\n",
    "        state = self.state\n",
    "        canvas = np.zeros(im_size)\n",
    "        canvas[self.state[0], self.state[1]] = 1                                         # draw fruit\n",
    "        canvas[-1, self.state[2]-self.basketSize:self.state[2] + self.basketSize+1] = 1  # draw basket\n",
    "        return canvas.flatten()\n",
    "    \n",
    "    def step(self, action):\n",
    "        if action == 0:   action = -1 # move left\n",
    "        elif action == 1: action =  0 # stay\n",
    "        else:             action =  1 # move right\n",
    "        f0, f1, basket = self.state\n",
    "        new_basket = min(max(self.basketSize, basket + action), self.grid_size-self.basketSize)\n",
    "        f0 += 1                       # fruit dropped by one pixel\n",
    "        out = np.asarray([f0, f1, new_basket])\n",
    "        self.state = out\n",
    "        \n",
    "        return self.observe(), self._get_reward(), self._is_over() # returns whole canvas, R, done?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Store all intermediate states \n",
    "* after each step, train a random batch from memory\n",
    "* train target are the Q-values computed from current Q_tables predicted my the current model (`model.predict(state)`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 00100 | Loss 0.0081 | Win rate 0.120\n",
      "Episode 00200 | Loss 0.0054 | Win rate 0.190\n",
      "Episode 00300 | Loss 0.0092 | Win rate 0.210\n",
      "Episode 00400 | Loss 0.0051 | Win rate 0.230\n",
      "Episode 00500 | Loss 0.0123 | Win rate 0.380\n",
      "Episode 00600 | Loss 0.0055 | Win rate 0.370\n",
      "Episode 00700 | Loss 0.0080 | Win rate 0.620\n",
      "Episode 00800 | Loss 0.0048 | Win rate 0.580\n",
      "Episode 00900 | Loss 0.0020 | Win rate 0.640\n",
      "Episode 01000 | Loss 0.0017 | Win rate 0.720\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "class DQN(object):  #model+memory\n",
    "    def __init__(self,catchenv,\n",
    "                 gamma=.9, # gamma is reward decay in computing G_t=R_t+\\gamma*R_{t+1}+...\n",
    "                 max_memory=500):\n",
    "        self.memory = deque(maxlen=max_memory)\n",
    "        self.env = catchenv\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.model = tf.keras.models.Sequential([\n",
    "                         tf.keras.layers.Dense(100, input_shape=(self.env.grid_size**2,), activation='relu'),\n",
    "                         tf.keras.layers.Dense(100, activation='relu'),\n",
    "                         tf.keras.layers.Dense(3),  # [move_left, stay, move_right]\n",
    "                        ])\n",
    "        self.model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam())     # learning rate handled by optimizer\n",
    "        self.target_model = tf.keras.models.clone_model(self.model)\n",
    "    \n",
    "    def get_epsilon(self, episode):\n",
    "        ###################### EPSILON to use for epsilon-greedy (probability of exploration)\n",
    "#         return 1/(1+e*.2)\n",
    "#         return max(.01, 0.995**episode)\n",
    "        return 0.1\n",
    "        #####################################################################################\n",
    "\n",
    "    def choose_action(self, state, epsilon):\n",
    "        if np.random.random() <= epsilon:\n",
    "            return np.random.randint(0, 3, size=1)\n",
    "        else:\n",
    "            return np.argmax(self.model.predict(state[np.newaxis])[0])\n",
    "        \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def replay_train(self, batch_size=50):\n",
    "        X, Y = [], []  #(batch_size, n_pixels-whole canvas), (batch_size, num_actions)\n",
    "        minibatch = random.sample( self.memory, min(len(self.memory), batch_size))\n",
    "        ys=self.model.predict(np.array([e[0] for e in minibatch]))         # current estimation of the Q(a|s)\n",
    "        qs=self.target_model.predict(np.array([e[3] for e in minibatch]))  # Q(a|s') for next state (target is r+g*max(this) for the action taken, otherwise use current estimation as the target)\n",
    "\n",
    "        for i,(state, action, reward, next_state, done) in enumerate(minibatch):\n",
    "            y = ys[i]  #self.model.predict(state[np.newaxis])[0]\n",
    "            q = qs[i]  #self.model.predict(next_state[np.newaxis])[0]\n",
    "            y[action] = reward + self.gamma*(0 if done else np.max(q))     # R_t + gam * max_a' Q(s',a') ONLY for the action executed; others remain unchanged from current prediction\n",
    "            X += state,\n",
    "            Y += y,\n",
    "            \n",
    "        return self.model.train_on_batch(np.array(X), np.array(Y)) #returns current loss\n",
    "\n",
    "    def run(self,episdoes=1000,eval_mode=False):\n",
    "        scores = deque(maxlen=episdoes//10) #store new episodes after previous print\n",
    "\n",
    "        for e in range(1,episdoes+1):\n",
    "            EPSILON = 0 if eval_mode else self.get_epsilon(e)\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            R = 0  #total return at the end of episode\n",
    "            while not done:\n",
    "                action = self.choose_action(state, EPSILON)\n",
    "                next_state, reward, done = self.env.step(action)\n",
    "                R += reward\n",
    "                next_state = next_state\n",
    "                self.remember(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "            loss = np.nan if eval_mode else self.replay_train()\n",
    "            scores.append((R+1)/2)\n",
    "            \n",
    "            if e%(episdoes/10) == 0:\n",
    "                print(f'Episode {e:05d} | Loss {loss:.4f} | Win rate {np.mean(scores):.3f}')\n",
    "            if e%1==0:  #update frequency of target network\n",
    "                self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "env=Catch(grid_size = 10)\n",
    "agent = DQN(env)\n",
    "agent.run(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation (epsilon=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 00010 | Loss nan | Win rate 0.700\n",
      "Episode 00020 | Loss nan | Win rate 0.600\n",
      "Episode 00030 | Loss nan | Win rate 0.800\n",
      "Episode 00040 | Loss nan | Win rate 0.700\n",
      "Episode 00050 | Loss nan | Win rate 0.600\n",
      "Episode 00060 | Loss nan | Win rate 0.900\n",
      "Episode 00070 | Loss nan | Win rate 0.800\n",
      "Episode 00080 | Loss nan | Win rate 0.900\n",
      "Episode 00090 | Loss nan | Win rate 0.500\n",
      "Episode 00100 | Loss nan | Win rate 0.800\n"
     ]
    }
   ],
   "source": [
    "agent.run(100,eval_mode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib import animation, rc\n",
    "from IPython.display import HTML\n",
    "frames = []\n",
    "\n",
    "for e in range(100):\n",
    "    loss = 0.\n",
    "    env.reset()\n",
    "    done = False\n",
    "    state = env.observe()\n",
    "    frames.append(state.reshape(grid_size,grid_size))\n",
    "    while not done:\n",
    "        q = agent.model.predict(state[np.newaxis])  # q table at current state\n",
    "        action = np.argmax(q[0])\n",
    "        next_state, reward, done = env.step(action)\n",
    "        frames.append(next_state.reshape(grid_size,grid_size))\n",
    "        state = next_state\n",
    "# plt.imshow(frames[9],interpolation='none', cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#animation\n",
    "fig, ax = plt.subplots()\n",
    "im  = ax.imshow(np.random.random((grid_size,)*2),interpolation='none', cmap='gray')\n",
    "def init():\n",
    "#     im.set_array(np.random.random((grid_size,grid_size)))\n",
    "    return (im,)\n",
    "def animate(i):\n",
    "    im.set_array(frames[i])\n",
    "    return (im,)\n",
    "anim = animation.FuncAnimation(fig, animate, init_func=init,\n",
    "                               frames=len(frames), interval=50, blit=True)\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to files\n",
    "for i in range(len(frames)):\n",
    "    plt.imshow(frames[i],interpolation='none', cmap='gray')\n",
    "    plt.savefig(\"%03d.png\" % i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# [CartPole](https://gym.openai.com/envs/CartPole-v0/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  MLP(value+target) on 4-tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://gym.openai.com/evaluations/eval_EIcM1ZBnQW2LBaFN6FY65g/, https://gym.openai.com/evaluations/eval_OeUSZwUcR2qSAqMmOE1UIw/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 00050: Mean Time 10.380, Max Time 13\n",
      "Episode 00100: Mean Time 50.940, Max Time 111\n",
      "Episode 00150: Mean Time 61.680, Max Time 200\n",
      "Episode 00200: Mean Time 196.580, Max Time 200\n",
      "Episode 00250: Mean Time 151.740, Max Time 200\n",
      "Episode 00300: Mean Time 138.980, Max Time 200\n",
      "Episode 00350: Mean Time 192.720, Max Time 200\n",
      "Episode 00400: Mean Time 197.160, Max Time 200\n",
      "Episode 00450: Mean Time 193.620, Max Time 200\n",
      "Episode 00500: Mean Time 136.940, Max Time 200\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import gym\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "\n",
    "class DQN(object): # model + memory\n",
    "    def __init__(self, env, # gym's env\n",
    "                 gamma=.99, # gamma is reward decay in computing G_t=R_t+\\gamma*R_{t+1}+...\n",
    "                 max_memory=1000):\n",
    "        self.memory = deque(maxlen=max_memory)\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.model = tf.keras.models.Sequential()\n",
    "        self.model.add(tf.keras.layers.Dense(16, input_shape=self.env.observation_space.shape, activation='relu'))\n",
    "        self.model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "        self.model.add(tf.keras.layers.Dense(self.env.action_space.n, activation='linear'))\n",
    "        self.model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(.001)) #optimizer has its parameters\n",
    "        \n",
    "        self.target_model = tf.keras.models.clone_model(self.model)\n",
    "    \n",
    "    ###################### EPSILON to use for epsilon-greedy (probability of exploration)##############################\n",
    "    def get_epsilon(self, episode):\n",
    "#         return 1/(1+episode*.1)  \n",
    "#         return max(.01, 0.995**episode)\n",
    "        return 0.1\n",
    "    ###################################################################################################################\n",
    "\n",
    "    def choose_action(self, state, epsilon):\n",
    "        if np.random.random() <= epsilon:\n",
    "            return self.env.action_space.sample() \n",
    "        else:\n",
    "            return np.argmax(self.model.predict(state[np.newaxis])[0])\n",
    "        \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def replay_train(self, batch_size=32):\n",
    "        X, Y = [], []  #(batch_size, n_pixels-whole canvas), (batch_size, num_actions)\n",
    "        \n",
    "        minibatch = random.sample( self.memory, min(len(self.memory), batch_size))\n",
    "        ys=self.model.predict(np.array([e[0] for e in minibatch]))      # current estimation of the Q(a|s)\n",
    "        qs=self.model.predict(np.array([e[3] for e in minibatch]))      # Q(a|s') for next state (target is r+g*max(this) for the action taken, otherwise use current estimation as the target)\n",
    "#         qs should use target_model\n",
    "\n",
    "        for i,(state, action, reward, next_state, done) in enumerate(minibatch):\n",
    "            y = ys[i]  #self.model.predict(state[np.newaxis])[0]\n",
    "            q = qs[i]  #self.model.predict(next_state[np.newaxis])[0]\n",
    "            y[action] = reward + (0 if done else self.gamma*np.max(q))     # R_t + gam * max_a' Q(s',a') ONLY for the action executed; others remain unchanged from current prediction\n",
    "            X += state,\n",
    "            Y += y,\n",
    "            \n",
    "        return self.model.train_on_batch(np.array(X), np.array(Y)) #returns current loss\n",
    "\n",
    "    def run(self,episodes=2000,eval_mode=False):\n",
    "        scores = deque(maxlen=episodes//10)\n",
    "\n",
    "        for e in range(1,episodes+1):\n",
    "            EPSILON = 0 if eval_mode else self.get_epsilon(e)\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            TotalR = 0\n",
    "            while not done:\n",
    "                action = self.choose_action(state, EPSILON)\n",
    "                next_state, reward, done, info = self.env.step(action)\n",
    "                TotalR += reward\n",
    "                next_state = next_state\n",
    "#                 if done: reward=-200  # important(?)\n",
    "                self.remember(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                if not eval_mode: self.replay_train()   # train once every step better than once every episode\n",
    "            scores.append(TotalR)\n",
    "            \n",
    "            if e%(episodes/10) == 0:\n",
    "                print(f'Episode {e:05d}: Mean Time {np.mean(scores):.3f}, Max Time {np.max(scores):.0f}')\n",
    "            \n",
    "            if e%1==0:\n",
    "                self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "agent = DQN(env)\n",
    "agent.run(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 00010: Mean Time 200.000, Max Time 200\n",
      "Episode 00020: Mean Time 200.000, Max Time 200\n",
      "Episode 00030: Mean Time 200.000, Max Time 200\n",
      "Episode 00040: Mean Time 200.000, Max Time 200\n",
      "Episode 00050: Mean Time 199.600, Max Time 200\n",
      "Episode 00060: Mean Time 200.000, Max Time 200\n",
      "Episode 00070: Mean Time 200.000, Max Time 200\n",
      "Episode 00080: Mean Time 200.000, Max Time 200\n",
      "Episode 00090: Mean Time 199.900, Max Time 200\n",
      "Episode 00100: Mean Time 200.000, Max Time 200\n"
     ]
    }
   ],
   "source": [
    "# evaluation\n",
    "agent.run(100,eval_mode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN on raw pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "\n",
    "network input: `state (raw pixels)` output: `Q(a|state)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, h, w, outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "\n",
    "        # Number of Linear input connections depends on output of conv2d layers\n",
    "        # and therefore the input image size, so compute it.\n",
    "        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        linear_input_size = convw * convh * 32\n",
    "        self.head = nn.Linear(linear_input_size, outputs)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        return self.head(x.view(x.size(0), -1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
