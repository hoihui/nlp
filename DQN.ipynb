{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Catch](https://gist.github.com/EderSantana/c7222daa328f0e885093) MLP on raw pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Catch(object): # 1 game is 1 fruit dropped from top to bottom\n",
    "    def __init__(self, grid_size=10):\n",
    "        self.grid_size = grid_size\n",
    "        self.basketSize = 0 # actually this is 2*basket_size+1\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        n = np.random.randint(0, self.grid_size-1)  # starting fruit_col\n",
    "        m = np.random.randint(1, self.grid_size-2)  # starting basket col\n",
    "        self.state = np.asarray([0, n, m])          # [fruit_row, fruit_col, basket]\n",
    "        return self.observe()\n",
    "    \n",
    "    def _get_reward(self):   # inc/dec score only if fruit has dropped to bottom\n",
    "        fruit_row, fruit_col, basket = self.state\n",
    "        if fruit_row == self.grid_size-1:\n",
    "            return 1 if abs(fruit_col - basket) <= self.basketSize else -1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def _is_over(self):    # game over if fruit dropped to bottom\n",
    "        return (self.state[0] == self.grid_size-1)\n",
    "    \n",
    "    def observe(self):\n",
    "        im_size = (self.grid_size, self.grid_size)\n",
    "        state = self.state\n",
    "        canvas = np.zeros(im_size)\n",
    "        canvas[self.state[0], self.state[1]] = 1                                         # draw fruit\n",
    "        canvas[-1, self.state[2]-self.basketSize:self.state[2] + self.basketSize+1] = 1  # draw basket\n",
    "        return canvas.flatten()\n",
    "    \n",
    "    def step(self, action):\n",
    "        if action == 0:   action = -1 # move left\n",
    "        elif action == 1: action =  0 # stay\n",
    "        else:             action =  1 # move right\n",
    "        f0, f1, basket = self.state\n",
    "        new_basket = min(max(self.basketSize, basket + action), self.grid_size-self.basketSize)\n",
    "        f0 += 1                       # fruit dropped by one pixel\n",
    "        out = np.asarray([f0, f1, new_basket])\n",
    "        self.state = out\n",
    "        \n",
    "        return self.observe(), self._get_reward(), self._is_over() # returns whole canvas, R, done?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Store all intermediate states \n",
    "* after each step, train a random batch from memory\n",
    "* train target are the Q-values computed from current Q_tables predicted my the current model (`model.predict(state)`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 00099 | Loss 0.0254 | Win rate 0.120\n",
      "Episode 00199 | Loss 0.0261 | Win rate 0.140\n",
      "Episode 00299 | Loss 0.0083 | Win rate 0.260\n",
      "Episode 00399 | Loss 0.0084 | Win rate 0.280\n",
      "Episode 00499 | Loss 0.0073 | Win rate 0.250\n",
      "Episode 00599 | Loss 0.0296 | Win rate 0.230\n",
      "Episode 00699 | Loss 0.0170 | Win rate 0.270\n",
      "Episode 00799 | Loss 0.0048 | Win rate 0.290\n",
      "Episode 00899 | Loss 0.0016 | Win rate 0.310\n",
      "Episode 00999 | Loss 0.0016 | Win rate 0.340\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "class DQN(object):  #model+memory\n",
    "    def __init__(self,catchenv,\n",
    "                 gamma=.9, # gamma is reward decay in computing G_t=R_t+\\gamma*R_{t+1}+...\n",
    "                 max_memory=500):\n",
    "        self.memory = deque(maxlen=max_memory)\n",
    "        self.env = catchenv\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.model = tf.keras.models.Sequential([\n",
    "                         tf.keras.layers.Dense(100, input_shape=(self.env.grid_size**2,), activation='relu'),\n",
    "                         tf.keras.layers.Dense(100, activation='relu'),\n",
    "                         tf.keras.layers.Dense(3),  # [move_left, stay, move_right]\n",
    "                        ])\n",
    "        self.model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam())\n",
    "    \n",
    "    def get_epsilon(self, episode):\n",
    "        ###################### EPSILON to use for epsilon-greedy (probability of exploration)\n",
    "#         return 1/(1+e*.2)\n",
    "#         return max(.01, 0.995**episode)\n",
    "        return 0.1\n",
    "        #####################################################################################\n",
    "\n",
    "    def choose_action(self, state, epsilon):\n",
    "        if np.random.random() <= epsilon:\n",
    "            return np.random.randint(0, num_actions, size=1)\n",
    "        else:\n",
    "            return np.argmax(self.model.predict(state[np.newaxis])[0])\n",
    "        \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def replay_train(self, batch_size=50):\n",
    "        x_batch, y_batch = [], []  #(batch_size, n_pixels-whole canvas), (batch_size, num_actions)\n",
    "        minibatch = random.sample( self.memory, min(len(self.memory), batch_size))\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            y_target = self.model.predict(state[np.newaxis])[0]    # current estimation of the Q(s,a) (batch size 1)            \n",
    "            Q_sa = np.max(model.predict(next_state[np.newaxis])[0])# max among 3 rewards (corr 3 actions) \n",
    "            if done:\n",
    "                y_target[action] = reward \n",
    "            else: # reward_t + gamma * max_a' Q(s', a') ONLY for the action a that is executed; others remain unchanged\n",
    "                y_target[action] = reward + self.gamma * Q_sa\n",
    "            x_batch.append(state)\n",
    "            y_batch.append(y_target)\n",
    "            \n",
    "        # learning rate handled by optimizer\n",
    "#       self.model.fit(np.array(x_batch), np.array(y_batch), batch_size=len(x_batch), verbose=0)  # returns history\n",
    "        return self.model.train_on_batch(np.array(x_batch), np.array(y_batch)) #returns current loss\n",
    "\n",
    "    def run(self,episdoes=1000,explore=True):\n",
    "        scores = deque(maxlen=episdoes//10) #store new episodes after previous print\n",
    "\n",
    "        for e in range(episdoes):\n",
    "            EPSILON = self.get_epsilon(e)*explore\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            R = 0  #total return at the end of episode\n",
    "            while not done:\n",
    "                action = self.choose_action(state, EPSILON)\n",
    "                next_state, reward, done = self.env.step(action)\n",
    "                R += reward\n",
    "                next_state = next_state\n",
    "                self.remember(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "            scores.append((R+1)/2)\n",
    "            \n",
    "            loss = self.replay_train()\n",
    "            if (e+1)%(episdoes/10) == 0:\n",
    "                print(f'Episode {e:05d} | Loss {loss:.4f} | Win rate {np.mean(scores):.3f}')\n",
    "\n",
    "env=Catch(grid_size = 10)\n",
    "agent = DQN(env)\n",
    "agent.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation (epsilon=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 00009 | Loss 0.0013 | Win rate 0.700\n",
      "Episode 00019 | Loss 0.0042 | Win rate 0.500\n",
      "Episode 00029 | Loss 0.0016 | Win rate 0.700\n",
      "Episode 00039 | Loss 0.0090 | Win rate 0.700\n",
      "Episode 00049 | Loss 0.0018 | Win rate 0.400\n",
      "Episode 00059 | Loss 0.0041 | Win rate 0.700\n",
      "Episode 00069 | Loss 0.0030 | Win rate 0.700\n",
      "Episode 00079 | Loss 0.0003 | Win rate 0.900\n",
      "Episode 00089 | Loss 0.0002 | Win rate 0.600\n",
      "Episode 00099 | Loss 0.0011 | Win rate 0.700\n"
     ]
    }
   ],
   "source": [
    "agent.run(100,explore=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib import animation, rc\n",
    "from IPython.display import HTML\n",
    "frames = []\n",
    "\n",
    "for e in range(100):\n",
    "    loss = 0.\n",
    "    env.reset()\n",
    "    done = False\n",
    "    state = env.observe()\n",
    "    frames.append(state.reshape(grid_size,grid_size))\n",
    "    while not done:\n",
    "        q = agent.model.predict(state[np.newaxis])  # q table at current state\n",
    "        action = np.argmax(q[0])\n",
    "        next_state, reward, done = env.step(action)\n",
    "        frames.append(next_state.reshape(grid_size,grid_size))\n",
    "        state = next_state\n",
    "# plt.imshow(frames[9],interpolation='none', cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#animation\n",
    "fig, ax = plt.subplots()\n",
    "im  = ax.imshow(np.random.random((grid_size,)*2),interpolation='none', cmap='gray')\n",
    "def init():\n",
    "#     im.set_array(np.random.random((grid_size,grid_size)))\n",
    "    return (im,)\n",
    "def animate(i):\n",
    "    im.set_array(frames[i])\n",
    "    return (im,)\n",
    "anim = animation.FuncAnimation(fig, animate, init_func=init,\n",
    "                               frames=len(frames), interval=50, blit=True)\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to files\n",
    "for i in range(len(frames)):\n",
    "    plt.imshow(frames[i],interpolation='none', cmap='gray')\n",
    "    plt.savefig(\"%03d.png\" % i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# [CartPole](https://gym.openai.com/envs/CartPole-v0/) MLP on 4-tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://gym.openai.com/evaluations/eval_EIcM1ZBnQW2LBaFN6FY65g/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode 199] - Mean survival time over last 100 episodes was 15.775\n",
      "[Episode 399] - Mean survival time over last 100 episodes was 14.82\n",
      "[Episode 599] - Mean survival time over last 100 episodes was 16.485\n",
      "[Episode 799] - Mean survival time over last 100 episodes was 26.53\n",
      "[Episode 999] - Mean survival time over last 100 episodes was 71.77\n",
      "[Episode 1199] - Mean survival time over last 100 episodes was 36.44\n",
      "[Episode 1399] - Mean survival time over last 100 episodes was 48.74\n",
      "[Episode 1599] - Mean survival time over last 100 episodes was 46.26\n",
      "[Episode 1799] - Mean survival time over last 100 episodes was 81.4\n",
      "[Episode 1999] - Mean survival time over last 100 episodes was 95.72\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import gym\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "\n",
    "class DQN(object): # model + memory\n",
    "    def __init__(self, env, # gym's env\n",
    "                 gamma=.99, # gamma is reward decay in computing G_t=R_t+\\gamma*R_{t+1}+...\n",
    "                 max_memory=1000):\n",
    "        self.memory = deque(maxlen=max_memory)\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.model = tf.keras.models.Sequential()\n",
    "        self.model.add(tf.keras.layers.Dense(24, input_shape=self.env.observation_space.shape, activation='relu'))\n",
    "        self.model.add(tf.keras.layers.Dense(24, activation='relu'))\n",
    "        self.model.add(tf.keras.layers.Dense(self.env.action_space.n, activation='linear'))\n",
    "        self.model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(.001)) #optimizer has its parameters\n",
    "    \n",
    "    def get_epsilon(self, episode):\n",
    "        ###################### EPSILON to use for epsilon-greedy (probability of exploration)\n",
    "#         return 1/(1+e*.1)  \n",
    "        return max(.01, 0.995**episode)\n",
    "        #####################################################################################\n",
    "\n",
    "    def choose_action(self, state, epsilon):\n",
    "        if np.random.random() <= epsilon:\n",
    "            return self.env.action_space.sample() \n",
    "        else:\n",
    "            return np.argmax(self.model.predict(state[np.newaxis])[0])\n",
    "        \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def replay_train(self, batch_size=50):\n",
    "        x_batch, y_batch = [], []\n",
    "        minibatch = random.sample( self.memory, min(len(self.memory), batch_size))\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            y_target = self.model.predict(state[np.newaxis])[0]\n",
    "            y_target[action] = reward if done else reward + self.gamma * np.max(self.model.predict(next_state[np.newaxis])[0])\n",
    "            x_batch.append(state)\n",
    "            y_batch.append(y_target)\n",
    "        \n",
    "        self.model.fit(np.array(x_batch), np.array(y_batch), batch_size=len(x_batch), verbose=0)\n",
    "\n",
    "    def run(self,episodes=2000,explore=True):\n",
    "        scores = deque(maxlen=episodes//10) #only store most recent 100\n",
    "\n",
    "        for e in range(episodes):\n",
    "            EPSILON = self.get_epsilon(e)*explore\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            R = 0  #total return at the end of episode\n",
    "            while not done:\n",
    "                action = self.choose_action(state, EPSILON)\n",
    "                next_state, reward, done, info = self.env.step(action)\n",
    "                R += reward\n",
    "                next_state = next_state\n",
    "                self.remember(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "            scores.append(R)\n",
    "            \n",
    "            if (e+1)%(episodes/10) == 0:\n",
    "                print(f'[Episode {e}] - Mean survival time over last 100 episodes was {np.mean(scores)}')\n",
    "\n",
    "            self.replay_train()\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "agent = DQN(env)\n",
    "agent.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
