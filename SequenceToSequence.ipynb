{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2.0.0-alpha0', '2.2.4-tf', False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install tensorflow==2.0.0-alpha0\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "tf.__version__,keras.__version__,tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Integer Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/encoder-decoder-attention-sequence-to-sequence-prediction-keras/\n",
    "\n",
    "- Input: sequence of 5 integers\n",
    "- Output: the first two integers, then padded with 0; or other criterions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqlen = 5\n",
    "maxint = 60\n",
    "batchsize = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 3, 57, 19,  5, 28],\n",
       "        [47, 31, 34, 21, 54]]), array([[ 1, 31, 41, 44, 59],\n",
       "        [14, 24, 35, 41, 59]]))"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_decode = lambda X: np.argmax(X,axis=X.ndim-1)\n",
    " \n",
    "def gen_pairs(batchsize=2, onehot=True):\n",
    "    while True:\n",
    "        X = np.random.randint(0, maxint-1,(batchsize,seqlen),dtype=int)\n",
    "        y = X.copy(); \n",
    "#         y = np.hstack((y[:,:2],np.zeros((batchsize,3),dtype=int)))  # original: first 2 integers\n",
    "#         y = np.clip(y,maxint//3,2*maxint//3)\n",
    "#         y.sort(axis=1)\n",
    "        y = np.cumsum(y,1)/np.tile(y.sum(1),(seqlen,1)).T*(maxint-1); y = y.astype(int)\n",
    "        \n",
    "        if onehot: #https://stackoverflow.com/a/49217762\n",
    "            I = np.eye(maxint)\n",
    "            X = I[X.reshape(-1)].reshape(*X.shape,maxint)\n",
    "            y = I[y.reshape(-1)].reshape(*y.shape,maxint)\n",
    "        \n",
    "        yield X,y\n",
    "\n",
    "X,y = next(gen_pairs(2,False))\n",
    "X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Model w/o Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define \n",
    "cellSize = 100\n",
    "model = tf.keras.models.Sequential()\n",
    "# model.add(keras.layers.LSTM(cellSize, input_shape=(seqlen, maxint)))  #original\n",
    "# model.add(keras.layers.RepeatVector(seqlen))                          #original\n",
    "# model.add(keras.layers.LSTM(cellSize, return_sequences=True))         #original\n",
    "model.add(keras.layers.Bidirectional(keras.layers.LSTM(cellSize, return_sequences=True, input_shape=(seqlen, maxint))))\n",
    "model.add(keras.layers.Bidirectional(keras.layers.LSTM(cellSize, return_sequences=True)))\n",
    "model.add(keras.layers.TimeDistributed(keras.layers.Dense(maxint,\n",
    "                                                          activation='softmax')\n",
    "                                      ))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2968 - acc: 0.8760 - val_loss: 0.3174 - val_acc: 0.8634\n",
      "Epoch 2/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2891 - acc: 0.8790 - val_loss: 0.3023 - val_acc: 0.8734\n",
      "Epoch 3/1000\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.2782 - acc: 0.8834 - val_loss: 0.2819 - val_acc: 0.8812\n",
      "Epoch 4/1000\n",
      "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2772 - acc: 0.8832 - val_loss: 0.2454 - val_acc: 0.8971\n",
      "Epoch 5/1000\n",
      "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2733 - acc: 0.8848 - val_loss: 0.2796 - val_acc: 0.8813\n",
      "Epoch 6/1000\n",
      "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2710 - acc: 0.8860 - val_loss: 0.2937 - val_acc: 0.8768\n",
      "Epoch 7/1000\n",
      "1000/1000 [==============================] - 12s 12ms/step - loss: 0.2604 - acc: 0.8909 - val_loss: 0.2790 - val_acc: 0.8801\n",
      "Epoch 00007: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7ff3a10a9320>"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train\n",
    "import tempfile\n",
    "_,weightsfile=tempfile.mkstemp()\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "es=EarlyStopping(monitor='val_acc', patience=3, verbose=1)\n",
    "mc=ModelCheckpoint(weightsfile,monitor='val_acc',save_best_only=True,save_weights_only=True,verbose=0)\n",
    "\n",
    "model.fit_generator(gen_pairs(32, True), steps_per_epoch=1000, epochs=1000,callbacks=[es,mc],\n",
    "                    validation_data=gen_pairs(32, True),validation_steps=100)\n",
    "model.load_weights(weightsfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5846%\n",
      "True:[ 0  4 20 43 59], Prediction:[ 0  5 21 43 59]\n",
      "True:[ 2 11 30 58 59], Prediction:[ 2 12 30 58 59]\n",
      "True:[11 31 37 49 59], Prediction:[10 31 38 49 59]\n",
      "True:[21 22 39 42 59], Prediction:[20 22 39 43 59]\n",
      "True:[14 25 40 52 59], Prediction:[14 25 40 51 59]\n",
      "True:[23 37 38 53 59], Prediction:[23 36 37 53 59]\n",
      "True:[10 23 49 50 59], Prediction:[10 22 50 50 59]\n",
      "True:[12 19 35 53 59], Prediction:[12 18 35 53 59]\n",
      "True:[ 5 13 35 53 59], Prediction:[ 4 13 35 53 59]\n",
      "True:[ 2 29 29 48 59], Prediction:[ 2 29 29 49 59]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "X,y = next(gen_pairs(10000,True))\n",
    "yhat = model.predict(X, verbose=0)\n",
    "wrong = list(map(np.any, one_hot_decode(y)-one_hot_decode(yhat))) #np.any returns true (1) if any element is nonzero\n",
    "print(f'Accuracy: {1-sum(wrong)/y.shape[0]}%') # different from acc during training as we count the WHOLE sequence as right/wrong here\n",
    "\n",
    "for i in np.where(wrong)[0][:10]:   # output examples that are wrong\n",
    "    print(f'True:{one_hot_decode(y[i])}, Prediction:{one_hot_decode(yhat[i])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Model w/ Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translate dates to standard format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.coursera.org/learn/nlp-sequence-models/notebook/npjGi/neural-machine-translation-with-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# language translation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/alpha/tutorials/text/nmt_with_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib, os, tempfile, zipfile\n",
    "os.chdir(tempfile.gettempdir())\n",
    "urllib.request.urlretrieve('http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip','spa-eng.zip')\n",
    "zipfile.ZipFile('spa-eng.zip').extractall()\n",
    "# ./spa-eng/spa.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> may i borrow this book ? <end>\n",
      "b'<start> \\xc2\\xbf puedo tomar prestado este libro ? <end>'\n"
     ]
    }
   ],
   "source": [
    "import unicodedata, re\n",
    "\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "\n",
    "    # \"he is a boy.\" => \"he is a boy .\" (https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation)\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "    w = w.rstrip().strip()\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w\n",
    "\n",
    "print(preprocess_sentence(\"May I borrow this book?\"))\n",
    "print(preprocess_sentence(\"¿Puedo tomar prestado este libro?\").encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
