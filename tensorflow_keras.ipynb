{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/hoihui/pkgs/blob/master/tensorflow_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2.2.4-tf', '2.0.0-alpha0', False)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install tensorflow==2.0.0-alpha0\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "keras.__version__,tf.__version__,tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1241, shape=(2, 3), dtype=float32, numpy=\n",
       "array([[1., 1., 1.],\n",
       "       [1., 1., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=tf.Variable(1,dtype=float)    #init scalar\n",
    "tf.zeros([2,3])\n",
    "tf.ones([2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.assign(2.)       # so that memory location is overwritten in-place\n",
    "x.assign_add(0.1)  # add a number from current val\n",
    "x.assign_sub(0.1)  # subtract a number from current val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=950, shape=(), dtype=int32, numpy=2>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.add([1, 2], [3, 4])\n",
    "tf.square(5)\n",
    "tf.reduce_sum([1, 2, 3],axis=None)   #  sum of elements across dimensions\n",
    "tf.reduce_mean([1, 2, 3],axis=None)  # mean of elements across dimensions\n",
    "# x, x.item()            # convert to python scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Alegra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=25, shape=(1, 2), dtype=int32, numpy=array([[2, 3]], dtype=int32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.matmul([[1]], [[2, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-4.0629369e-01  4.4463485e-01  1.3414291e+00 -2.3801818e-01\n",
      "   1.5644118e+00]\n",
      " [ 5.3685808e-01 -1.0169245e+00 -1.0178088e-03  1.1458488e+00\n",
      "  -1.7978226e+00]], shape=(2, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "features = tf.random.normal((2,5)) #tensor of size (2,5)\n",
    "# weights = \n",
    "# bias = tf.randn((1,1))\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=335063, shape=(), dtype=float32, numpy=0.1717741>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/(1 + tf.exp(tf.reduce_sum(features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=335065, shape=(2, 5), dtype=float32, numpy=\n",
       "array([[0.05733178, 0.13426073, 0.32917118, 0.06783855, 0.41139778],\n",
       "       [0.2680354 , 0.0566752 , 0.1565293 , 0.4928032 , 0.02595693]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.softmax(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.48153564 -0.04361424]\n",
      " [-0.28472823 -0.22350502]] [[-1.9261426  -0.17445697]\n",
      " [-1.1389129  -0.8940201 ]]\n"
     ]
    }
   ],
   "source": [
    "x = tf.random.normal((2,2))\n",
    "print(x.numpy()/2,x.numpy()*2)\n",
    "with tf.GradientTape(persistent=True) as t:  #persistent=True so that t.gradient can be called multiple times\n",
    "    t.watch(x)\n",
    "    y = x*x\n",
    "    z = tf.reduce_mean(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prove $\n",
    "\\frac{\\partial z}{\\partial x} = \\frac{\\partial}{\\partial x}\\left[\\frac{1}{n}\\sum_i^n x_i^2\\right] = \\frac{x}{2}\n",
    "$ and $\\frac{\\partial y}{\\partial x} = 2x$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.48153564 -0.04361424]\n",
      " [-0.28472823 -0.22350502]], shape=(2, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[-1.9261426  -0.17445697]\n",
      " [-1.1389129  -0.8940201 ]], shape=(2, 2), dtype=float32)\n",
      "[<tf.Tensor: id=83420, shape=(2, 2), dtype=float32, numpy=\n",
      "array([[-0.48153564, -0.04361424],\n",
      "       [-0.28472823, -0.22350502]], dtype=float32)>, <tf.Tensor: id=83417, shape=(2, 2), dtype=float32, numpy=\n",
      "array([[0.25, 0.25],\n",
      "       [0.25, 0.25]], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "print(t.gradient(z,x))  #=dz/dx\n",
    "print(t.gradient(y,x))  #=dy/dx\n",
    "print(t.gradient(z,[x,y]))  # can differentiate wrt multiple variables\n",
    "del t  # if persistent=True, remember to release the ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.608767 29.608765416240203\n",
      "18.84954 18.84954071044922\n"
     ]
    }
   ],
   "source": [
    "# Higher order gradients using multiple GradientTape's\n",
    "x = tf.Variable(3.14159)\n",
    "\n",
    "with tf.GradientTape() as t:\n",
    "    with tf.GradientTape() as t2:\n",
    "        y = x * x * x\n",
    "    dy_dx = t2.gradient(y, x)\n",
    "d2y_dx2 = t.gradient(dy_dx, x)\n",
    "\n",
    "print(dy_dx.numpy(), 3*x.numpy()**2) #d(x^3)/dx=3x^2\n",
    "print(d2y_dx2.numpy(), 6*x.numpy())  #d(3x^2)/dx=6x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To/from Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is memory shared?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.07790517 0.87036976]\n",
      " [0.05034188 0.88710919]], shape=(2, 2), dtype=float64)\n",
      "[[0.07790517 0.87036976]\n",
      " [0.05034188 0.88710919]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.random.rand(2,2)\n",
    "b = tf.add(a,0)\n",
    "print(b)\n",
    "print(b.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.07790517, 0.87036976],\n",
       "       [0.05034188, 0.88710919]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b*=2 # inplace multiplication\n",
    "a # shows that memory is shared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [TF function and AutoGraph](https://www.tensorflow.org/alpha/tutorials/eager/tf_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Keras datasets](https://keras.io/datasets/) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all uint8:\n",
    "* mnist: X.shape is (N, 28, 28); y.shape is (N,) from 0 to 9\n",
    "* cifar10: X.shape is (N, 3, 32, 32); y.shape is (N,) from 0 to 9\n",
    "* cifar100: X.shape is (N, 3, 32, 32); y.shape is (N,) of string labels\n",
    " * label_mode: \"fine\" or \"coarse\".\n",
    "* imdb: \n",
    "* reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "from keras.utils import np_utils\n",
    "Y_train = np_utils.to_categorical(y_train, 10)# convert class vectors to binary class matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conform with Convolution2D's requirements for different backends ?\n",
    "# X_train_ = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "# X_test_ = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "# input_shape = (img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/guide/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_tensors = tf.data.Dataset.from_tensor_slices([1, 2, 3, 4, 5, 6])\n",
    "\n",
    "import tempfile\n",
    "_, filename = tempfile.mkstemp()\n",
    "with open(filename, 'w') as f:    f.write(\"\"\"Line 1\\nLine 2\\nLine 3\\n  \"\"\")\n",
    "ds_file = tf.data.TextLineDataset(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformation .map/.batch/.shuffle\n",
    "ds_tensors = ds_tensors.map(tf.square).shuffle(2).batch(2)\n",
    "ds_file = ds_file.batch(2)  #loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elements of ds_tensors:\n",
      "tf.Tensor([4 1], shape=(2,), dtype=int32)\n",
      "tf.Tensor([ 9 25], shape=(2,), dtype=int32)\n",
      "tf.Tensor([36 16], shape=(2,), dtype=int32)\n",
      "\n",
      "Elements in ds_file:\n",
      "tf.Tensor([b'Line 1' b'Line 2'], shape=(2,), dtype=string)\n",
      "tf.Tensor([b'Line 3' b'  '], shape=(2,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "#iteration\n",
    "print('Elements of ds_tensors:')\n",
    "for x in ds_tensors:    print(x)\n",
    "print('\\nElements in ds_file:')\n",
    "for x in ds_file:    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloaded csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local copy of the dataset file: /Users/hoi/.keras/datasets/iris_training.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "url = \"https://storage.googleapis.com/download.tensorflow.org/data/iris_training.csv\"\n",
    "\n",
    "train_fp = keras.utils.get_file(fname=os.path.basename(train_dataset_url),\n",
    "                                origin=train_dataset_url)\n",
    "\n",
    "print(\"Local copy of the dataset file: {}\".format(train_fp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lazy loader\n",
    "train_Xy = tf.data.experimental.make_csv_dataset(\n",
    "    train_fp,\n",
    "    batch_size=5,\n",
    "    column_names=['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species'],\n",
    "    label_name='species',\n",
    "    num_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('sepal_length', <tf.Tensor: id=83673, shape=(5,), dtype=float32, numpy=array([4.7, 4.9, 7.7, 6. , 5.8], dtype=float32)>), ('sepal_width', <tf.Tensor: id=83674, shape=(5,), dtype=float32, numpy=array([3.2, 2.4, 3.8, 2.7, 2.6], dtype=float32)>), ('petal_length', <tf.Tensor: id=83671, shape=(5,), dtype=float32, numpy=array([1.6, 3.3, 6.7, 5.1, 4. ], dtype=float32)>), ('petal_width', <tf.Tensor: id=83672, shape=(5,), dtype=float32, numpy=array([0.2, 1. , 2.2, 1.6, 1.2], dtype=float32)>)]) \n",
      " tf.Tensor([0 1 2 1 1], shape=(5,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# sample\n",
    "X, y = next(iter(train_Xy))  #returns a batch\n",
    "print(X,'\\n',y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[4.7 3.2 1.6 0.2]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.8 2.6 4.  1.2]], shape=(5, 4), dtype=float32) \n",
      " tf.Tensor([0 1 2 1 1], shape=(5,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Convert from OrderedDict to a tensor\n",
    "train_Xy = train_Xy.map(lambda X,y: (tf.stack(list(X.values()), axis=1),y) )\n",
    "X, y = next(iter(train_Xy))  #returns a batch\n",
    "print(X,'\\n',y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prebuilt <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers\">Layers</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'dense_5/kernel:0' shape=(5, 10) dtype=float32, numpy=\n",
       " array([[-0.23853138, -0.47463644, -0.40410075,  0.3337345 ,  0.05842251,\n",
       "         -0.2110303 , -0.37228498, -0.52343273, -0.1935311 ,  0.31415182],\n",
       "        [-0.47757697,  0.1872893 ,  0.36996263,  0.53191036, -0.41023302,\n",
       "         -0.36609358, -0.62482274, -0.32694978, -0.46359128, -0.11053449],\n",
       "        [ 0.5979156 , -0.6117383 ,  0.52128893, -0.20676479, -0.3525084 ,\n",
       "          0.2735374 , -0.5580343 , -0.06586772,  0.1952563 ,  0.3177566 ],\n",
       "        [-0.04371983,  0.25442594,  0.5144964 ,  0.03669256, -0.4263901 ,\n",
       "         -0.47154313,  0.18181372, -0.14908463, -0.6010451 ,  0.37639374],\n",
       "        [-0.60666305,  0.17177355, -0.03292   , -0.6297653 , -0.30985603,\n",
       "          0.22744763,  0.08481169,  0.03695041, -0.4091086 ,  0.21218657]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'dense_5/bias:0' shape=(10,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = tf.keras.layers.Dense(10)\n",
    "layer(tf.zeros([10, 5]))  #forward an input tensor\n",
    "layer.variables # ==.weights; outputs both \"weights\" and biases\n",
    "layer.kernel, layer.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'my_dense_layer_1/kernel:0' shape=(5, 10) dtype=float32, numpy=\n",
       " array([[-0.00734526,  0.10645199, -0.5941866 , -0.25046846,  0.22676909,\n",
       "         -0.55612785, -0.2539965 ,  0.07490993, -0.00648773,  0.41902   ],\n",
       "        [ 0.40334123,  0.23245806, -0.3132097 ,  0.1817038 ,  0.17335898,\n",
       "          0.05050379,  0.1570611 , -0.08801335,  0.09774137,  0.41143197],\n",
       "        [ 0.33316398, -0.00309026,  0.20375234, -0.53965926, -0.50916684,\n",
       "         -0.2689581 , -0.49531913, -0.16076416,  0.10149634, -0.2719392 ],\n",
       "        [ 0.08511382, -0.0683853 , -0.2273733 , -0.20938778, -0.2959872 ,\n",
       "          0.0690949 , -0.40579018, -0.44823477, -0.40371758, -0.5016375 ],\n",
       "        [ 0.606834  ,  0.3403492 ,  0.08920056, -0.08068699,  0.60987383,\n",
       "          0.07875824, -0.15887764,  0.28663874,  0.5244569 ,  0.14796907]],\n",
       "       dtype=float32)>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyDenseLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_outputs):     # does not know the input shape yet\n",
    "        super(MyDenseLayer, self).__init__()\n",
    "        self.num_outputs = num_outputs\n",
    "\n",
    "    def build(self, input_shape):        # called 1st when forwarding; knows the input shape already\n",
    "        self.kernel = self.add_variable(\"kernel\",\n",
    "                                        shape=[int(input_shape[-1]),\n",
    "                                               self.num_outputs])\n",
    "\n",
    "    def call(self, inp):                 # called 2nd when forwarding\n",
    "        return tf.matmul(inp, self.kernel)\n",
    "\n",
    "layer = MyDenseLayer(10)\n",
    "layer(tf.zeros([10, 5]))  # forwarding: first call .build(), then call .call()\n",
    "layer.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]]], shape=(1, 2, 3, 3), dtype=float32)\n",
      "['resnet_identity_block/conv2d/kernel:0', 'resnet_identity_block/conv2d/bias:0', 'resnet_identity_block/batch_normalization_v2/gamma:0', 'resnet_identity_block/batch_normalization_v2/beta:0', 'resnet_identity_block/conv2d_1/kernel:0', 'resnet_identity_block/conv2d_1/bias:0', 'resnet_identity_block/batch_normalization_v2_1/gamma:0', 'resnet_identity_block/batch_normalization_v2_1/beta:0', 'resnet_identity_block/conv2d_2/kernel:0', 'resnet_identity_block/conv2d_2/bias:0', 'resnet_identity_block/batch_normalization_v2_2/gamma:0', 'resnet_identity_block/batch_normalization_v2_2/beta:0']\n"
     ]
    }
   ],
   "source": [
    "# composing multiple layers to a usable layer\n",
    "class ResnetIdentityBlock(tf.keras.Model):\n",
    "    def __init__(self, kernel_size, filters):\n",
    "        super(ResnetIdentityBlock, self).__init__(name='')\n",
    "        filters1, filters2, filters3 = filters\n",
    "\n",
    "        self.conv2a = tf.keras.layers.Conv2D(filters1, (1, 1))\n",
    "        self.bn2a = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "        self.conv2b = tf.keras.layers.Conv2D(filters2, kernel_size, padding='same')\n",
    "        self.bn2b = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "        self.conv2c = tf.keras.layers.Conv2D(filters3, (1, 1))\n",
    "        self.bn2c = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "    def call(self, input_tensor, training=False):\n",
    "        x = self.conv2a(input_tensor)\n",
    "        x = self.bn2a(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "\n",
    "        x = self.conv2b(x)\n",
    "        x = self.bn2b(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "\n",
    "        x = self.conv2c(x)\n",
    "        x = self.bn2c(x, training=training)\n",
    "\n",
    "        x += input_tensor\n",
    "        return tf.nn.relu(x)\n",
    "\n",
    "block = ResnetIdentityBlock(1, [1, 2, 3])\n",
    "print(block(tf.zeros([1, 2, 3, 3])))\n",
    "print([x.name for x in block.trainable_variables])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "model.add(Dense(512, input_shape=(96,96)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://keras.io/layers/core/#dense\">Dense</a>(output_dim, input_shape, init, W_regularizer)\n",
    "* output_dim: Linear channel's length\n",
    "* input_shape: only necessary on first layer; otherwise is inferred internally\n",
    "* init: \"glorot_normal\"\n",
    "* W_regularizer=l2(0.1) [from keras.regularizers import l2]\n",
    "* activation: if specified, equiv to addling the Activation layer as the next item\n",
    "\n",
    "<a href=\"https://keras.io/layers/core/#activation\">Activation</a>(<a href=\"https://keras.io/activations/\">activation</a>)\n",
    "* 'relu', 'softplus', 'softsign', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear', 'softmax'(3dim or 2dim)\n",
    "* custom function with 1 input & 1 output\n",
    "\n",
    "<a href=\"https://keras.io/layers/convolutional/#convolution2d\">Convolution2D</a>(nb_filter, nb_row, nb_col, init='glorot_uniform', activation='linear', weights=None, border_mode='valid', subsample=(1, 1), dim_ordering='default', W_regularizer=None, b_regularizer=None, activity_regularizer=None, W_constraint=None, b_constraint=None, bias=True)\n",
    "* Input shape: 4D tensor with shape: (samples, channels, rows, cols) if dim_ordering='th' or 4D tensor with shape: (samples, rows, cols, channels) if dim_ordering='tf'.\n",
    "\n",
    "MaxPooling2D(pool_size=pool_size)\n",
    "\n",
    "merge([tower_1, tower_2, tower_3], mode='concat', concat_axis=1)\n",
    "\n",
    "Flatten()\n",
    "\n",
    "<a href=\"https://keras.io/layers/wrappers/#timedistributed\">TimeDistributed</a>(Dense(10,input_shape))\n",
    "* input must be at least 3D (batchSize x timeSteps x DenseInput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functional API ~nngraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://keras.io/getting-started/functional-api-guide/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, TimeDistributed\n",
    "from keras.layers import Dense, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(100,), dtype='int32')#Sequential\n",
    "x = Dense(64, activation='relu')(inputs)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "predictions = Dense(10, activation='softmax')(x)\n",
    "model = Model(input=inputs, output=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Input(shape=(dim1, dim2, dim3))\n",
    "a = TimeDistributed(LSTM(output_dim=10))(x) #apply \"LSTM\" model to each of dim1's elements\n",
    "b = LSTM(col_hidden)(a)\n",
    "prediction = Dense(nb_classes, activation='softmax')(b)\n",
    "model = Model(input=x, output=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(input=[main_input, auxiliary_input], output=[main_output, auxiliary_output]) #multi-input/outputs\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', loss_weights=[1., 0.2])\n",
    "model.fit([headline_data, additional_data], [labels, labels],nb_epoch=50, batch_size=32)\n",
    "# if names are provided for the inputs/outputs:\n",
    "model.compile(optimizer='rmsprop',loss={'main_output': 'binary_crossentropy', 'aux_output': 'binary_crossentropy'},\n",
    "                          loss_weights={'main_output': 1., 'aux_output': 0.2})\n",
    "model.fit({'main_input': headline_data, 'aux_input': additional_data},\n",
    "          {'main_output': labels, 'aux_output': labels},\n",
    "          nb_epoch=50, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Model Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()  #human-readable\n",
    "model.output_shape # shape of output\n",
    "model.trainable_variables  #list of trainable variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "model.save_weights(\"model.h5\", overwrite=True)\n",
    "with open(\"model.json\", \"w\") as outfile:  #uncompiled\n",
    "    json.dump(model.to_json(), outfile)\n",
    "with open(\"model.json\", \"r\") as jfile:\n",
    "    model = model_from_json(json.load(jfile))\n",
    "model.load_weights(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(64, activation='relu', name=\"dense_one\"))\n",
    "model.get_layer(\"dense_one\")  #get layer by name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Layer Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = model.layers[0]  #extract the first layer from the list of model's layers\n",
    "layer.get_weights()[0] # For \"Dense\": 0 for weights, 1 for bias\n",
    "layer.set_weights(weights) # sets the weights of the layer from a list of Numpy arrays \n",
    "layer.get_config()# returns a dictionary containing the configuration of the layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a layer has a single node (i.e. if it isn't a shared layer), you can get its input tensor, output tensor, input shape and output shape via:\n",
    "\n",
    "    layer.input\n",
    "    layer.output\n",
    "    layer.input_shape\n",
    "    layer.output_shape\n",
    "\n",
    "If the layer has multiple nodes (see: the concept of layer node and shared layers), you can use the following methods:\n",
    "\n",
    "    layer.get_input_at(node_index)\n",
    "    layer.get_output_at(node_index)\n",
    "    layer.get_input_shape_at(node_index)\n",
    "    layer.get_output_shape_at(node_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compute d(loss)/d(param)\n",
    "with tf.GradientTape() as t:\n",
    "    train_loss = loss(model(inputs), outputs)                # forward pass with gradients recorded\n",
    "dlossdp = t.gradient(train_loss, model.trainable_variables)  # differentiate loss wrt parameters\n",
    "for i, param in enumerate(model.trainable_variables):\n",
    "    param.assign_sub(learning_alpha * dlossdp[i])  # simple gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9692693 1.9745175\n"
     ]
    }
   ],
   "source": [
    "class Model(object):                     # simple linear model y=wx+b\n",
    "    def __init__(self):\n",
    "        self.w = tf.Variable(5.0)\n",
    "        self.b = tf.Variable(0.0)\n",
    "    def __call__(self, x):               # defines behavior for model()\n",
    "        return self.w * x + self.b\n",
    "model = Model()\n",
    " \n",
    "def loss(pred, true):                    # simple mean square error\n",
    "    return tf.reduce_mean(tf.square(pred - true))\n",
    "\n",
    "inputs  = tf.random.normal(shape=[1000])\n",
    "noise   = tf.random.normal(shape=[1000])\n",
    "outputs = 3. * inputs + 2. + noise       # correct w & b\n",
    "\n",
    "for _ in range(1000):\n",
    "    with tf.GradientTape() as t:\n",
    "        train_loss = loss(model(inputs), outputs)\n",
    "    dlossdw, dlossdb = t.gradient(train_loss, [model.w, model.b])\n",
    "    model.w.assign_sub(.01 * dlossdw)    # simple gradient descent\n",
    "    model.b.assign_sub(.01 * dlossdb)\n",
    "    \n",
    "print(model.w.numpy(), model.b.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom grad + [Keras Optimizer](https://keras.io/optimizers/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.RMSprop(learning_rate=0.01)\n",
    "optimizer.apply_gradients(zip(grads, [model.w, model.b]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.974263 1.9795204\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.RMSprop(learning_rate=0.01)\n",
    "\n",
    "class Model(object):                     # simple linear model y=wx+b\n",
    "    def __init__(self):\n",
    "        self.w = tf.Variable(5.0)\n",
    "        self.b = tf.Variable(0.0)\n",
    "    def __call__(self, x):               # defines behavior for model()\n",
    "        return self.w * x + self.b\n",
    "model = Model()\n",
    " \n",
    "def loss(pred, true):                    # simple mean square error\n",
    "    return tf.reduce_mean(tf.square(pred - true))\n",
    "\n",
    "for _ in range(1000):\n",
    "    with tf.GradientTape() as t:\n",
    "        train_loss = loss(model(inputs), outputs)\n",
    "    grads = t.gradient(train_loss, [model.w, model.b])\n",
    "    optimizer.apply_gradients(zip(grads, [model.w, model.b]))\n",
    "    \n",
    "print(model.w.numpy(), model.b.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Compilation & Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.RMSprop(),\n",
    "              metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Compilation parameters](https://keras.io/models/model/):\n",
    "* [loss](https://keras.io/losses/):\n",
    "  * `categorical_crossentropy`: model output before sigmoid. y of shape (N,nclass), i.e. one-hot encoded\n",
    "  * `sparse_categorical_crossentropy`: model output before sigmoid. y of shape (N,), i.e. non-one-hot encoded\n",
    "  * `binary_crossentropy`: model output before sigmoid. y is a single number\n",
    "  * 'mse' (mean_squared_error)\n",
    "  * 'mae'\n",
    "  * 'mape', squared_hinge, hinge, , kld (kullback_leibler_divergence),\n",
    "  * 'poisson' [Mean of (predictions - targets * log(predictions))]\n",
    "  * 'cosine_proximity'\n",
    "* [optimizer](https://keras.io/optimizers/): common parameters are clipnorm (max allowed gradient 2-norm) and clipvalue (max allowed gradient 1-norm)\n",
    "  * 'sgd' or SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
    "  * 'adagrad' or Adagrad(lr=0.01, epsilon=1e-08, decay=0.0)\n",
    "  * 'rmsprop' or RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "  * 'adadelta' or Adadelta(lr=1.0, rho=0.95, epsilon=1e-08, decay=0.0)\n",
    "  * 'adam' or Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "  * 'adamax' or Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "  * 'nadam' or Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004)\n",
    "* [metrics](https://keras.io/metrics/): list of metrics to be evaluated by the model during training and testing\n",
    "  * 'accuracy'\n",
    "* sample_weight_mode: If you need to do timestep-wise sample weighting (2D weights), set this to \"temporal\".  None defaults to sample-wise weights (1D). If the model has multiple outputs, you can use a different  sample_weight_mode on each output by passing a dictionary or a list of modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with early stopping...\n",
    "import tempfile\n",
    "_,weightsfile=tempfile.mkstemp()\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "es=EarlyStopping(monitor='val_loss', patience=patience, verbose=1)\n",
    "mc=ModelCheckpoint(weightsfile,monitor='val_loss',save_best_only=True,save_weights_only=True,verbose=0)\n",
    "\n",
    "history = model.fit(x,y,batch_size=32, epochs=10,\n",
    "                    callbacks=[es,mc],\n",
    "                    verbose=1,validation_split=0.0,\n",
    "                    validation_data=None, shuffle=True, class_weight=None, sample_weight=None)\n",
    "model.load_weights(weightsfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Parameters](https://keras.io/models/model/):\n",
    "\n",
    "* `x`: Numpy array of training data, or list of Numpy arrays if the model has multiple inputs. If all inputs in the model are named, you can also pass a dictionary mapping input names to Numpy arrays.\n",
    "* `y`: Numpy array of target data, or list of Numpy arrays if the model has multiple outputs. If all outputs in the model are named, you can also pass a dictionary mapping output names to Numpy arrays.\n",
    "* `batch_size`: integer. Number of samples per gradient update.\n",
    "* `epochs`: integer, the number of times to iterate over the training data arrays.\n",
    "* `verbose`: 0, 1, or 2. Verbosity mode. 0 = silent, 1 = verbose, 2 = one log line per epoch.\n",
    "* `validation_split`: float between 0 and 1: fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch.\n",
    "* `validation_data`: data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. This could be a tuple (x_val, y_val) or a tuple (val_x, val_y, val_sample_weights).\n",
    "* `shuffle`: boolean, whether to shuffle the training data before each epoch.\n",
    "* `class_weight`: optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class.\n",
    "* `sample_weight`: optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. In this case you should make sure to specify sample_weight_mode=\"temporal\" in compile().\n",
    "* `callbacks`: list of callbacks to be called during training. See callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General model() class\n",
    "# https://keras.io/models/model/\n",
    "evaluate(self, x, y, batch_size=32, verbose=1, sample_weight=None)\n",
    "predict(self, x, batch_size=32, verbose=0)\n",
    "train_on_batch(self, x, y, sample_weight=None, class_weight=None)\n",
    "test_on_batch(self, x, y, sample_weight=None)\n",
    "predict_on_batch(self, x)\n",
    "fit_generator(self, generator, samples_per_epoch, nb_epoch, verbose=1, callbacks=[], validation_data=None, nb_val_samples=None, class_weight={}, max_q_size=10, nb_worker=1, pickle_safe=False)\n",
    "evaluate_generator(self, generator, val_samples, max_q_size=10, nb_worker=1, pickle_safe=False)\n",
    "predict_generator(self, generator, val_samples, max_q_size=10, nb_worker=1, pickle_safe=False)\n",
    "get_layer(self, name=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential() only ?\n",
    "# https://keras.io/models/sequential/\n",
    "predict_classes(self, x, batch_size=32, verbose=1)\n",
    "predict_proba(self, x, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model.predict_classes(x) #Sequential() only\n",
    "pandas.crosstab(y_hat, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sklearn-style interface ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0521 18:30:53.955404 4691105216 estimator.py:1799] Using temporary folder as model directory: /var/folders/r7/n9dny1wj46q8njz2gds66kqr0000gp/T/tmpsr2239n8\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets, metrics\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "def train_input_fn(features, labels, batch_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
    "    return dataset.shuffle(1000).repeat().batch(batch_size)\n",
    "train_x = {\n",
    "    '0': iris.data[:, 0],\n",
    "    '1': iris.data[:, 1],\n",
    "    '2': iris.data[:, 2],\n",
    "    '3': iris.data[:, 3],\n",
    "}\n",
    "\n",
    "my_feature_columns = []\n",
    "for key in train_x.keys():\n",
    "    my_feature_columns.append(tf.feature_column.numeric_column(key=key))\n",
    "\n",
    "clf = tf.estimator.DNNClassifier(\n",
    "    feature_columns=my_feature_columns,\n",
    "    hidden_units=[5],    # 1 hidden layers of 5 nodes\n",
    "    n_classes=3)\n",
    "\n",
    "# clf.train(input_fn=lambda: train_input_fn(train_x, iris.target, 32), steps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result = clf.evaluate(\n",
    "    input_fn=lambda:train_input_fn(train_x, iris.target, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test set accuracy: {accuracy:0.3f}'.format(**eval_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "192px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
