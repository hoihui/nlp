{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/hoihui/pkgs/blob/master/tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2.2.4-tf', '2.0.0-alpha0', False)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install tensorflow==2.0.0-alpha0\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "keras.__version__,tf.__version__,tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1241, shape=(2, 3), dtype=float32, numpy=\n",
       "array([[1., 1., 1.],\n",
       "       [1., 1., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=tf.Variable(1,dtype=float)    #init scalar\n",
    "tf.zeros([2,3])\n",
    "tf.ones([2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.assign(2.)   # so that memory location is overwritten in-place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=950, shape=(), dtype=int32, numpy=2>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.add([1, 2], [3, 4])\n",
    "tf.square(5)\n",
    "tf.reduce_sum([1, 2, 3],axis=None)   #  sum of elements across dimensions\n",
    "tf.reduce_mean([1, 2, 3],axis=None)  # mean of elements across dimensions\n",
    "# x, x.item()            # convert to python scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Alegra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=25, shape=(1, 2), dtype=int32, numpy=array([[2, 3]], dtype=int32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.matmul([[1]], [[2, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.79568696  2.1542447  -0.16709523 -0.12590285 -1.0148194 ]\n",
      " [ 0.4772951   1.001898   -0.22688027 -0.36836457 -1.2996123 ]], shape=(2, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "features = tf.random.normal((2,5)) #tensor of size (2,5)\n",
    "# weights = \n",
    "# bias = tf.randn((1,1))\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=958, shape=(), dtype=float32, numpy=0.5902318>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/(1 + tf.exp(tf.reduce_sum(features)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.7093405  -0.6687705 ]\n",
      " [-0.19664665  0.26482087]] [[-2.837362  -2.675082 ]\n",
      " [-0.7865866  1.0592835]]\n"
     ]
    }
   ],
   "source": [
    "x = tf.random.normal((2,2))\n",
    "print(x.numpy()/2,x.numpy()*2)\n",
    "with tf.GradientTape(persistent=True) as t:  #persistent=True so that t.gradient can be called multiple times\n",
    "    t.watch(x)\n",
    "    y = x*x\n",
    "    z = tf.reduce_mean(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prove $\n",
    "\\frac{\\partial z}{\\partial x} = \\frac{\\partial}{\\partial x}\\left[\\frac{1}{n}\\sum_i^n x_i^2\\right] = \\frac{x}{2}\n",
    "$ and $\\frac{\\partial y}{\\partial x} = 2x$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.7093405  -0.6687705 ]\n",
      " [-0.19664665  0.26482087]], shape=(2, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[-2.837362  -2.675082 ]\n",
      " [-0.7865866  1.0592835]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(t.gradient(z,x))  #=dz/dx\n",
    "print(t.gradient(y,x))  #=dy/dx\n",
    "del t  # if persistent=True, remember to release the ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.608767 29.608765416240203\n",
      "18.84954 18.84954071044922\n"
     ]
    }
   ],
   "source": [
    "# Higher order gradients using multiple GradientTape's\n",
    "x = tf.Variable(3.14159)\n",
    "\n",
    "with tf.GradientTape() as t:\n",
    "    with tf.GradientTape() as t2:\n",
    "        y = x * x * x\n",
    "    dy_dx = t2.gradient(y, x)\n",
    "d2y_dx2 = t.gradient(dy_dx, x)\n",
    "\n",
    "print(dy_dx.numpy(), 3*x.numpy()**2) #d(x^3)/dx=3x^2\n",
    "print(d2y_dx2.numpy(), 6*x.numpy())  #d(3x^2)/dx=6x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To/from Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is memory shared?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.07790517 0.87036976]\n",
      " [0.05034188 0.88710919]], shape=(2, 2), dtype=float64)\n",
      "[[0.07790517 0.87036976]\n",
      " [0.05034188 0.88710919]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.random.rand(2,2)\n",
    "b = tf.add(a,0)\n",
    "print(b)\n",
    "print(b.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.07790517, 0.87036976],\n",
       "       [0.05034188, 0.88710919]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b*=2 # inplace multiplication\n",
    "a # shows that memory is shared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Keras datasets](https://keras.io/datasets/) (all uint8):\n",
    "* mnist: X.shape is (N, 28, 28); y.shape is (N,) from 0 to 9\n",
    "* cifar10: X.shape is (N, 3, 32, 32); y.shape is (N,) from 0 to 9\n",
    "* cifar100: X.shape is (N, 3, 32, 32); y.shape is (N,) of string labels\n",
    " * label_mode: \"fine\" or \"coarse\".\n",
    "* imdb: \n",
    "* reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "from keras.utils import np_utils\n",
    "Y_train = np_utils.to_categorical(y_train, 10)# convert class vectors to binary class matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conform with Convolution2D's requirements for different backends\n",
    "# X_train_ = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "# X_test_ = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "# input_shape = (img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/guide/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_tensors = tf.data.Dataset.from_tensor_slices([1, 2, 3, 4, 5, 6])\n",
    "\n",
    "import tempfile\n",
    "_, filename = tempfile.mkstemp()\n",
    "\n",
    "with open(filename, 'w') as f:\n",
    "    f.write(\"\"\"Line 1\\nLine 2\\nLine 3\\n  \"\"\")\n",
    "\n",
    "ds_file = tf.data.TextLineDataset(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformation map/batch/shuffle\n",
    "\n",
    "ds_tensors = ds_tensors.map(tf.square).shuffle(2).batch(2)\n",
    "\n",
    "ds_file = ds_file.batch(2)  #loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iteration\n",
    "\n",
    "print('Elements of ds_tensors:')\n",
    "for x in ds_tensors:\n",
    "    print(x)\n",
    "\n",
    "print('\\nElements in ds_file:')\n",
    "for x in ds_file:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prebuilt <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers\">Layers</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'dense_5/kernel:0' shape=(5, 10) dtype=float32, numpy=\n",
       " array([[-0.23853138, -0.47463644, -0.40410075,  0.3337345 ,  0.05842251,\n",
       "         -0.2110303 , -0.37228498, -0.52343273, -0.1935311 ,  0.31415182],\n",
       "        [-0.47757697,  0.1872893 ,  0.36996263,  0.53191036, -0.41023302,\n",
       "         -0.36609358, -0.62482274, -0.32694978, -0.46359128, -0.11053449],\n",
       "        [ 0.5979156 , -0.6117383 ,  0.52128893, -0.20676479, -0.3525084 ,\n",
       "          0.2735374 , -0.5580343 , -0.06586772,  0.1952563 ,  0.3177566 ],\n",
       "        [-0.04371983,  0.25442594,  0.5144964 ,  0.03669256, -0.4263901 ,\n",
       "         -0.47154313,  0.18181372, -0.14908463, -0.6010451 ,  0.37639374],\n",
       "        [-0.60666305,  0.17177355, -0.03292   , -0.6297653 , -0.30985603,\n",
       "          0.22744763,  0.08481169,  0.03695041, -0.4091086 ,  0.21218657]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'dense_5/bias:0' shape=(10,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = tf.keras.layers.Dense(10)\n",
    "layer(tf.zeros([10, 5]))  #forward an input tensor\n",
    "layer.variables # ==.weights; outputs both \"weights\" and biases\n",
    "layer.kernel, layer.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'my_dense_layer_1/kernel:0' shape=(5, 10) dtype=float32, numpy=\n",
       " array([[-0.00734526,  0.10645199, -0.5941866 , -0.25046846,  0.22676909,\n",
       "         -0.55612785, -0.2539965 ,  0.07490993, -0.00648773,  0.41902   ],\n",
       "        [ 0.40334123,  0.23245806, -0.3132097 ,  0.1817038 ,  0.17335898,\n",
       "          0.05050379,  0.1570611 , -0.08801335,  0.09774137,  0.41143197],\n",
       "        [ 0.33316398, -0.00309026,  0.20375234, -0.53965926, -0.50916684,\n",
       "         -0.2689581 , -0.49531913, -0.16076416,  0.10149634, -0.2719392 ],\n",
       "        [ 0.08511382, -0.0683853 , -0.2273733 , -0.20938778, -0.2959872 ,\n",
       "          0.0690949 , -0.40579018, -0.44823477, -0.40371758, -0.5016375 ],\n",
       "        [ 0.606834  ,  0.3403492 ,  0.08920056, -0.08068699,  0.60987383,\n",
       "          0.07875824, -0.15887764,  0.28663874,  0.5244569 ,  0.14796907]],\n",
       "       dtype=float32)>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyDenseLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_outputs):     # does not know the input shape yet\n",
    "        super(MyDenseLayer, self).__init__()\n",
    "        self.num_outputs = num_outputs\n",
    "\n",
    "    def build(self, input_shape):        # called 1st when forwarding; knows the input shape already\n",
    "        self.kernel = self.add_variable(\"kernel\",\n",
    "                                        shape=[int(input_shape[-1]),\n",
    "                                               self.num_outputs])\n",
    "\n",
    "    def call(self, inp):                 # called 2nd when forwarding\n",
    "        return tf.matmul(inp, self.kernel)\n",
    "\n",
    "layer = MyDenseLayer(10)\n",
    "layer(tf.zeros([10, 5]))  # forwarding: first call .build(), then call .call()\n",
    "layer.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]]], shape=(1, 2, 3, 3), dtype=float32)\n",
      "['resnet_identity_block/conv2d/kernel:0', 'resnet_identity_block/conv2d/bias:0', 'resnet_identity_block/batch_normalization_v2/gamma:0', 'resnet_identity_block/batch_normalization_v2/beta:0', 'resnet_identity_block/conv2d_1/kernel:0', 'resnet_identity_block/conv2d_1/bias:0', 'resnet_identity_block/batch_normalization_v2_1/gamma:0', 'resnet_identity_block/batch_normalization_v2_1/beta:0', 'resnet_identity_block/conv2d_2/kernel:0', 'resnet_identity_block/conv2d_2/bias:0', 'resnet_identity_block/batch_normalization_v2_2/gamma:0', 'resnet_identity_block/batch_normalization_v2_2/beta:0']\n"
     ]
    }
   ],
   "source": [
    "# composing multiple layers to a usable layer\n",
    "class ResnetIdentityBlock(tf.keras.Model):\n",
    "    def __init__(self, kernel_size, filters):\n",
    "        super(ResnetIdentityBlock, self).__init__(name='')\n",
    "        filters1, filters2, filters3 = filters\n",
    "\n",
    "        self.conv2a = tf.keras.layers.Conv2D(filters1, (1, 1))\n",
    "        self.bn2a = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "        self.conv2b = tf.keras.layers.Conv2D(filters2, kernel_size, padding='same')\n",
    "        self.bn2b = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "        self.conv2c = tf.keras.layers.Conv2D(filters3, (1, 1))\n",
    "        self.bn2c = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "    def call(self, input_tensor, training=False):\n",
    "        x = self.conv2a(input_tensor)\n",
    "        x = self.bn2a(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "\n",
    "        x = self.conv2b(x)\n",
    "        x = self.bn2b(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "\n",
    "        x = self.conv2c(x)\n",
    "        x = self.bn2c(x, training=training)\n",
    "\n",
    "        x += input_tensor\n",
    "        return tf.nn.relu(x)\n",
    "\n",
    "block = ResnetIdentityBlock(1, [1, 2, 3])\n",
    "print(block(tf.zeros([1, 2, 3, 3])))\n",
    "print([x.name for x in block.trainable_variables])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "model.add(Dense(512, input_shape=(96,96)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://keras.io/layers/core/#dense\">Dense</a>(output_dim, input_shape, init, W_regularizer)\n",
    "* output_dim: Linear channel's length\n",
    "* input_shape: only necessary on first layer; otherwise is inferred internally\n",
    "* init: \"glorot_normal\"\n",
    "* W_regularizer=l2(0.1) [from keras.regularizers import l2]\n",
    "* activation: if specified, equiv to addling the Activation layer as the next item\n",
    "\n",
    "<a href=\"https://keras.io/layers/core/#activation\">Activation</a>(<a href=\"https://keras.io/activations/\">activation</a>)\n",
    "* 'relu', 'softplus', 'softsign', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear', 'softmax'(3dim or 2dim)\n",
    "* custom function with 1 input & 1 output\n",
    "\n",
    "<a href=\"https://keras.io/layers/convolutional/#convolution2d\">Convolution2D</a>(nb_filter, nb_row, nb_col, init='glorot_uniform', activation='linear', weights=None, border_mode='valid', subsample=(1, 1), dim_ordering='default', W_regularizer=None, b_regularizer=None, activity_regularizer=None, W_constraint=None, b_constraint=None, bias=True)\n",
    "* Input shape: 4D tensor with shape: (samples, channels, rows, cols) if dim_ordering='th' or 4D tensor with shape: (samples, rows, cols, channels) if dim_ordering='tf'.\n",
    "\n",
    "MaxPooling2D(pool_size=pool_size)\n",
    "\n",
    "merge([tower_1, tower_2, tower_3], mode='concat', concat_axis=1)\n",
    "\n",
    "Flatten()\n",
    "\n",
    "<a href=\"https://keras.io/layers/wrappers/#timedistributed\">TimeDistributed</a>(Dense(10,input_shape))\n",
    "* input must be at least 3D (batchSize x timeSteps x DenseInput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functional API ~nngraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://keras.io/getting-started/functional-api-guide/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, TimeDistributed\n",
    "from keras.layers import Dense, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(100,), dtype='int32')#Sequential\n",
    "x = Dense(64, activation='relu')(inputs)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "predictions = Dense(10, activation='softmax')(x)\n",
    "model = Model(input=inputs, output=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Input(shape=(dim1, dim2, dim3))\n",
    "a = TimeDistributed(LSTM(output_dim=10))(x) #apply \"LSTM\" model to each of dim1's elements\n",
    "b = LSTM(col_hidden)(a)\n",
    "prediction = Dense(nb_classes, activation='softmax')(b)\n",
    "model = Model(input=x, output=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(input=[main_input, auxiliary_input], output=[main_output, auxiliary_output]) #multi-input/outputs\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', loss_weights=[1., 0.2])\n",
    "model.fit([headline_data, additional_data], [labels, labels],nb_epoch=50, batch_size=32)\n",
    "# if names are provided for the inputs/outputs:\n",
    "model.compile(optimizer='rmsprop',loss={'main_output': 'binary_crossentropy', 'aux_output': 'binary_crossentropy'},\n",
    "                          loss_weights={'main_output': 1., 'aux_output': 0.2})\n",
    "model.fit({'main_input': headline_data, 'aux_input': additional_data},\n",
    "          {'main_output': labels, 'aux_output': labels},\n",
    "          nb_epoch=50, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Model Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()  #human-readable\n",
    "model.output_shape # shape of output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "model.save_weights(\"model.h5\", overwrite=True)\n",
    "with open(\"model.json\", \"w\") as outfile:  #uncompiled\n",
    "    json.dump(model.to_json(), outfile)\n",
    "with open(\"model.json\", \"r\") as jfile:\n",
    "    model = model_from_json(json.load(jfile))\n",
    "model.load_weights(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(64, activation='relu', name=\"dense_one\"))\n",
    "model.get_layer(\"dense_one\")  #get layer by name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Layer Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = model.layers[0]  #extract the first layer from the list of model's layers\n",
    "layer.get_weights()[0] # For \"Dense\": 0 for weights, 1 for bias\n",
    "layer.set_weights(weights) # sets the weights of the layer from a list of Numpy arrays \n",
    "layer.get_config()# returns a dictionary containing the configuration of the layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a layer has a single node (i.e. if it isn't a shared layer), you can get its input tensor, output tensor, input shape and output shape via:\n",
    "\n",
    "    layer.input\n",
    "    layer.output\n",
    "    layer.input_shape\n",
    "    layer.output_shape\n",
    "\n",
    "If the layer has multiple nodes (see: the concept of layer node and shared layers), you can use the following methods:\n",
    "\n",
    "    layer.get_input_at(node_index)\n",
    "    layer.get_output_at(node_index)\n",
    "    layer.get_input_shape_at(node_index)\n",
    "    layer.get_output_shape_at(node_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Binay\n",
    "  * return a single number as output with `BCEWithLogitsLoss`, or\n",
    "  * return a probability as output with `BCELoss`\n",
    "* Multiclass\n",
    "  * return multiple numbers (eg. from `nn.Linear` output) with `CrossEntropyLoss`, or\n",
    "  * return multiple probabilities (eg. from `nn.LogSoftmax` output) with `NLLLoss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3173, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(784, 256),  #could also use class or OrderedDict\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(256, 10),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "images = images.view(64, -1)              #originally 64x1x28x28\n",
    "loss = criterion( model(images), labels)  #averaged over the batch\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first layer grad Before backward pass: \n",
      " None\n",
      "first layer grad After backward pass: \n",
      " tensor([[-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
      "        [-0.0058, -0.0058, -0.0058,  ..., -0.0058, -0.0058, -0.0058],\n",
      "        [-0.0059, -0.0059, -0.0059,  ..., -0.0059, -0.0059, -0.0059],\n",
      "        ...,\n",
      "        [-0.0012, -0.0012, -0.0012,  ..., -0.0012, -0.0012, -0.0012],\n",
      "        [-0.0015, -0.0015, -0.0015,  ..., -0.0015, -0.0015, -0.0015],\n",
      "        [ 0.0038,  0.0038,  0.0038,  ...,  0.0038,  0.0038,  0.0038]])\n"
     ]
    }
   ],
   "source": [
    "print('first layer grad Before backward pass: \\n', model[0].weight.grad)\n",
    "loss.backward()\n",
    "print('first layer grad After backward pass: \\n', model[0].weight.grad)\n",
    "nn.utils.clip_grad_norm_(model.parameters(), 5.0)   # gradient clipping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://pytorch.org/docs/master/optim.html\n",
    "* `optim.SGD`\n",
    "* `optim.Adam`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights -  Parameter containing:\n",
      "tensor([[ 0.0225,  0.0054, -0.0342,  ..., -0.0061,  0.0087,  0.0269],\n",
      "        [-0.0331,  0.0253,  0.0078,  ...,  0.0262,  0.0045, -0.0281],\n",
      "        [-0.0069,  0.0357, -0.0105,  ...,  0.0260,  0.0166,  0.0045],\n",
      "        ...,\n",
      "        [ 0.0043, -0.0352,  0.0160,  ...,  0.0255, -0.0023, -0.0298],\n",
      "        [ 0.0159, -0.0356, -0.0056,  ...,  0.0277,  0.0312,  0.0060],\n",
      "        [-0.0139,  0.0074,  0.0274,  ..., -0.0272, -0.0031, -0.0049]],\n",
      "       requires_grad=True)\n",
      "Gradient - tensor([[-7.0587e-05, -7.0587e-05, -7.0587e-05,  ..., -7.0587e-05,\n",
      "         -7.0587e-05, -7.0587e-05],\n",
      "        [-4.9133e-03, -4.9133e-03, -4.9133e-03,  ..., -4.9133e-03,\n",
      "         -4.9133e-03, -4.9133e-03],\n",
      "        [-6.5717e-03, -6.5717e-03, -6.5717e-03,  ..., -6.5717e-03,\n",
      "         -6.5717e-03, -6.5717e-03],\n",
      "        ...,\n",
      "        [-5.8080e-04, -5.8080e-04, -5.8080e-04,  ..., -5.8080e-04,\n",
      "         -5.8080e-04, -5.8080e-04],\n",
      "        [-1.1184e-03, -1.1184e-03, -1.1184e-03,  ..., -1.1184e-03,\n",
      "         -1.1184e-03, -1.1184e-03],\n",
      "        [ 2.5707e-03,  2.5707e-03,  2.5707e-03,  ...,  2.5707e-03,\n",
      "          2.5707e-03,  2.5707e-03]])\n"
     ]
    }
   ],
   "source": [
    "# One step\n",
    "optimizer.zero_grad()                   # gradients are accumulated -- clear first\n",
    "loss = criterion(model(images), labels) # forward a batch\n",
    "loss.backward()\n",
    "# print(model[0].weight.grad)           # get the grad of parameters\n",
    "print('Initial weights - ', model[0].weight)\n",
    "optimizer.step()                        # update params with accumulated grad\n",
    "print('Gradient -', model[0].weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.9859724194129139\n",
      "Training loss: 0.44501842942827546\n",
      "Training loss: 0.3749764274432461\n",
      "Training loss: 0.34324646042163437\n",
      "Training loss: 0.3232783184154456\n"
     ]
    }
   ],
   "source": [
    "# Full epochs\n",
    "for epoch in range(5):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:        \n",
    "        images = images.view(images.shape[0], -1)\n",
    "        \n",
    "        optimizer.zero_grad()                     # empty grad\n",
    "        loss = criterion(model(images), labels)   # forward\n",
    "        loss.backward()                           # backward        \n",
    "        optimizer.step()                          # update params with accumulated grad\n",
    "        \n",
    "        running_loss += loss.item()               # loss value itself??\n",
    "    else:\n",
    "        print(f\"Training loss: {running_loss/len(trainloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With early stopping\n",
    "import tempfile                    # tempfile for saving model\n",
    "_,weightsfile=tempfile.mkstemp()\n",
    "#####.....\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        torch.save(model.state_dict(), weightsfile)\n",
    "    else:\n",
    "        epochs_without_improvement+=1\n",
    "    if epochs_without_improvement>patience:\n",
    "        break\n",
    "model.load_state_dict(torch.load(weightsfile))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.cuda() #move to cuda\n",
    "# model.cpu()  #move back to cpu\n",
    "model.to(device)\n",
    "images=images.to(device)  #is not inplace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer=keras.optimizers.RMSprop(),metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Compilation parameters](https://keras.io/models/model/):\n",
    "* [loss](https://keras.io/losses/):\n",
    "  * `categorical_crossentropy`: model output before sigmoid. y of shape (N,nclass), i.e. one-hot encoded\n",
    "  * `sparse_categorical_crossentropy`: model output before sigmoid. y of shape (N,), i.e. non-one-hot encoded\n",
    "  * `binary_crossentropy`: model output before sigmoid. y is a single number\n",
    "  * 'mse' (mean_squared_error)\n",
    "  * 'mae'\n",
    "  * 'mape', squared_hinge, hinge, , kld (kullback_leibler_divergence),\n",
    "  * 'poisson' [Mean of (predictions - targets * log(predictions))]\n",
    "  * 'cosine_proximity'\n",
    "* [optimizer](https://keras.io/optimizers/): common parameters are clipnorm (max allowed gradient 2-norm) and clipvalue (max allowed gradient 1-norm)\n",
    "  * 'sgd' or SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
    "  * 'adagrad' or Adagrad(lr=0.01, epsilon=1e-08, decay=0.0)\n",
    "  * 'rmsprop' or RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "  * 'adadelta' or Adadelta(lr=1.0, rho=0.95, epsilon=1e-08, decay=0.0)\n",
    "  * 'adam' or Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "  * 'adamax' or Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "  * 'nadam' or Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004)\n",
    "* [metrics](https://keras.io/metrics/): list of metrics to be evaluated by the model during training and testing\n",
    "  * 'accuracy'\n",
    "* sample_weight_mode: If you need to do timestep-wise sample weighting (2D weights), set this to \"temporal\".  None defaults to sample-wise weights (1D). If the model has multiple outputs, you can use a different  sample_weight_mode on each output by passing a dictionary or a list of modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "_,weightsfile=tempfile.mkstemp()\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "es=EarlyStopping(monitor='val_loss', patience=patience, verbose=1)\n",
    "mc=ModelCheckpoint(weightsfile,monitor='val_loss',save_best_only=True,save_weights_only=True,verbose=0)\n",
    "\n",
    "history = model.fit(x,y,batch_size=32, epochs=10,\n",
    "                    callbacks=[es,mc],\n",
    "                    verbose=1,validation_split=0.0,\n",
    "                    validation_data=None, shuffle=True, class_weight=None, sample_weight=None)\n",
    "model.load_weights(weightsfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Parameters](https://keras.io/models/model/):\n",
    "\n",
    "* `x`: Numpy array of training data, or list of Numpy arrays if the model has multiple inputs. If all inputs in the model are named, you can also pass a dictionary mapping input names to Numpy arrays.\n",
    "* `y`: Numpy array of target data, or list of Numpy arrays if the model has multiple outputs. If all outputs in the model are named, you can also pass a dictionary mapping output names to Numpy arrays.\n",
    "* `batch_size`: integer. Number of samples per gradient update.\n",
    "* `epochs`: integer, the number of times to iterate over the training data arrays.\n",
    "* `verbose`: 0, 1, or 2. Verbosity mode. 0 = silent, 1 = verbose, 2 = one log line per epoch.\n",
    "* `validation_split`: float between 0 and 1: fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch.\n",
    "* `validation_data`: data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. This could be a tuple (x_val, y_val) or a tuple (val_x, val_y, val_sample_weights).\n",
    "* `shuffle`: boolean, whether to shuffle the training data before each epoch.\n",
    "* `class_weight`: optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class.\n",
    "* `sample_weight`: optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. In this case you should make sure to specify sample_weight_mode=\"temporal\" in compile().\n",
    "* `callbacks`: list of callbacks to be called during training. See callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General model() class\n",
    "# https://keras.io/models/model/\n",
    "evaluate(self, x, y, batch_size=32, verbose=1, sample_weight=None)\n",
    "predict(self, x, batch_size=32, verbose=0)\n",
    "train_on_batch(self, x, y, sample_weight=None, class_weight=None)\n",
    "test_on_batch(self, x, y, sample_weight=None)\n",
    "predict_on_batch(self, x)\n",
    "fit_generator(self, generator, samples_per_epoch, nb_epoch, verbose=1, callbacks=[], validation_data=None, nb_val_samples=None, class_weight={}, max_q_size=10, nb_worker=1, pickle_safe=False)\n",
    "evaluate_generator(self, generator, val_samples, max_q_size=10, nb_worker=1, pickle_safe=False)\n",
    "predict_generator(self, generator, val_samples, max_q_size=10, nb_worker=1, pickle_safe=False)\n",
    "get_layer(self, name=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential() only ?\n",
    "# https://keras.io/models/sequential/\n",
    "predict_classes(self, x, batch_size=32, verbose=1)\n",
    "predict_proba(self, x, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model.predict_classes(x) #Sequential() only\n",
    "pandas.crosstab(y_hat, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `with torch.no_grad()`:  no grad tracking\n",
    "* `model.eval()`        :  zero dropout probability\n",
    "* `model.train()`       :  back to nonzero original dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.5433e-01, 1.0443e-03, 3.7916e-01, 5.2210e-02, 3.2306e-04, 1.2050e-02,\n",
      "        3.9326e-01, 2.8806e-04, 7.2790e-03, 5.5042e-05])\n",
      "torch.return_types.topk(\n",
      "values=tensor([[0.3933, 0.3792],\n",
      "        [0.9888, 0.0055],\n",
      "        [0.9653, 0.0344]]),\n",
      "indices=tensor([[6, 2],\n",
      "        [6, 2],\n",
      "        [7, 9]]))\n"
     ]
    }
   ],
   "source": [
    "# inference\n",
    "with torch.no_grad():             # tells torch does not need to keep track of grad\n",
    "    model.eval()                  # .eval() mode sets dropout probability to zero\n",
    "    prob=torch.exp(model(images)) # The last layer is LogSoftmax, not Softmax\n",
    "model.train()                     # opposite of .eval()\n",
    "    \n",
    "print(prob[0])\n",
    "top_p, top_class = prob[:3].topk(2,dim=1)     # top-k most likely probability and the predicted classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test LogLoss:  0.31288541873285514\n",
      "Test Accuracy: 0.9099313616752625\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test\n",
    "running_loss = 0\n",
    "running_acc = 0\n",
    "for images, labels in testloader:\n",
    "    images = images.view(images.shape[0], -1)\n",
    "    with torch.no_grad():             # tells torch does not need to keep track of grad\n",
    "        out = model(images)           # forward\n",
    "        \n",
    "    loss = criterion(out, labels)     \n",
    "    prob = torch.exp(out)\n",
    "    top_p, top_class = prob.topk(1, dim=1)\n",
    "    equals = (top_class == labels.view(*top_class.shape))\n",
    "    \n",
    "    running_loss += loss.item()\n",
    "    running_acc += torch.mean(equals.type(torch.FloatTensor))   #cast torch.ByteTensor to torch.FloatTensor for taking .mean()\n",
    "\n",
    "print(f\"Test LogLoss:  {running_loss/len(trainloader)}\")\n",
    "print(f\"Test Accuracy: {running_acc/len(trainloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save / Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state_dict() # to ordered dict\n",
    "torch.save(model.state_dict(), 'filename')      # save\n",
    "model.load_state_dict( torch.load('filename') ) # load; \"model\" variable must have the same architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to manually build a dictionary with all the information you need to compeletely rebuild the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/docs/stable/torchvision/models.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "most model has `classifier` and `features` part.\n",
    "\n",
    "For transfer learning, turn of grad for `features` part and attach custom classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "model = models.densenet121(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old: Linear(in_features=1024, out_features=1000, bias=True)\n",
      "new: Sequential(\n",
      "  (fc1): Linear(in_features=1024, out_features=256, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=256, out_features=2, bias=True)\n",
      "  (output): LogSoftmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():      # Freeze parameters of the feature detector\n",
    "    param.requires_grad = False\n",
    "    \n",
    "print(f\"old: {model.classifier}\")     # seeing the input is 1024, we need to match it in our own classifier\n",
    "from collections import OrderedDict   # create our own (binary) classifier\n",
    "classifier = nn.Sequential(OrderedDict([\n",
    "                          ('fc1', nn.Linear(1024, 256)),\n",
    "                          ('relu', nn.ReLU()),\n",
    "                          ('fc2', nn.Linear(256, 2)),\n",
    "                          ('output', nn.LogSoftmax(dim=1))\n",
    "                          ]))    \n",
    "model.classifier = classifier\n",
    "print(f\"new: {model.classifier}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sklearn interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.estimator as skflow\n",
    "from sklearn import datasets, metrics\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "classifier = skflow.LinearClassifier(n_classes=3)\n",
    "classifier.fit(iris.data, iris.target)\n",
    "score = metrics.accuracy_score(iris.target, classifier.predict(iris.data))\n",
    "print(\"Accuracy: %f\" % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.contrib.learn as skflow\n",
    "from sklearn import datasets, metrics, preprocessing\n",
    "\n",
    "boston = datasets.load_boston()\n",
    "X = preprocessing.StandardScaler().fit_transform(boston.data)\n",
    "regressor = skflow.LinearRegressor()\n",
    "regressor.fit(X, boston.target)\n",
    "score = metrics.mean_squared_error(regressor.predict(X), boston.target)\n",
    "print (\"MSE: %f\" % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.contrib.learn as skflow\n",
    "from sklearn import datasets, metrics\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "classifier = skflow.DNNClassifier(hidden_units=[10, 20, 10], n_classes=3)\n",
    "classifier.fit(iris.data, iris.target)\n",
    "score = metrics.accuracy_score(iris.target, classifier.predict(iris.data))\n",
    "print(\"Accuracy: %f\" % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.contrib.learn as skflow\n",
    "from sklearn import datasets, metrics\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "def my_model(X, y):\n",
    "    \"\"\"This is DNN with 10, 20, 10 hidden layers, and dropout of 0.5 probability.\"\"\"\n",
    "    layers = skflow.ops.dnn(X, [10, 20, 10])\n",
    "    return skflow.models.logistic_regression(layers, y)\n",
    "\n",
    "classifier = skflow.TensorFlowEstimator(model_fn=my_model, n_classes=3)\n",
    "classifier.fit(iris.data, iris.target)\n",
    "score = metrics.accuracy_score(iris.target, classifier.predict(iris.data))\n",
    "print(\"Accuracy: %f\" % score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "192px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
