{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.1.0', False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip3 install -U torch torchvision\n",
    "import torch\n",
    "torch.manual_seed(7)\n",
    "torch.__version__, torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.xxx_()` means inplace function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udacity.com/course/deep-learning-pytorch--ud188  ,  https://github.com/udacity/deep-learning-v2-pytorch/tree/master/intro-to-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(5), 5)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(5)\n",
    "x, x.item()            # convert to python scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 0, 0, 0], dtype=torch.uint8)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.type(torch.ByteTensor) # typecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Alegra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1328, 0.1373, 0.2405, 1.3955, 1.3470]]) tensor([[2.4382, 0.2028, 2.4505, 2.0256, 1.7792]]) tensor([[-0.9179]])\n"
     ]
    }
   ],
   "source": [
    "features = torch.randn((1,5)) #tensor of size (1,5)\n",
    "weights = torch.randn_like(features)\n",
    "bias = torch.randn((1,1))\n",
    "print(features,weights,bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.9796]])\n",
      "tensor([[-1.9796]])\n",
      "tensor([[-1.9796]])\n",
      "tensor([[-1.9796]])\n"
     ]
    }
   ],
   "source": [
    "# matrix multiplication must have (ncols from 1st = nrows from 2nd)\n",
    "print(torch.mm(features,weights.transpose(0,1)))  #args are dimenions to swap\n",
    "print(torch.mm(features,weights.reshape(5,1))) #args are new nrow,ncol\n",
    "print(torch.mm(features,weights.resize_(5,1))) #args are new nrow,ncol; if different numel, remove or add (uninitialized)\n",
    "print(torch.mm(features,weights.view(5,1))) #.view returns new tensor; can use -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9552]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/(1 + torch.exp(torch.sum(features * weights) + bias))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To/from Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "memory is shared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2744, 0.6924],\n",
      "        [0.9054, 0.0106]], dtype=torch.float64)\n",
      "[[0.2743871  0.69241128]\n",
      " [0.90539865 0.01060206]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.random.rand(2,2)\n",
    "b = torch.from_numpy(a)\n",
    "print(b)\n",
    "print(b.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.54877421, 1.38482256],\n",
       "       [1.8107973 , 0.02120412]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.mul_(2) # inplace multiplicatino\n",
    "a # shows that memory is shared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.3915, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2,2, requires_grad=True)\n",
    "y = x*x\n",
    "z = y.mean()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.9434, -0.2670],\n",
      "        [ 0.6160, -0.2253]])\n",
      "tensor([[ 0.9434, -0.2670],\n",
      "        [ 0.6160, -0.2253]], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z.backward()\n",
    "print(x.grad)\n",
    "print(x/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "proving $\n",
    "\\frac{\\partial z}{\\partial x} = \\frac{\\partial}{\\partial x}\\left[\\frac{1}{n}\\sum_i^n x_i^2\\right] = \\frac{x}{2}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://pytorch.org/docs/master/torchvision/datasets.html#imagefolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "dataset = datasets.ImageFolder('path/to/datafolder', transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "labels are automatically set to subfolder names inside"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "image transformations: http://pytorch.org/docs/master/torchvision/transforms.html\n",
    "* `transforms.Resize(255)`: \n",
    "* `transforms.CenterCrop(224)`: \n",
    "* `transforms.ToTensor()`: to PyTorch tensors\n",
    "* `transforms.RandomRotation(30)`: add randomness to improve network's robustness (different for each iteration); for test data, don't add randomness\n",
    "* `transforms.RandomResizedCrop(224)`\n",
    "* `transforms.RandomHorizontalFlip()`\n",
    "* `transforms.Normalize((0.5,), (0.5,))`: mean and std for each channel:  `(input[channel] - mean[channel]) / std[channel]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create generator that go through batches:\n",
    "\n",
    "`dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `datasets.MNIST`\n",
    "* `datasets.FashionMNIST`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "# image transform pipeline\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,)), \n",
    "                               ])\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)   # control train/test on train kw\n",
    "testset = datasets.MNIST('~/.pytorch/MNIST_data/',  download=True, train=False, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)  #generator\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)  #generator\n",
    "\n",
    "images, labels = next(iter(trainloader))  #first batch of 64\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cats/Dogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([transforms.RandomRotation(30),\n",
    "                                       transforms.RandomResizedCrop(224),\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                            [0.229, 0.224, 0.225])])\n",
    "\n",
    "test_transforms = transforms.Compose([transforms.Resize(255),\n",
    "                                      transforms.CenterCrop(224),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                           [0.229, 0.224, 0.225])])\n",
    "\n",
    "train_data = datasets.ImageFolder('~/.pytorch/catdog', transform=train_transforms)\n",
    "test_data = datasets.ImageFolder('~/.pytorch/catdog', transform=test_transforms)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(test_data, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purely functional operations (without parameters/weights/bias) can just use torch.nn.functional in place of nn elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden  = nn.Linear(784, 256)  # can access weights on .hidden.weight, .hidden.bias\n",
    "        self.output  = nn.Linear(256, 10)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax(dim=1)    # dim=1 calculates softmax across the columns\n",
    "    def forward(self, x):    # x is 2D: batchsize*inputdim\n",
    "#         x = x.reshape(x.shape[0],784)  #make sure dimensions agree\n",
    "        x = self.hidden(x)\n",
    "        x = self.sigmoid(x)  # equiv. x = F.sigmoid(x); can also use like F.relu(x), etc...\n",
    "        x = self.output(x)\n",
    "        x = self.softmax(x)  # equiv. x = F.softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network(\n",
      "  (hidden): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      "  (softmax): Softmax()\n",
      ")\n",
      "Parameter containing:\n",
      "tensor([[ 0.0156,  0.0062,  0.0037,  ..., -0.0175,  0.0002, -0.0028],\n",
      "        [-0.0309, -0.0274, -0.0252,  ...,  0.0282, -0.0348, -0.0207],\n",
      "        [-0.0301, -0.0090, -0.0113,  ..., -0.0034, -0.0336, -0.0045],\n",
      "        ...,\n",
      "        [ 0.0103,  0.0287,  0.0193,  ...,  0.0154,  0.0196, -0.0301],\n",
      "        [ 0.0153, -0.0250, -0.0262,  ...,  0.0334,  0.0218,  0.0190],\n",
      "        [ 0.0265,  0.0232, -0.0021,  ..., -0.0223, -0.0150,  0.0172]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "model = Network()\n",
    "print(model)\n",
    "print(model.hidden.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples of `forward` member functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x):\n",
    "    x = self.dropout(F.relu(self.fc1(x)))    # self.dropout = nn.Dropout(p=0.2)\n",
    "    x = self.dropout(F.relu(self.fc2(x)))\n",
    "    x = F.log_softmax(self.fc2(x), dim=1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access member variable to fill them (`_` indicates in-place):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0154, -0.0056, -0.0049,  ..., -0.0126, -0.0023,  0.0071],\n",
       "        [ 0.0025,  0.0191, -0.0067,  ...,  0.0007, -0.0099,  0.0121],\n",
       "        [-0.0073, -0.0180, -0.0143,  ...,  0.0043,  0.0129,  0.0060],\n",
       "        ...,\n",
       "        [ 0.0062, -0.0167, -0.0162,  ...,  0.0028, -0.0019, -0.0234],\n",
       "        [-0.0151, -0.0146, -0.0093,  ...,  0.0034, -0.0014,  0.0079],\n",
       "        [ 0.0068, -0.0141, -0.0177,  ...,  0.0089,  0.0093, -0.0067]])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.hidden.bias.data.fill_(0)\n",
    "model.output.weight.data.normal_(std=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images=images.view(64, -1)  #originally 64x1x28x28\n",
    "model.forward(images)  #forward all images\n",
    "model(images) #equivalent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using nn. without class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=784, out_features=256, bias=True)\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(784, 256),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(256, 10),\n",
    "                      nn.Softmax(dim=1))\n",
    "print(model[0])  #accessing element\n",
    "# model.forward(images)     # equiv model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "model = nn.Sequential(OrderedDict([\n",
    "                      ('fc1', nn.Linear(784, 256)),\n",
    "                      ('relu1', nn.ReLU()),\n",
    "                      ('output', nn.Linear(256, 10)),\n",
    "                      ('softmax', nn.Softmax(dim=1))]))\n",
    "print(model[0].bias)    # accessing elements\n",
    "print(model.fc1.bias)   # accessing elements\n",
    "# model.forward(images)    # equiv model(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Linear output with CrossEntropyLoss, or\n",
    "* LogSoftmax with NLLLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3173, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(784, 256),  #could also use class or OrderedDict\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(256, 10),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "images = images.view(64, -1)              #originally 64x1x28x28\n",
    "loss = criterion( model(images), labels)  #averaged over the batch\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first layer grad Before backward pass: \n",
      " None\n",
      "first layer grad After backward pass: \n",
      " tensor([[-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
      "        [-0.0058, -0.0058, -0.0058,  ..., -0.0058, -0.0058, -0.0058],\n",
      "        [-0.0059, -0.0059, -0.0059,  ..., -0.0059, -0.0059, -0.0059],\n",
      "        ...,\n",
      "        [-0.0012, -0.0012, -0.0012,  ..., -0.0012, -0.0012, -0.0012],\n",
      "        [-0.0015, -0.0015, -0.0015,  ..., -0.0015, -0.0015, -0.0015],\n",
      "        [ 0.0038,  0.0038,  0.0038,  ...,  0.0038,  0.0038,  0.0038]])\n"
     ]
    }
   ],
   "source": [
    "print('first layer grad Before backward pass: \\n', model[0].weight.grad)\n",
    "loss.backward()\n",
    "print('first layer grad After backward pass: \\n', model[0].weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://pytorch.org/docs/master/optim.html\n",
    "* `optim.SGD`\n",
    "* `optim.Adam`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights -  Parameter containing:\n",
      "tensor([[ 0.0225,  0.0054, -0.0342,  ..., -0.0061,  0.0087,  0.0269],\n",
      "        [-0.0331,  0.0253,  0.0078,  ...,  0.0262,  0.0045, -0.0281],\n",
      "        [-0.0069,  0.0357, -0.0105,  ...,  0.0260,  0.0166,  0.0045],\n",
      "        ...,\n",
      "        [ 0.0043, -0.0352,  0.0160,  ...,  0.0255, -0.0023, -0.0298],\n",
      "        [ 0.0159, -0.0356, -0.0056,  ...,  0.0277,  0.0312,  0.0060],\n",
      "        [-0.0139,  0.0074,  0.0274,  ..., -0.0272, -0.0031, -0.0049]],\n",
      "       requires_grad=True)\n",
      "Gradient - tensor([[-7.0587e-05, -7.0587e-05, -7.0587e-05,  ..., -7.0587e-05,\n",
      "         -7.0587e-05, -7.0587e-05],\n",
      "        [-4.9133e-03, -4.9133e-03, -4.9133e-03,  ..., -4.9133e-03,\n",
      "         -4.9133e-03, -4.9133e-03],\n",
      "        [-6.5717e-03, -6.5717e-03, -6.5717e-03,  ..., -6.5717e-03,\n",
      "         -6.5717e-03, -6.5717e-03],\n",
      "        ...,\n",
      "        [-5.8080e-04, -5.8080e-04, -5.8080e-04,  ..., -5.8080e-04,\n",
      "         -5.8080e-04, -5.8080e-04],\n",
      "        [-1.1184e-03, -1.1184e-03, -1.1184e-03,  ..., -1.1184e-03,\n",
      "         -1.1184e-03, -1.1184e-03],\n",
      "        [ 2.5707e-03,  2.5707e-03,  2.5707e-03,  ...,  2.5707e-03,\n",
      "          2.5707e-03,  2.5707e-03]])\n"
     ]
    }
   ],
   "source": [
    "# One step\n",
    "optimizer.zero_grad()                   # gradients are accumulated -- clear first\n",
    "loss = criterion(model(images), labels) # forward a batch\n",
    "loss.backward()\n",
    "# print(model[0].weight.grad)           # get the grad of parameters\n",
    "print('Initial weights - ', model[0].weight)\n",
    "optimizer.step()                        # update params with accumulated grad\n",
    "print('Gradient -', model[0].weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.9859724194129139\n",
      "Training loss: 0.44501842942827546\n",
      "Training loss: 0.3749764274432461\n",
      "Training loss: 0.34324646042163437\n",
      "Training loss: 0.3232783184154456\n"
     ]
    }
   ],
   "source": [
    "# Full epochs\n",
    "for epoch in range(5):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:        \n",
    "        images = images.view(images.shape[0], -1)\n",
    "        \n",
    "        optimizer.zero_grad()                     # empty grad\n",
    "        loss = criterion(model(images), labels)   # forward\n",
    "        loss.backward()                           # backward        \n",
    "        optimizer.step()                          # update params with accumulated grad\n",
    "        \n",
    "        running_loss += loss.item()               # loss value itself??\n",
    "    else:\n",
    "        print(f\"Training loss: {running_loss/len(trainloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.cuda() #move to cuda\n",
    "# model.cpu()  #move back to cpu\n",
    "model.to(device)\n",
    "images=images.to(device)  #all data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `with torch.no_grad()`:  no grad tracking\n",
    "* `model.eval()`        :  zero dropout probability\n",
    "* `model.train()`       :  back to nonzero original dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.5433e-01, 1.0443e-03, 3.7916e-01, 5.2210e-02, 3.2306e-04, 1.2050e-02,\n",
      "        3.9326e-01, 2.8806e-04, 7.2790e-03, 5.5042e-05])\n",
      "torch.return_types.topk(\n",
      "values=tensor([[0.3933, 0.3792],\n",
      "        [0.9888, 0.0055],\n",
      "        [0.9653, 0.0344]]),\n",
      "indices=tensor([[6, 2],\n",
      "        [6, 2],\n",
      "        [7, 9]]))\n"
     ]
    }
   ],
   "source": [
    "# inference\n",
    "with torch.no_grad():             # tells torch does not need to keep track of grad\n",
    "    model.eval()                  # .eval() mode sets dropout probability to zero\n",
    "    prob=torch.exp(model(images)) # The last layer is LogSoftmax, not Softmax\n",
    "model.train()                     # opposite of .eval()\n",
    "    \n",
    "print(prob[0])\n",
    "top_p, top_class = prob[:3].topk(2,dim=1)     # top-k most likely probability and the predicted classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test LogLoss:  0.31288541873285514\n",
      "Test Accuracy: 0.9099313616752625\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test\n",
    "running_loss = 0\n",
    "running_acc = 0\n",
    "for images, labels in testloader:\n",
    "    images = images.view(images.shape[0], -1)\n",
    "    with torch.no_grad():             # tells torch does not need to keep track of grad\n",
    "        out = model(images)           # forward\n",
    "        \n",
    "    loss = criterion(out, labels)     \n",
    "    prob = torch.exp(out)\n",
    "    top_p, top_class = prob.topk(1, dim=1)\n",
    "    equals = (top_class == labels.view(*top_class.shape))\n",
    "    \n",
    "    running_loss += loss.item()\n",
    "    running_acc += torch.mean(equals.type(torch.FloatTensor))   #cast torch.ByteTensor to torch.FloatTensor for taking .mean()\n",
    "\n",
    "print(f\"Test LogLoss:  {running_loss/len(trainloader)}\")\n",
    "print(f\"Test Accuracy: {running_acc/len(trainloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save / Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state_dict() # to ordered dict\n",
    "torch.save(model.state_dict(), 'filename')      # save\n",
    "model.load_state_dict( torch.load('filename') ) # load; \"model\" variable must have the same architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to manually build a dictionary with all the information you need to compeletely rebuild the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://pytorch.org/docs/0.3.0/torchvision/models.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "model = models.densenet121(pretrained=True)\n",
    "for param in model.parameters():      # Freeze parameters of the feature detector\n",
    "    param.requires_grad = False\n",
    "model.classifier = nn.Sequential(OrderedDict([     # custom classifier\n",
    "                          ('fc1', nn.Linear(1024, 256)),\n",
    "                          ('relu', nn.ReLU()),\n",
    "                          ('fc2', nn.Linear(256, 2)),\n",
    "                          ('output', nn.LogSoftmax(dim=1))\n",
    "                          ]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "192px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
