{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# David Silver\n",
    "* https://www.youtube.com/playlist?list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ\n",
    "* http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Lecture 1](https://www.youtube.com/watch?v=2pWv7GOvuf0) [Intro](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/intro_RL.pdf)\n",
    "* Each step, agent ...\n",
    "  - execute action $A_t$\n",
    "  - receives observation $O_t$ and reward $R_t$\n",
    "* History $H_t\\equiv\\left\\{O_t,R_t,A_t\\right\\}$\n",
    "* *States*\n",
    "  - envirnoment state $S_t^e=f(H_T)$ the information environment used to pick next observation/reward (independent of past history)\n",
    "  - agent state $S_t^a=f(H_T)$ internal information used by agent to decide what to do next\n",
    "* State is **Markov** iff independent of the past given present. Examples:\n",
    "  - environment state $S_t^e$\n",
    "  - full history $H_t$\n",
    "* **Full observability**/**Markov decision process** $O_t=S_t^a=S_t^e$\n",
    "* **Partial observability** $S_t^a\\neq S_t^e$. Agent choose how to construct $S_t^a$, e.g.\n",
    "  - $S_t^a=H_t$\n",
    "  - Beliefs, i.e. a probability distribution of possible environment state\n",
    "  - RNN, i.e. vector state via training $S_t^a=\\sigma\\left[{\\rm Dense}\\left(S_{t-1}^a,O_t\\right)\\right]$\n",
    "* *Components of an agent*\n",
    "  - **Policy**: action given state, either deterministic $a=\\pi(s)$ or stochastic $\\pi(a|s)$\n",
    "  - **Value function**: prediction of future rewards (given the current policy) to evalute the favorability of a state: $v_\\pi(s)=E\\left[R_{t+1}+\\gamma R_{t+2}+\\gamma^2 R_{t+3}+...|S_t\\right]$\n",
    "  - **Model** (optional): predict the next state and next reward given current state and action\n",
    "  ${\\cal P}=p(S_{t+1}|S_t,A_t)$, ${\\cal R}=p(R_{t+1}|S_t,A_t)$\n",
    "* *Categorization of agent*\n",
    "  * based on existence of value function and policy\n",
    "    - **Value based**: has value function, on which policy is implied from\n",
    "    - **Policy based**: has policy but no value function\n",
    "    - **Actor critic**: has explicit value function and policy\n",
    "  * based on existence of a model\n",
    "    - **Model free**: based on policy/value function, without trying to figure out how the environment works\n",
    "    - **Model based**\n",
    "* Problems to be tackled:\n",
    "    * RL = **Learning** (unknown environment/rules, figure out policy through interaction) + **planning** (known env/rules, figure out best policy)\n",
    "    * Learning = **Exploration** + **exploitation**\n",
    "    * Planning = **Prediction** (evaluate future given policy) + **control** (find the best policy, i.e. need the prediction on all policies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Lecture 2](https://www.youtube.com/watch?v=lfHX2hHRMVQ) [Markov Decision Processes](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MDP.pdf)\n",
    "Model for the agent/environment\n",
    "\n",
    "* **Transition probability** ${\\cal P_{ss'}}\\equiv p(S_{t+1}=s'|S_t=s)=p(S_{t+1}|S_1...S_t)$ (2nd by definition)\n",
    "\n",
    "* **Markov Process** $\\left<{\\cal S,P}\\right>$ is specificed by the set of states ${\\cal S}$ and transition probability ${\\cal P}$\n",
    "\n",
    "* **Markov Reward Process** $\\left<{\\cal S,P,R,\\gamma}\\right>$ is a markov process with (expected) rewards $R_s=\\mathbb{E}(R_{t+1}|S_t=s)$ and discount factor $\\gamma$\n",
    "  - The goal (at each time step) is the maximize the return $G_t=\\sum_{k=0}^\\infty \\gamma^k R_{t+k+1}$\n",
    "  - $\\gamma=0$ is myopic, $\\gamma=1$ is far-sighted\n",
    "  - Having discount factor means we have an imperfect model (also, mathematically convenient and avoids infinity if there're infinite loops)\n",
    "  * **Value function of MRP** $v(s)=\\mathbb{E}(G_t|S_t=s)$ is the expected total rewards (i.e. return) if we start off at a certain state\n",
    "  * **Bellman Expectation Equation for MRP** decomposes value function's $G_t$ to immediate reward $R_{t+1}$ and the discounted value at the next step $v(s)=\\mathbb{E}\\left[R_{t+1}+\\gamma v(S_{t+1})|S_t=s\\right]$\n",
    "    - use transition matrix to evalute the next state: $v(s)={\\cal R}_s+\\gamma \\sum_{s'}{\\cal P}_{ss'}v(s')$\n",
    "    - cast everything to vectors/matrices, $v={\\cal R}+\\gamma{\\cal P}v \\implies v=\\left(I-\\gamma {\\cal P}\\right)^{-1}{\\cal R}$\n",
    "    - not practically for direct usage as O(n^3)\n",
    "* **Markov Decision Process** $\\left<{\\cal S,A,P,R,\\gamma}\\right>$ adds actions $\\cal A$ to a markov reward process\n",
    "  - Transition matrix $\\cal P_{ss'}^a$ depends on the action the agent take\n",
    "  - Rewards $\\cal R_s^a$ also depends on the action the agent take\n",
    "  - **Policy** $\\pi(a|s)=p(A_t=a|S_t=s)$ completely defines the behavior (actions) of the agent (only consider stationary policy i.e. independent of time)\n",
    "  - Given policy which governs the action, $\\left<{\\cal S,P^\\pi,R^\\pi,\\gamma}\\right>$ reduces to a markov reward process, where $P^\\pi=\\sum_a \\pi(a|s){\\cal P}_{ss'}^a$\n",
    "  - **State-value function of MDP** $v_\\pi(s)=\\mathbb{E}_\\pi(G_t|S_t=s)$ is the expected return starting off at state $s$ and following policy $\\pi$\n",
    "  - **Action-value function of MDP** $q_\\pi(s)=\\mathbb{E}_\\pi(G_t|S_t=s,A_t=a)$ is the expected return starting off at state $s$, *taking action $a$*, and following policy $\\pi$\n",
    "  - **Bellman Expectation Equation for MDP** still true: $v_\\pi(s)=\\mathbb{E}_\\pi\\left[R_{t+1}+\\gamma v_\\pi(S_{t+1})|S_t=s\\right]$  and $q_\\pi(s,a)=\\mathbb{E}_\\pi\\left[R_{t+1}+\\gamma q_\\pi(S_{t+1},A_{t+1})|S_t=s,A_t=a\\right]$\n",
    "    - Condition on next action: $v_\\pi(s)=\\sum_a\\pi(a|s)q_\\pi(s,a)$\n",
    "    - Condition on the state after the action about to take:  $q_\\pi(s,a)={\\cal R}_s^a+\\gamma\\sum_{s'}{\\cal P}_{ss'}^a v_\\pi(s')$\n",
    "    - Combining for $v$: $v_\\pi(s)=\\sum_a \\pi(a|s) \\left({\\cal R}_s^a+\\gamma\\sum_{s'}{\\cal P}_{ss'}^a v_\\pi(s')\\right)$\n",
    "    - Combining for $q$: $q_\\pi(s,a)={\\cal R}_s^a+\\gamma\\sum_{s'}{\\cal P}_{ss'}^a \\sum_{a'}\\pi(a'|s')q_\\pi(s',a')$\n",
    "    - One way to solve is to cast to MRP and then use $v_\\pi={\\cal R^\\pi}+\\gamma{\\cal P^\\pi}v_\\pi \\implies v_\\pi=\\left(I-\\gamma {\\cal P^\\pi}\\right)^{-1}{\\cal R^\\pi}$\n",
    "  - **Optimal state-value function** $v_*(s)=\\max_\\pi v_\\pi(s)$<br/>\n",
    "    **Optimal action-value function** $q_*(s,a)=\\max_\\pi q_\\pi(s,a)$ (the goal is to solve for this -- directly informs the correct policy to follow)\n",
    "* **Optimality** of policy in MDP\n",
    "  - **Ordering** of policy is defined as $\\pi\\geq\\pi'$ if $v_\\pi(s)\\geq v_{\\pi'}(s) \\forall s$\n",
    "  - Theorem:\n",
    "    - there is at least a deterministic optimal policy $\\pi_*\\geq\\pi, \\forall \\pi$\n",
    "    - if there are multiple optimal policies, they all achieve the same optimal $v$ and $q$: $v_{\\pi_*}(s)=v_*(s)$ amd $q_{\\pi_*}(s,a)=q_*(s,a)$\n",
    "    - if $q_*$ is known, $\\pi_*(a|s)$ is to follow ${\\rm argmax}_a q_*(s,a)$\n",
    "* **Bellman Optimality Equations**  in MDP\n",
    "  - $v_*(s)=\\max_a q_*(s,a)$\n",
    "  - $q_*(s,a)={\\cal R}_s^a+\\gamma\\sum_{s'}{\\cal P}_{ss'}^a v_*(s')$ just average over the possible states that the environment state will go into\n",
    "  - Combining for $v$...\n",
    "  - Combining for $q$...\n",
    "  - Nonlinear due to max function\n",
    "  - No closed form in general, use iterative methods:  Value Iteration, Policy Iteration, Q-learning, Sarsa\n",
    "* **Extensions**\n",
    "  - Infinite (# states/action spaces) continuous (states/action spaces or time)\n",
    "  - Partially observable MDP: add observations $\\cal O$ and observation function $p(o|s,a)$ to problem definition\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Lecture 3](https://www.youtube.com/watch?v=Nd1-UUMVfz4) [Planning by Dynamic Programming](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/DP.pdf)\n",
    "Given a *known* MDP, find best policy\n",
    "* **Dynamic programming**: a general method to solve problems with \n",
    "  - Optimal substructure -- combination of optimal subsolution is the optimal solution of the main problem\n",
    "  - Overlapping subproblem -- subproblems appear again and again so you can cache the solution\n",
    "* **Policy Evaluation**: evaluate $v_\\pi$ given a $\\pi$, via iterations ($k$ as the index)\n",
    "  - **(a)synchronus backups** -- whether to update $v_k(s)$ for all states together at once (it matters as they depend on each other)\n",
    "  - iterate using Bellman equation $v_{k+1}(s)=\\sum_a \\pi(a|s) \\left({\\cal R}_s^a+\\gamma\\sum_{s'}{\\cal P}_{ss'}^a v_k(s')\\right)$\n",
    "  - simple example: walking to NW or SE corner of a grid, each step rewards -1. Policy is equal prob for all directions. At each step, for each grid point (except NW and SE whose $v_\\pi$ is 0), average all the $v$ of the neighbors and add -1 to it. Iterate to convergence\n",
    "* **Policy Iteration**: After evaluating $v_\\pi$, modify policy to act greedily w.r.t. $v_\\pi$ i.e. maximize the expected reward+value on the next timestep, *based on Bellman Expectation equation* $v_\\pi(s)=\\mathbb{E}_\\pi\\left[R_{t+1}+\\gamma v_\\pi(S_{t+1})|S_t=s\\right]$ (modify $\\pi$ to maximize $v$)\n",
    "  - Iterate: $v=v_\\pi \\Longleftrightarrow \\pi={\\rm greedy}(v)$, converges to $\\{v_*, \\pi_*\\}$\n",
    "  - For deterministic policy $\\left[a=\\pi(s)\\right]$, the iteration step for $\\pi$ is $\\pi'(s)={\\rm argmax}_aq_\\pi(s,a)$\n",
    "    - can be proven by showing $q_\\pi$ improves by following this for one-step, then two-step, and so on.\n",
    "    - *if* iteration stops, automatically satisfies Bellman equation, so $v_\\pi=v_*$\n",
    "  - **Modified Policy Iteration**: way earlier than convergence of the value function, optimal policy could be found. \n",
    "    - 3 iterations of value function are sufficient to give correct policy\n",
    "    - if stop at 1 iteration, equivalent to value iteration\n",
    "* **Value Iteration**: iterate by *Bellman optimality equation* $v_*(s)=\\max_a \\left[{\\cal R}_s^a+\\gamma\\sum_{s'}{\\cal P}_{ss'}^a v_*(s')\\right]$ (start with random guess and get new lhs each time)\n",
    "  - aims at finding $v_*$ (value of optimal policy);  the value function at intermedate iteration steps $v_k(s)$ does not corresponds to any policy\n",
    "  - synchronous 'backup': at each step, for all states, update $v_{k+1}(s)$ from $v_k(s)$\n",
    "  - taking $\\max_a$ equivalent to formulating the correct policy. i.e. equiv to policy iteration that do only 1 iteration in finding $v$\n",
    "* **Asynchronous update**:\n",
    "  - Complexity for synchronous update for $m$ actions and $n$ states: ${\\cal O}(mn^2)$ per iteration\n",
    "  - asynch: updates $v(s)$ for each state individually; i.e. does not need to update all $v(s)$ simultaneously\n",
    "  - Guaranteed to converge if all states are selected\n",
    "  - Flavors:\n",
    "      - **in-place DP**: update each stat immediately in any order\n",
    "      - **prioritised sweeping**: update states that were changing the most, i.e. largest $\\left|\\max_a\\left[{\\cal R}_s^a+\\gamma\\sum_{s'}{\\cal P}_{ss'}^a v_*(s')\\right]-v(s)\\right|$\n",
    "      - **real-tiem DP**: use real agent to run through the problem, and update states around those visited by the agent\n",
    "* **Sample backups**: using sample rewards and transitions instead of the provided (or unavailable) rewards $\\cal R$ and transition matrix $\\cal P$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Lecture 4](https://www.youtube.com/watch?v=PnHCvfgC_ZA) [Model-Free Prediction](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MC-TD.pdf)\n",
    "*Estimate* value function of policies for an *unknown* MDP (knows rewards but not transition probabilities)\n",
    "* **Monte Carlo**: learn $v_\\pi$ from *complete* episode under policy $\\pi$\n",
    "  - only for episodic MDPs i.e. all episodes must terminate\n",
    "  - 1 episode = full history $H=S_1,A_1,R_2,S_2,A_2,...S_k$\n",
    "  - compute $v_\\pi(s)=\\mathbb{E}_\\pi[G_t|S_t=s]$, where $G_t=R_{t+1}+\\gamma R_{t+2}+...+\\gamma^{T-1} R_T$, by *empirically* taking the mean for expectation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
