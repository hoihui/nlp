{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0-beta1'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "from collections import deque # for finite memory \n",
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementations\n",
    "* `env` must support the methods `.reset()`, `.step()` with returns like `gym`'s\n",
    "* if `env` is not `gym`'s environment, must provide `state_dim` and `action_dim`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Value Network, Q learning\n",
    "* network input: `state (raw pixels)` output: `Q(a|state)`\n",
    "* target are the Q-values by target_network on *the state/action actually followed*. Others not used for training (just set the target to what the model predicts now, so loss is zero)\n",
    "* the loss `mse` is chosen such that gradient of loss is the update we want on the parameters: $-\\nabla[r+\\gamma\\max_{a'}Q(s',a')-\\hat{Q}_{\\bf w}(s,a)]^2=[r+\\gamma\\max_{a'}Q(s',a')-\\hat{Q}_{\\bf w}(s,a)]\\nabla_{\\bf w}\\hat{Q}_{\\bf w}\\rightarrow\\Delta {\\bf w}$\n",
    "\n",
    "[Three improvements](https://www.youtube.com/watch?v=EX1CIVVkWdE):\n",
    "1. **Experience Replay**: If we only use most recent states, the training samples are highly correlated (all coming from the same episodes). Store all intermediate states; after each step, train a random batch from memory.\n",
    "2. **Target Network**: Training is difficult to converge if the target is constantly shifting. The TD target is output from another network that got updated to the most up-to-date value network once in a while.\n",
    "3. **Huber Loss**: Avoid exploding gradients by using Huber loss instead of `mse`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(object):  #model+memory\n",
    "    def __init__(self,env,state_dim=None, action_dim=None,\n",
    "                 gamma=.9,max_memory=5000):\n",
    "        self.memory = deque(maxlen=max_memory)\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.action_dim = action_dim or self.env.action_space.n\n",
    "        self.state_shape = (state_dim,) if state_dim else self.env.observation_space.shape\n",
    "        \n",
    "        self.model = tf.keras.models.Sequential([\n",
    "                         tf.keras.layers.Dense(96, input_shape=self.state_shape, activation='relu'),\n",
    "                         tf.keras.layers.Dense(48, activation='relu'),\n",
    "                         tf.keras.layers.Dense(self.action_dim),\n",
    "                        ])\n",
    "        self.model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam())     # learning rate handled by optimizer\n",
    "        self.target_model = tf.keras.models.clone_model(self.model)\n",
    "    \n",
    "    def get_epsilon(self, episode):\n",
    "        ############# EPSILON to use for epsilon-greedy (probability of exploration)#########\n",
    "#         return 1/(1+e*.2)\n",
    "#         return max(.01, 0.995**episode)\n",
    "        return 0.1\n",
    "        #####################################################################################\n",
    "\n",
    "    def choose_action(self, state, epsilon):\n",
    "        if np.random.random() <= epsilon:\n",
    "            return np.random.randint(0, self.action_dim)\n",
    "        else:\n",
    "            return np.argmax(self.model.predict(state[np.newaxis])[0])\n",
    "        \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def replay_train(self, batch_size=50):\n",
    "        batch = random.sample( self.memory, min(len(self.memory), batch_size))\n",
    "        states,actions,rewards,nextstates,dones = map(np.array,zip(*batch))        \n",
    "        ys=self.model.predict(states)             # current estimation of the Q(a|s)\n",
    "        qs=self.target_model.predict(nextstates)  # Q(a|s') for next state (target is r+g*max(this) for the action taken, otherwise use current estimation as the target)\n",
    "        \n",
    "#         X, Y = [], [] \n",
    "#         for i,(state, action, reward, next_state, done) in enumerate(batch):\n",
    "#             y = ys[i]  # = self.model.predict(state[np.newaxis])[0]\n",
    "#             q = qs[i]  # = self.model.predict(next_state[np.newaxis])[0]\n",
    "#             y[action] = reward + (0 if done else self.gamma*np.max(q)) # R_t + gam * max_a' Q(s',a') ONLY for the action executed; others remain unchanged from current prediction\n",
    "#             X += state,\n",
    "#             Y += y,\n",
    "#         return self.model.train_on_batch(np.array(X), np.array(Y))   \n",
    "\n",
    "        ys[np.arange(ys.shape[0]),actions] = rewards + (1-dones)*self.gamma*np.max(qs,axis=1) #equivalent to above\n",
    "        return self.model.train_on_batch(states, ys)\n",
    "              \n",
    "    def run(self,episodes=1000,show_every=None,fit_data='per_step',update_target_every=2):\n",
    "        show_every = show_every or episodes//10\n",
    "        scores = deque(maxlen=show_every) #store new episodes after previous print\n",
    "\n",
    "        for e in range(1,episodes+1):\n",
    "            EPSILON = 0 if not fit_data else self.get_epsilon(e)\n",
    "            state, done = self.env.reset(), False\n",
    "            R = 0\n",
    "            while not done:\n",
    "                action = self.choose_action(state, EPSILON)\n",
    "                next_state, reward, done,_ = self.env.step(action)\n",
    "                R += reward\n",
    "                self.remember(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                if fit_data=='per_step': self.replay_train()\n",
    "            if fit_data=='per_episode': self.replay_train()\n",
    "            scores+=R,\n",
    "                     \n",
    "            if e%show_every == 0:\n",
    "                print(f'Episode {e:4d} | Average R {np.mean(scores):6.4g} | Median R {np.median(scores)}')\n",
    "            if e%update_target_every==0:  #update frequency of target network\n",
    "                self.target_model.set_weights(self.model.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Policy Network, MC\n",
    "* no epsilon-greedy necessary since network output softmax (probabilities), not deterministic\n",
    "* loss `categorical_crossentropy` is chosen such that gradient of loss is the update we want on the network parameters $\\theta$:\n",
    "\n",
    "  Loss $L(v, \\hat{p}) = -\\sum_av_a \\log \\hat{p}_a$, where the targets used are $v_a$=value or advantage following action $a$ that was actually taken (zeros for other non-taken actions), and $\\hat{p}_a$ is output from policy network for the probability of choosing action $a$. So $L=-v_t\\log\\pi_\\theta(s,a)$, and $-\\nabla_\\theta L=v_t\\nabla_\\theta\\log\\pi_\\theta(s,a)\\rightarrow\\Delta\\theta$, exactly what wanted.\n",
    "* use the mean return of the whole episode as the baseline -- a constant value\n",
    "* calculate the correct targets at the end of an episode, then add to memory for replay\n",
    "* cannot use TD as there is no estimation of value function available\n",
    "* train batch size should be ~ episode length? train once per episode or per step?\n",
    "* ?not good for game that does have reward only at the end?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(object):  #model+memory\n",
    "    def __init__(self,env,state_dim=None, action_dim=None,\n",
    "                 gamma=.9,max_memory=None):\n",
    "        self.memory = deque(maxlen=max_memory)\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.action_dim = action_dim or self.env.action_space.n\n",
    "        self.state_shape = (state_dim,) if state_dim else self.env.observation_space.shape\n",
    "        \n",
    "        self.model = tf.keras.models.Sequential([\n",
    "                         tf.keras.layers.Dense(24, input_shape=self.state_shape, activation='relu'),\n",
    "                         tf.keras.layers.Dense(24, activation='relu'),\n",
    "                         tf.keras.layers.Dense(self.action_dim, activation='softmax'),\n",
    "                        ])\n",
    "        self.model.compile(loss=\"categorical_crossentropy\", optimizer=tf.keras.optimizers.RMSprop())\n",
    "        \n",
    "    def choose_action(self, state):\n",
    "        policy = self.model.predict(state[np.newaxis])[0]\n",
    "        return np.random.choice(self.action_dim, p=policy)\n",
    "\n",
    "    def remember(self,mem):\n",
    "        states,actions,Gs = zip(*mem)\n",
    "        \n",
    "        Gs = np.asarray(Gs,dtype=np.float32) #convert rewards to returns\n",
    "        G = 0  \n",
    "        for t in reversed(range(len(states))):\n",
    "            G = G * self.gamma + Gs[t]\n",
    "            Gs[t] = G            \n",
    "        Gs -= np.mean(Gs)\n",
    "        Gs /= np.std(Gs)+1e-12    #subtract a uniform value \n",
    "        \n",
    "        Y = np.zeros((len(states), self.action_dim))\n",
    "        for t in range(len(states)):\n",
    "            Y[t,actions[t]] = Gs[t]\n",
    "        \n",
    "        self.memory.extend(zip(states,Y))\n",
    "#         self.memory=list(zip(states,Y))           #only keep last episode\n",
    "        \n",
    "    def replay_train(self, batch_size=64):\n",
    "        if not self.memory: return\n",
    "        batch = random.sample( self.memory, min(len(self.memory), batch_size))\n",
    "        X,Y = map(np.asarray,zip(*batch))        \n",
    "        return self.model.train_on_batch(X, Y)\n",
    "              \n",
    "    def run(self,episodes=1000,show_every=None,fit_data='per_step'):\n",
    "        show_every = show_every or episodes//10\n",
    "        scores = deque(maxlen=show_every)\n",
    "        \n",
    "        for e in range(1,episodes+1):\n",
    "            state,done = self.env.reset(),False\n",
    "            short_term_mem = []\n",
    "            R = 0\n",
    "            while not done:\n",
    "                action = self.choose_action(state)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                R += reward\n",
    "                short_term_mem += (state, action, reward),\n",
    "                state = next_state\n",
    "                if done: self.remember(short_term_mem)\n",
    "                if fit_data=='per_step': self.replay_train()\n",
    "            if fit_data=='per_episode': self.replay_train()\n",
    "            scores+=R,\n",
    "            \n",
    "            if e%show_every == 0:\n",
    "                print(f'Episode {e:4d} | Average R {np.mean(scores):6.4g} | Median R {np.median(scores)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep policy+value (Actor-Critic) networks\n",
    "* Policy network predicts probabilities of actions given state, $\\pi_\\theta(a|s)$. Value network estimates *state*-value function  for the given state $V_v(s)$\n",
    "* Policy network loss is $L= -\\sum_a[r+\\gamma V_v(s_{t+1})-V_v(s_t)]\\log\\pi_\\theta(a|s)$, so $-\\nabla_\\theta L=[r+\\gamma V_v(s_{t+1})-V_v(s_t)]\\nabla_\\theta\\log\\pi_\\theta(s,a)\\rightarrow\\Delta\\theta$\n",
    "* Value network loss is $L= [r+\\gamma V_v(s_{t+1})-V_v(s_t)]^2$, so $-\\nabla_v L =[r+\\gamma V_v(s_{t+1})-V_v(s_t)]\\nabla_v V_v(s_t)\\rightarrow\\Delta {\\bf v}$\n",
    "* update by TD(0) now as there is a network to give estimation of V(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(object):  #models+memory\n",
    "    def __init__(self,env,state_dim=None, action_dim=None,\n",
    "                 gamma=.9,\n",
    "                 max_memory=5000):\n",
    "        self.memory = deque(maxlen=max_memory)\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.action_dim = action_dim or self.env.action_space.n\n",
    "        self.state_shape = (state_dim,) if state_dim else self.env.observation_space.shape\n",
    "        \n",
    "        self.actor = tf.keras.models.Sequential([\n",
    "                         tf.keras.layers.Dense(48, input_shape=self.state_shape, activation='relu',kernel_initializer='he_uniform'),\n",
    "                         tf.keras.layers.Dense(self.action_dim, activation='softmax',kernel_initializer='he_uniform'),\n",
    "                     ])\n",
    "        self.actor.compile(loss=\"categorical_crossentropy\", optimizer=tf.keras.optimizers.Adam(lr=0.001))\n",
    "        \n",
    "        self.critic = tf.keras.models.Sequential([\n",
    "                         tf.keras.layers.Dense(48, input_shape=self.state_shape, activation='relu',kernel_initializer='he_uniform'),\n",
    "                         tf.keras.layers.Dense(1, kernel_initializer='he_uniform'),  #value of the state\n",
    "                     ])\n",
    "        self.critic.compile(loss=\"mse\", optimizer=tf.keras.optimizers.Adam(lr=0.005))\n",
    "        \n",
    "    def choose_action(self, state):\n",
    "        policy = self.actor.predict(state[np.newaxis])[0]\n",
    "        return np.random.choice(self.action_dim, p=policy)\n",
    "\n",
    "    def remember(self,state, action, reward, next_state, done):\n",
    "        self.memory += (state, action, reward, next_state, done),\n",
    "        \n",
    "    def replay_train(self, batch_size=32):\n",
    "        if not self.memory: return\n",
    "        batch_size = min(len(self.memory), batch_size)\n",
    "        batch = random.sample( self.memory, batch_size)\n",
    "        states,actions,rewards,next_states,dones = map(np.array,zip(*batch))        \n",
    "        \n",
    "        value_target = np.zeros((batch_size, 1))\n",
    "        advantages   = np.zeros((batch_size, self.action_dim))\n",
    "        \n",
    "        curr_values = self.critic.predict(states).squeeze()\n",
    "        next_values = self.critic.predict(next_states).squeeze()\n",
    "        \n",
    "        advantages[np.arange(batch_size),actions] = rewards + (1-dones)*self.gamma*next_values - curr_values\n",
    "        value_target[:,0] = rewards + (1-dones)*self.gamma*next_values\n",
    "    \n",
    "        return self.actor.train_on_batch(states,advantages),\\\n",
    "               self.critic.train_on_batch(states,value_target), \n",
    "              \n",
    "    def run(self,episodes=1000,show_every=None,fit_data='per_step'):\n",
    "        show_every = show_every or episodes//10\n",
    "        scores = deque(maxlen=show_every)\n",
    "        \n",
    "        for e in range(1,episodes+1):\n",
    "            state,done = self.env.reset(),False\n",
    "            short_term_mem = []\n",
    "            R = 0\n",
    "            while not done:\n",
    "                action = self.choose_action(state)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                R += reward\n",
    "                self.remember(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                if fit_data=='per_step': self.replay_train(128)\n",
    "            if fit_data=='per_episode': self.replay_train(128)\n",
    "            scores+=R,\n",
    "            \n",
    "            if e%show_every == 0:\n",
    "                print(f'Episode {e:4d} | Average R {np.mean(scores):6.4g} | Median R {np.median(scores)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Catch](https://gist.github.com/EderSantana/c7222daa328f0e885093) using raw pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Catch(object): # 1 game is 1 fruit dropped from top to bottom\n",
    "    def __init__(self, grid_size=10):\n",
    "        self.grid_size = grid_size\n",
    "        self.basketSize = 1\n",
    "        \n",
    "    def reset(self):\n",
    "        n = np.random.randint(0, self.grid_size-1)                # starting fruit_col\n",
    "        m = np.random.randint(0, self.grid_size-self.basketSize)  # starting basket col\n",
    "        self.state = np.asarray([0, n, m])                        # [fruit_row, fruit_col, basket's left end]\n",
    "        return self.observe()\n",
    "    \n",
    "    def _get_reward(self):   # inc/dec score only if fruit has dropped to bottom\n",
    "        fruit_row, fruit_col, basket_left = self.state\n",
    "        if fruit_row == self.grid_size-1 and basket_left <= fruit_col < basket_left+self.basketSize:\n",
    "            return 1.\n",
    "        else:\n",
    "            return 0.\n",
    "\n",
    "    def _is_over(self):    # game over if fruit dropped to bottom\n",
    "        return (self.state[0] == self.grid_size-1)\n",
    "    \n",
    "    def observe(self):\n",
    "        im_size = (self.grid_size, self.grid_size)\n",
    "        state = self.state\n",
    "        canvas = np.zeros(im_size)\n",
    "        canvas[self.state[0], self.state[1]] = 1                         # draw fruit\n",
    "        canvas[-1, self.state[2]:self.state[2] + self.basketSize+1] = 1  # draw basket\n",
    "        return canvas.flatten()\n",
    "    \n",
    "    def step(self, action):\n",
    "        if action == 0:   action = -1 # move left\n",
    "        elif action == 1: action =  0 # stay\n",
    "        else:             action =  1 # move right\n",
    "        f0, f1, basket_left = self.state\n",
    "        new_basket_left = min(max(0, basket_left + action), self.grid_size-self.basketSize)\n",
    "        f0 += 1                       # fruit dropped by one pixel\n",
    "        out = np.asarray([f0, f1, new_basket_left])\n",
    "        self.state = out\n",
    "        \n",
    "        return self.observe(), self._get_reward(), self._is_over(), None # returns whole canvas, R, done?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`gym`'s environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Catch(gym.Env):   # 1 game is 1 fruit dropped from top to bottom. agent at bottom row to catch it\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, grid_size=10,basket_size=1):\n",
    "        super(Catch, self).__init__()\n",
    "        \n",
    "        self.action_space = gym.spaces.Discrete(3)\n",
    "        self.observation_space = gym.spaces.MultiBinary(grid_size*grid_size)\n",
    "        \n",
    "        self.grid_size = grid_size\n",
    "        self.basket_size = basket_size\n",
    "\n",
    "    def reset(self):           # Reset the state of the environment to an initial state\n",
    "        n = np.random.randint(0, self.grid_size-1)                # starting fruit_col\n",
    "        m = np.random.randint(0, self.grid_size-self.basket_size)  # starting basket col\n",
    "        self.state = np.asarray([0, n, m])                        # [fruit_row, fruit_col, basket's left end]\n",
    "        return self._observe()\n",
    "    \n",
    "    def step(self, action):    # Execute one time step within the environment\n",
    "        if action == 0:   action = -1 # move left\n",
    "        elif action == 1: action =  0 # stay\n",
    "        else:             action =  1 # move right\n",
    "        f0, f1, basket_left = self.state\n",
    "        new_basket_left = min(max(0, basket_left + action), self.grid_size-self.basket_size)\n",
    "        f0 += 1                       # fruit dropped by one pixel\n",
    "        out = np.asarray([f0, f1, new_basket_left])\n",
    "        self.state = out\n",
    "        \n",
    "        return self._observe(), self._get_reward(), self._is_over(), None # returns whole canvas, R, done?\n",
    "    \n",
    "    def render(self, mode='human', close=False):        # Render the environment to the screen\n",
    "        print(state)\n",
    "    #############################helper methods---not required by gym.env##################################\n",
    "    def _get_reward(self):   # inc/dec score only if fruit has dropped to bottom\n",
    "        fruit_row, fruit_col, basket_left = self.state\n",
    "        return fruit_row == self.grid_size-1 and basket_left <= fruit_col < basket_left+self.basket_size\n",
    "\n",
    "    def _is_over(self):    # game over if fruit dropped to bottom\n",
    "        return (self.state[0] == self.grid_size-1)\n",
    "    \n",
    "    def _observe(self):\n",
    "        im_size = (self.grid_size, self.grid_size)\n",
    "        state = self.state\n",
    "        canvas = np.zeros(im_size)\n",
    "        canvas[self.state[0], self.state[1]] = 1                          # draw fruit\n",
    "        canvas[-1, self.state[2]:self.state[2] + self.basket_size+1] = 1  # draw basket\n",
    "        return canvas.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Various Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  200 | Average R   0.21 | Median R 0.0\n",
      "Episode  400 | Average R   0.33 | Median R 0.0\n",
      "Episode  600 | Average R   0.37 | Median R 0.0\n",
      "Episode  800 | Average R  0.465 | Median R 0.0\n",
      "Episode 1000 | Average R  0.655 | Median R 1.0\n",
      "Episode 1200 | Average R   0.79 | Median R 1.0\n",
      "Episode 1400 | Average R   0.76 | Median R 1.0\n",
      "Episode 1600 | Average R  0.815 | Median R 1.0\n",
      "Episode 1800 | Average R  0.815 | Median R 1.0\n",
      "Episode 2000 | Average R   0.85 | Median R 1.0\n"
     ]
    }
   ],
   "source": [
    "env = Catch(grid_size = 10)\n",
    "agent = DQN(env)\n",
    "agent.run(2000,fit_data='per_episode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  100 | Average R   0.07 | Median R 0.0\n",
      "Episode  200 | Average R   0.15 | Median R 0.0\n",
      "Episode  300 | Average R   0.04 | Median R 0.0\n",
      "Episode  400 | Average R   0.04 | Median R 0.0\n",
      "Episode  500 | Average R    0.1 | Median R 0.0\n",
      "Episode  600 | Average R   0.07 | Median R 0.0\n",
      "Episode  700 | Average R   0.12 | Median R 0.0\n",
      "Episode  800 | Average R    0.1 | Median R 0.0\n",
      "Episode  900 | Average R   0.01 | Median R 0.0\n",
      "Episode 1000 | Average R   0.04 | Median R 0.0\n"
     ]
    }
   ],
   "source": [
    "env = Catch(grid_size = 10)\n",
    "agent = PolicyNet(env)\n",
    "agent.run(1000,fit_data='per_step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  200 | Average R   0.15 | Median R 0.0\n",
      "Episode  400 | Average R  0.265 | Median R 0.0\n",
      "Episode  600 | Average R  0.355 | Median R 0.0\n",
      "Episode  800 | Average R  0.325 | Median R 0.0\n",
      "Episode 1000 | Average R   0.36 | Median R 0.0\n",
      "Episode 1200 | Average R   0.34 | Median R 0.0\n",
      "Episode 1400 | Average R  0.375 | Median R 0.0\n",
      "Episode 1600 | Average R  0.305 | Median R 0.0\n",
      "Episode 1800 | Average R  0.355 | Median R 0.0\n",
      "Episode 2000 | Average R  0.365 | Median R 0.0\n"
     ]
    }
   ],
   "source": [
    "env = Catch(grid_size = 10)\n",
    "agent = ActorCritic(env, state_dim = 100, action_dim = 3)\n",
    "agent.run(2000,fit_data='per_episode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode   10 | Average R      1 | Median R 1.0\n",
      "Episode   20 | Average R      1 | Median R 1.0\n",
      "Episode   30 | Average R      1 | Median R 1.0\n",
      "Episode   40 | Average R      1 | Median R 1.0\n",
      "Episode   50 | Average R      1 | Median R 1.0\n",
      "Episode   60 | Average R      1 | Median R 1.0\n",
      "Episode   70 | Average R      1 | Median R 1.0\n",
      "Episode   80 | Average R      1 | Median R 1.0\n",
      "Episode   90 | Average R      1 | Median R 1.0\n",
      "Episode  100 | Average R      1 | Median R 1.0\n"
     ]
    }
   ],
   "source": [
    "## Evaluation (epsilon=0)\n",
    "agent.run(100,fit_data=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib import animation, rc\n",
    "from IPython.display import HTML\n",
    "frames = []\n",
    "\n",
    "for e in range(100):\n",
    "    loss = 0.\n",
    "    env.reset()\n",
    "    done = False\n",
    "    state = env.observe()\n",
    "    frames.append(state.reshape(env.grid_size,env.grid_size))\n",
    "    while not done:\n",
    "        q = agent.model.predict(state[np.newaxis])  # q table at current state\n",
    "        action = np.argmax(q[0])\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        frames.append(next_state.reshape(env.grid_size,env.grid_size))\n",
    "        state = next_state\n",
    "# plt.imshow(frames[9],interpolation='none', cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#animation\n",
    "fig, ax = plt.subplots()\n",
    "im  = ax.imshow(np.random.random((env.grid_size,)*2),interpolation='none', cmap='gray')\n",
    "def init():\n",
    "#     im.set_array(np.random.random((grid_size,grid_size)))\n",
    "    return (im,)\n",
    "def animate(i):\n",
    "    im.set_array(frames[i])\n",
    "    return (im,)\n",
    "anim = animation.FuncAnimation(fig, animate, init_func=init,\n",
    "                               frames=len(frames), interval=50, blit=True)\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to files\n",
    "for i in range(len(frames)):\n",
    "    plt.imshow(frames[i],interpolation='none', cmap='gray')\n",
    "    plt.savefig(\"%03d.png\" % i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# [CartPole](https://gym.openai.com/envs/CartPole-v0/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network on default state representation\n",
    "* https://gym.openai.com/evaluations/eval_EIcM1ZBnQW2LBaFN6FY65g/\n",
    "* https://gym.openai.com/evaluations/eval_OeUSZwUcR2qSAqMmOE1UIw/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode   20 | Average R  10.75 | Median R 10.0\n",
      "Episode   40 | Average R   39.1 | Median R 36.0\n",
      "Episode   60 | Average R   81.4 | Median R 86.0\n",
      "Episode   80 | Average R  125.5 | Median R 128.0\n",
      "Episode  100 | Average R  136.6 | Median R 134.0\n",
      "Episode  120 | Average R  145.3 | Median R 146.0\n",
      "Episode  140 | Average R  146.1 | Median R 141.0\n",
      "Episode  160 | Average R  161.9 | Median R 162.0\n",
      "Episode  180 | Average R  135.5 | Median R 119.0\n",
      "Episode  200 | Average R  133.1 | Median R 155.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')#.unwrapped\n",
    "agent = DQN(env)\n",
    "agent.run(200,fit_data='per_step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode   20 | Average R   54.3 | Median R 28.0\n",
      "Episode   40 | Average R    197 | Median R 200.0\n",
      "Episode   60 | Average R  117.8 | Median R 122.0\n",
      "Episode   80 | Average R   48.3 | Median R 44.0\n",
      "Episode  100 | Average R  75.85 | Median R 84.0\n",
      "Episode  120 | Average R  129.3 | Median R 141.5\n",
      "Episode  140 | Average R  135.2 | Median R 136.0\n",
      "Episode  160 | Average R  105.7 | Median R 98.0\n",
      "Episode  180 | Average R   70.1 | Median R 71.5\n",
      "Episode  200 | Average R   60.6 | Median R 60.5\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "agent = PolicyNet(env)\n",
    "agent.run(200,fit_data='per_step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode   10 | Average R   13.1 | Median R 12.0\n",
      "Episode   20 | Average R   51.4 | Median R 44.5\n",
      "Episode   30 | Average R   44.4 | Median R 43.0\n",
      "Episode   40 | Average R   36.2 | Median R 38.0\n",
      "Episode   50 | Average R  156.9 | Median R 180.5\n",
      "Episode   60 | Average R    200 | Median R 200.0\n",
      "Episode   70 | Average R    200 | Median R 200.0\n",
      "Episode   80 | Average R  161.4 | Median R 146.0\n",
      "Episode   90 | Average R  120.1 | Median R 120.5\n",
      "Episode  100 | Average R  123.7 | Median R 120.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "agent = ActorCritic(env)\n",
    "agent.run(100,fit_data='per_step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode   10 | Average R  138.3 | Median R 138.5\n",
      "Episode   20 | Average R  137.4 | Median R 139.0\n",
      "Episode   30 | Average R  135.8 | Median R 137.5\n",
      "Episode   40 | Average R  137.1 | Median R 137.0\n",
      "Episode   50 | Average R  137.1 | Median R 138.0\n",
      "Episode   60 | Average R  138.1 | Median R 140.0\n",
      "Episode   70 | Average R  138.6 | Median R 137.5\n",
      "Episode   80 | Average R    139 | Median R 137.5\n",
      "Episode   90 | Average R  138.7 | Median R 138.5\n",
      "Episode  100 | Average R    135 | Median R 134.0\n"
     ]
    }
   ],
   "source": [
    "# evaluation\n",
    "agent.run(100,fit_data=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN value network on raw pixels\n",
    "https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "\n",
    "network input: `state (raw pixels)` output: `Q(a|state)`"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "        self.model = tf.keras.models.Sequential()\n",
    "        self.model.add(tf.keras.layers.Conv2D(16, kernel_size=5, strides=2, input_shape=screen_shape, activation='relu'))\n",
    "        self.model.add(tf.keras.layers.BatchNormalization())\n",
    "        self.model.add(tf.keras.layers.Conv2D(32, kernel_size=5, strides=2, input_shape=screen_shape, activation='relu'))\n",
    "        self.model.add(tf.keras.layers.BatchNormalization())\n",
    "        self.model.add(tf.keras.layers.Conv2D(32, kernel_size=5, strides=2, input_shape=screen_shape, activation='relu'))\n",
    "        self.model.add(tf.keras.layers.BatchNormalization())\n",
    "        self.model.add(tf.keras.layers.Flatten())\n",
    "#         self.model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "        self.model.add(tf.keras.layers.Dense(self.env.action_space.n, activation='linear'))\n",
    "        self.model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(.001)) #optimizer has its parameters\n",
    "        \n",
    "\n",
    "    def get_screen(self):\n",
    "        def get_cart_location(screen_width):\n",
    "            world_width = env.x_threshold * 2\n",
    "            scale = screen_width / world_width\n",
    "            return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
    "        screen = self.env.render(mode='rgb_array')\n",
    "        # Cart is in the lower half, so strip off the top and bottom of the screen\n",
    "        screen_height, screen_width, _ = screen.shape\n",
    "        screen = screen[int(screen_height*0.4):int(screen_height * 0.8)]\n",
    "        view_width = int(screen_width * 0.6)\n",
    "        cart_location = get_cart_location(screen_width)\n",
    "        # Strip off the edges, so that we have a square image centered on a cart\n",
    "        if cart_location < view_width // 2:\n",
    "            slice_range = slice(view_width)\n",
    "        elif cart_location > (screen_width - view_width // 2):\n",
    "            slice_range = slice(-view_width, None)\n",
    "        else:\n",
    "            slice_range = slice(cart_location - view_width // 2,\n",
    "                                cart_location + view_width // 2)\n",
    "        screen = screen[:, slice_range,:]\n",
    "        screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "        return screen\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "315.4px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
