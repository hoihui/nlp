{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0-beta1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "from collections import deque # for finite memory \n",
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementations\n",
    "* `env` must support the methods `.reset()`, `.step()` with returns like `gym`'s\n",
    "* if `env` is not `gym`'s environment, must provide `state_shape` and `action_dim`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN -- deep network for action-value function (Q) with off-policy learning\n",
    "* network input: `state (raw pixels)` output: `Q(a|state)`\n",
    "* target are the Q-values by target_network on *the state/action actually followed*. Others not used for training (just set the target to what the model predicts now, so loss is zero)\n",
    "* the loss `mse` is chosen such that gradient of loss is the update we want on the parameters: $-\\nabla[r+\\gamma\\max_{a'}Q(s',a')-\\hat{Q}_{\\bf w}(s,a)]^2=[r+\\gamma\\max_{a'}Q(s',a')-\\hat{Q}_{\\bf w}(s,a)]\\nabla_{\\bf w}\\hat{Q}_{\\bf w}\\rightarrow\\Delta {\\bf w}$\n",
    "\n",
    "[Three improvements](https://www.youtube.com/watch?v=EX1CIVVkWdE):\n",
    "1. **Experience Replay**: If we only use most recent states, the training samples are highly correlated (all coming from the same episodes). Store all intermediate states; after each step, train a random batch from memory.\n",
    "2. **Target Network**: Training is difficult to converge if the target is constantly shifting. The TD target is output from another network that got updated to the most up-to-date value network once in a while.\n",
    "3. **Huber Loss**: Avoid exploding gradients by using Huber loss instead of `mse`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(object):  #model+memory\n",
    "    def __init__(self,env,state_shape=None, action_dim=None,\n",
    "                 gamma=.9,max_memory=5000):\n",
    "        self.memory = deque(maxlen=max_memory)\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.action_dim = action_dim or self.env.action_space.n\n",
    "        self.state_shape = state_shape or self.env.observation_space.shape\n",
    "        \n",
    "        self.model = tf.keras.models.Sequential([\n",
    "                         tf.keras.layers.Dense(96, input_shape=self.state_shape, activation='relu'),\n",
    "                         tf.keras.layers.Dense(48, activation='relu'),\n",
    "                         tf.keras.layers.Dense(self.action_dim),\n",
    "                        ])\n",
    "        self.model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam())     # learning rate handled by optimizer\n",
    "        self.target_model = tf.keras.models.clone_model(self.model)\n",
    "    \n",
    "    def get_epsilon(self, episode):\n",
    "        ############# EPSILON to use for epsilon-greedy (probability of exploration)#########\n",
    "#         return 1/(1+e*.2)\n",
    "#         return max(.01, 0.995**episode)\n",
    "        return 0.1\n",
    "        #####################################################################################\n",
    "\n",
    "    def choose_action(self, state, epsilon):\n",
    "        if np.random.random() <= epsilon:\n",
    "            return np.random.randint(0, self.action_dim)\n",
    "        else:\n",
    "            return np.argmax(self.model.predict(state[np.newaxis])[0])\n",
    "        \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def replay_train(self, batch_size=50):\n",
    "        batch = random.choices( self.memory, k=batch_size)\n",
    "        states,actions,rewards,nextstates,dones = map(np.array,zip(*batch))\n",
    "        states=states.reshape(len(rewards),-1)\n",
    "        ys=self.model.predict(states)             # current estimation of the Q(a|s)\n",
    "        qs=self.target_model.predict(nextstates)  # Q(a|s') for next state (target is r+g*max(this) for the action taken, otherwise use current estimation as the target)\n",
    "        \n",
    "#         X, Y = [], [] \n",
    "#         for i,(state, action, reward, next_state, done) in enumerate(batch):\n",
    "#             y = ys[i]  # = self.model.predict(state[np.newaxis])[0]\n",
    "#             q = qs[i]  # = self.model.predict(next_state[np.newaxis])[0]\n",
    "#             y[action] = reward + (0 if done else self.gamma*np.max(q)) # R_t + gam * max_a' Q(s',a') ONLY for the action executed; others remain unchanged from current prediction\n",
    "#             X += state,\n",
    "#             Y += y,\n",
    "#         return self.model.train_on_batch(np.array(X), np.array(Y))   \n",
    "\n",
    "        ys[np.arange(ys.shape[0]),actions] = rewards + (1-dones)*self.gamma*np.max(qs,axis=1) #equivalent to above\n",
    "        return self.model.train_on_batch(states, ys)\n",
    "              \n",
    "    def run(self,episodes=1000,show_every=None,fit_data='per_step',update_target_every=2):\n",
    "        show_every = show_every or episodes//10\n",
    "        scores = deque(maxlen=show_every) #store new episodes after previous print\n",
    "\n",
    "        for e in range(1,episodes+1):\n",
    "            EPSILON = 0 if not fit_data else self.get_epsilon(e)\n",
    "            state, done = self.env.reset(), False\n",
    "            R = 0\n",
    "            while not done:\n",
    "                action = self.choose_action(state, EPSILON)\n",
    "                next_state, reward, done,_ = self.env.step(action)\n",
    "                R += reward\n",
    "                self.remember(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                if fit_data=='per_step': self.replay_train()\n",
    "            if fit_data=='per_episode': self.replay_train()\n",
    "            scores+=R,\n",
    "                     \n",
    "            if e%show_every == 0:\n",
    "                print(f'Episode {e:4d} | Average R {np.mean(scores):6.4g} | Median R {np.median(scores)}')\n",
    "            if e%update_target_every==0:  #update frequency of target network\n",
    "                self.target_model.set_weights(self.model.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DRQN -- deep recurrent Q network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TimeDistributed\n",
    "\n",
    "#####choose 'Use all steps for training' or 'Use last step for training' below (3 places)#####\n",
    "\n",
    "class DRQN(object):  #model+memory\n",
    "    def __init__(self,env,state_shape=None, action_dim=None,\n",
    "                 nstep=8, ncell=256,\n",
    "                 gamma=.9,max_memory=5000):\n",
    "        self.memory = deque(maxlen=max_memory)\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.action_dim = action_dim or self.env.action_space.n\n",
    "        self.state_shape = state_shape or self.env.observation_space.shape\n",
    "        self.nstep = nstep\n",
    "        \n",
    "        self.model = tf.keras.models.Sequential([        #input = (batch_size,nstep,state_shape)\n",
    "                         TimeDistributed(tf.keras.layers.Dense(96, input_shape=self.state_shape, activation='relu'),\n",
    "                                         input_shape=(nstep,self.state_shape[0])),\n",
    "                         tf.keras.layers.LSTM(ncell, return_sequences=True, activation='tanh'), # Use all steps for training\n",
    "                         TimeDistributed(tf.keras.layers.Dense(self.action_dim)),\n",
    "#                          tf.keras.layers.LSTM(ncell, activation='tanh'),                        # Use last step for training\n",
    "#                          tf.keras.layers.Dense(self.action_dim),\n",
    "                        ])\n",
    "        self.model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam())     # learning rate handled by optimizer\n",
    "        self.target_model = tf.keras.models.clone_model(self.model)\n",
    "    \n",
    "    def get_epsilon(self, episode):\n",
    "        return max(.01, 0.99**episode)\n",
    "\n",
    "    def choose_action(self, states, epsilon):\n",
    "        if states.shape[0]<self.nstep or np.random.random() <= epsilon:\n",
    "            return np.random.randint(0, self.action_dim)\n",
    "        else:\n",
    "            ret = self.model.predict(states[np.newaxis])[0]\n",
    "            return np.argmax(ret[-1]) # Use all steps for training\n",
    "#             return np.argmax(ret)     # Use last step for training\n",
    "        \n",
    "    def remember(self, episode):#state, action, reward, next_state, done):\n",
    "        self.memory.append(episode)#(state, action, reward, next_state, done))\n",
    "\n",
    "    def replay_train(self, batch_size=32):\n",
    "        if len(self.memory)==0: return 1\n",
    "        episodes = random.choices( self.memory, k=batch_size) #(bs,episode_length)\n",
    "        \n",
    "        states = np.zeros((batch_size,self.nstep)+self.state_shape) # bs,nstep,statedim\n",
    "        nextstates = np.zeros((batch_size,self.nstep)+self.state_shape)\n",
    "        actions = np.zeros((batch_size, self.nstep),dtype=int)                # bs*nstep\n",
    "        rewards = np.zeros((batch_size, self.nstep))\n",
    "        dones = np.zeros((batch_size, self.nstep),dtype=int)\n",
    "        \n",
    "        for i,episode in enumerate(episodes):\n",
    "            if len(episode)<self.nstep: continue\n",
    "            start = random.randint(0,len(episode)-self.nstep)\n",
    "            trace = episode[start:start+self.nstep]\n",
    "            states[i],actions[i],rewards[i],nextstates[i],dones[i] = map(np.array,zip(*trace))\n",
    "        \n",
    "        ############ Use all steps for training ############\n",
    "        ys = self.model.predict(states).reshape(batch_size*self.nstep,self.action_dim)            # bs,nstep,action_dim -> bs*nstep,action_dim\n",
    "        qs = self.target_model.predict(nextstates).reshape(batch_size*self.nstep,self.action_dim) # bs,nstep,action_dim -> bs*nstep,action_dim\n",
    "        ys[np.arange(ys.shape[0]),actions.flatten()] = rewards.flatten() + (1-dones.flatten())*self.gamma*np.max(qs,axis=1).flatten()\n",
    "        ys = ys.reshape(batch_size,self.nstep,self.action_dim)\n",
    "        \n",
    "        ############ Use last step for training############\n",
    "#         ys = self.model.predict(states)           # bs,action_dim\n",
    "#         qs = self.target_model.predict(nextstates)# bs,action_dim\n",
    "#         ys[np.arange(ys.shape[0]),actions[:,-1]] = rewards[:,-1] + (1-dones[:,-1])*self.gamma*np.max(qs,axis=1)\n",
    "        \n",
    "        return self.model.train_on_batch(states, ys)\n",
    "              \n",
    "    def run(self,episodes=1000,show_every=None,fit_data='per_step',update_target_every=2):\n",
    "        show_every = show_every or episodes//10\n",
    "        scores = deque(maxlen=show_every) #store new episodes after previous print\n",
    "\n",
    "        for e in range(1,episodes+1):\n",
    "            EPSILON = 0 if not fit_data else self.get_epsilon(e)\n",
    "            episode_memory = []\n",
    "            state, done = self.env.reset(), False\n",
    "            R = 0\n",
    "            while not done:\n",
    "                action = self.choose_action(np.array([e[3] for e in episode_memory[-self.nstep:]]),\n",
    "                                            EPSILON)\n",
    "                next_state, reward, done,_ = self.env.step(action)\n",
    "                R += reward\n",
    "                episode_memory += (state, action, reward, next_state, done),\n",
    "                state = next_state\n",
    "                if fit_data=='per_step': self.replay_train()\n",
    "            self.remember(episode_memory)\n",
    "            if fit_data=='per_episode': self.replay_train()\n",
    "            scores+=R,\n",
    "                     \n",
    "            if e%show_every == 0:\n",
    "                print(f'Episode {e:4d} | Average R {np.mean(scores):6.4g} | Median R {np.median(scores)}')\n",
    "            if e%update_target_every==0:  #update frequency of target network\n",
    "                self.target_model.set_weights(self.model.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Policy Network, MC\n",
    "* no epsilon-greedy necessary since network output softmax (probabilities), not deterministic\n",
    "* loss `categorical_crossentropy` is chosen such that gradient of loss is the update we want on the network parameters $\\theta$:\n",
    "\n",
    "  Loss $L(v, \\hat{p}) = -\\sum_av_a \\log \\hat{p}_a$, where the targets used are $v_a$=value or advantage following action $a$ that was actually taken (zeros for other non-taken actions), and $\\hat{p}_a$ is output from policy network for the probability of choosing action $a$. So $L=-v_t\\log\\pi_\\theta(s,a)$, and $-\\nabla_\\theta L=v_t\\nabla_\\theta\\log\\pi_\\theta(s,a)\\rightarrow\\Delta\\theta$, exactly what wanted.\n",
    "* use the mean return of the whole episode as the baseline -- a constant value\n",
    "* calculate the correct targets at the end of an episode, then add to memory for replay\n",
    "* cannot use TD as there is no estimation of value function available\n",
    "* train batch size should be ~ episode length? train once per episode or per step?\n",
    "* ?not good for game that does have reward only at the end?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(object):  #model+memory\n",
    "    def __init__(self,env,state_shape=None, action_dim=None,\n",
    "                 gamma=.9,max_memory=None):\n",
    "        self.memory = deque(maxlen=max_memory)\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.action_dim = action_dim or self.env.action_space.n\n",
    "        self.state_shape = state_shape or self.env.observation_space.shape\n",
    "        \n",
    "        self.model = tf.keras.models.Sequential([\n",
    "                         tf.keras.layers.Dense(24, input_shape=self.state_shape, activation='relu'),\n",
    "                         tf.keras.layers.Dense(24, activation='relu'),\n",
    "                         tf.keras.layers.Dense(self.action_dim, activation='softmax'),\n",
    "                        ])\n",
    "        self.model.compile(loss=\"categorical_crossentropy\", optimizer=tf.keras.optimizers.RMSprop())\n",
    "        \n",
    "    def choose_action(self, state):\n",
    "        policy = self.model.predict(state[np.newaxis])[0]\n",
    "        return np.random.choice(self.action_dim, p=policy)\n",
    "\n",
    "    def remember(self,mem):\n",
    "        states,actions,Gs = zip(*mem)\n",
    "        \n",
    "        Gs = np.asarray(Gs,dtype=np.float32) #convert rewards to returns\n",
    "        G = 0  \n",
    "        for t in reversed(range(len(states))):\n",
    "            G = G * self.gamma + Gs[t]\n",
    "            Gs[t] = G            \n",
    "        Gs -= np.mean(Gs)\n",
    "        Gs /= np.std(Gs)+1e-12    #subtract a uniform value \n",
    "        \n",
    "        Y = np.zeros((len(states), self.action_dim))\n",
    "        for t in range(len(states)):\n",
    "            Y[t,actions[t]] = Gs[t]\n",
    "        \n",
    "        self.memory.extend(zip(states,Y))\n",
    "#         self.memory=list(zip(states,Y))           #only keep last episode\n",
    "        \n",
    "    def replay_train(self, batch_size=64):\n",
    "        if not self.memory: return\n",
    "        batch = random.choices( self.memory, k=batch_size)\n",
    "        X,Y = map(np.asarray,zip(*batch))        \n",
    "        return self.model.train_on_batch(X,Y)\n",
    "              \n",
    "    def run(self,episodes=1000,show_every=None,fit_data='per_step'):\n",
    "        show_every = show_every or episodes//10\n",
    "        scores = deque(maxlen=show_every)\n",
    "        \n",
    "        for e in range(1,episodes+1):\n",
    "            state,done = self.env.reset(),False\n",
    "            short_term_mem = []\n",
    "            R = 0\n",
    "            while not done:\n",
    "                action = self.choose_action(state)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                R += reward\n",
    "                short_term_mem += (state, action, reward),\n",
    "                state = next_state\n",
    "                if done: self.remember(short_term_mem)\n",
    "                if fit_data=='per_step': self.replay_train()\n",
    "            if fit_data=='per_episode': self.replay_train()\n",
    "            scores+=R,\n",
    "            \n",
    "            if e%show_every == 0:\n",
    "                print(f'Episode {e:4d} | Average R {np.mean(scores):6.4g} | Median R {np.median(scores)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A2C = Deep policy+value (Actor-Critic) networks\n",
    "* Policy network predicts probabilities of actions given state, $\\pi_\\theta(a|s)$. Value network estimates *state*-value function  for the given state $V_v(s)$\n",
    "* Policy network loss is $L= -\\sum_a[r+\\gamma V_v(s_{t+1})-V_v(s_t)]\\log\\pi_\\theta(a|s)$, so $-\\nabla_\\theta L=[r+\\gamma V_v(s_{t+1})-V_v(s_t)]\\nabla_\\theta\\log\\pi_\\theta(s,a)\\rightarrow\\Delta\\theta$\n",
    "* Value network loss is $L= [r+\\gamma V_v(s_{t+1})-V_v(s_t)]^2$, so $-\\nabla_v L =[r+\\gamma V_v(s_{t+1})-V_v(s_t)]\\nabla_v V_v(s_t)\\rightarrow\\Delta {\\bf v}$\n",
    "* update by TD(0) now as there is a network to give estimation of V(s)\n",
    "* two networks can have `### shared trunk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(object):  #models+memory\n",
    "    def __init__(self,env,state_shape=None, action_dim=None,\n",
    "                 gamma=.9,\n",
    "                 max_memory=5000):\n",
    "        self.memory = deque(maxlen=max_memory)\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.action_dim = action_dim or self.env.action_space.n\n",
    "        self.state_shape = state_shape or self.env.observation_space.shape\n",
    "        \n",
    "        self.actor = tf.keras.models.Sequential([\n",
    "                         tf.keras.layers.Dense(48, input_shape=self.state_shape,\n",
    "                                                               kernel_initializer='he_uniform', activation='relu'),\n",
    "                         tf.keras.layers.Dense(self.action_dim,kernel_initializer='he_uniform', activation='softmax'),\n",
    "                     ])\n",
    "        self.actor.compile(loss=\"categorical_crossentropy\", optimizer=tf.keras.optimizers.Adam(lr=0.001))\n",
    "        \n",
    "        self.critic = tf.keras.models.Sequential([\n",
    "                         tf.keras.layers.Dense(48, input_shape=self.state_shape,\n",
    "                                                  kernel_initializer='he_uniform', activation='relu'),\n",
    "                         tf.keras.layers.Dense(1, kernel_initializer='he_uniform'),\n",
    "                     ])\n",
    "        self.critic.compile(loss=\"mse\", optimizer=tf.keras.optimizers.Adam(lr=0.005))\n",
    "        \n",
    "        x = tf.keras.layers.Input(shape=self.state_shape)              #### shared trunk\n",
    "        y = tf.keras.layers.Dense(48,                kernel_initializer='he_uniform', activation='relu')(x)\n",
    "        pol = tf.keras.layers.Dense(48,              kernel_initializer='he_uniform', activation='relu')(y)\n",
    "        pol = tf.keras.layers.Dense(self.action_dim, kernel_initializer='he_uniform', activation='softmax')(pol)\n",
    "        val = tf.keras.layers.Dense(48,              kernel_initializer='he_uniform', activation='relu')(y)\n",
    "        val = tf.keras.layers.Dense(1,               kernel_initializer='he_uniform')(val)\n",
    "        self.actor_critic = tf.keras.models.Model(x,[pol,val])\n",
    "        self.actor_critic.compile(optimizer='adam', loss=['binary_crossentropy','mse'])\n",
    "        \n",
    "    def choose_action(self, state):\n",
    "        policy = self.actor.predict(state[np.newaxis])[0]            #### separate networks\n",
    "#         policy = self.actor_critic.predict(state[np.newaxis])[0][0]    #### shared trunk\n",
    "        return np.random.choice(self.action_dim, p=policy)\n",
    "\n",
    "    def remember(self,state, action, reward, next_state, done):\n",
    "        self.memory += (state, action, reward, next_state, done),\n",
    "        \n",
    "    def replay_train(self, batch_size=32):\n",
    "        if not self.memory: return\n",
    "        batch = random.choices( self.memory, k=batch_size)\n",
    "        states,actions,rewards,next_states,dones = map(np.array,zip(*batch))\n",
    "        states = states.reshape(len(rewards),-1)     \n",
    "        \n",
    "        value_target = np.zeros((batch_size, 1))\n",
    "        advantages   = np.zeros((batch_size, self.action_dim))\n",
    "        \n",
    "        curr_values = self.critic.predict(states).squeeze()         #### separate networks\n",
    "        next_values = self.critic.predict(next_states).squeeze()        \n",
    "#         curr_values = self.actor_critic.predict(states)[1].squeeze()  #### shared trunk\n",
    "#         next_values = self.actor_critic.predict(next_states)[1].squeeze()\n",
    "        \n",
    "        advantages[np.arange(batch_size),actions] = rewards + (1-dones)*self.gamma*next_values - curr_values\n",
    "        value_target[:,0] = rewards + (1-dones)*self.gamma*next_values\n",
    "    \n",
    "        return (self.actor.train_on_batch(states,advantages),self.critic.train_on_batch(states,value_target),) #### separate networks\n",
    "#         return self.actor_critic.train_on_batch(states,[advantages,value_target])  #### shared trunk\n",
    "              \n",
    "    def run(self,episodes=1000,show_every=None,fit_data='per_step'):\n",
    "        show_every = show_every or episodes//10\n",
    "        scores = deque(maxlen=show_every)\n",
    "        \n",
    "        for e in range(1,episodes+1):\n",
    "            state,done = self.env.reset(),False\n",
    "            short_term_mem = []\n",
    "            R = 0\n",
    "            while not done:\n",
    "                action = self.choose_action(state)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                R += reward\n",
    "                self.remember(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                if fit_data=='per_step': self.replay_train(128)\n",
    "            if fit_data=='per_episode': self.replay_train(128)\n",
    "            scores+=R,\n",
    "            \n",
    "            if e%show_every == 0:\n",
    "                print(f'Episode {e:4d} | Average R {np.mean(scores):6.4g} | Median R {np.median(scores)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A3C = Asynchronous A2C\n",
    "* multiple A2C agents running and updating weights simultaneously\n",
    "* ref: https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/rlcode/reinforcement-learning/blob/master/2-cartpole/5-a3c/cartpole_a3c.py\n",
    "import threading, copy\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# global variables for threading\n",
    "episode = 0\n",
    "scores = []\n",
    "EPISODES = 2000\n",
    "\n",
    "class A3CAgent:\n",
    "    def __init__(self, env, state_shape=None, action_dim=None,gamma=.9,\n",
    "                threads=4):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.action_dim = action_dim or self.env.action_space.n\n",
    "        self.state_shape = state_shape or self.env.observation_space.shape\n",
    "\n",
    "        self.threads = threads\n",
    "        \n",
    "        # models\n",
    "        state = Input(batch_shape=(None,)+self.state_shape )\n",
    "        shared = Dense(24, input_dim=self.state_shape, activation='relu', kernel_initializer='glorot_uniform')(state)\n",
    "\n",
    "        actions = Dense(24, activation='relu', kernel_initializer='glorot_uniform')(shared)\n",
    "        actions = Dense(self.action_dim, activation='softmax', kernel_initializer='glorot_uniform')(actions)\n",
    "\n",
    "        value = Dense(24, activation='relu', kernel_initializer='he_uniform')(shared)\n",
    "        value = Dense(1, activation='linear', kernel_initializer='he_uniform')(value)\n",
    "\n",
    "        self.actor = Model(state, actions)\n",
    "        self.critic = Model(state, value)\n",
    "\n",
    "        actor._make_predict_function()  #for thread-safety\n",
    "        critic._make_predict_function()\n",
    "\n",
    "        actor.summary()\n",
    "        critic.summary()\n",
    "\n",
    "        # optimizers\n",
    "        # [log(action probability) * advantages] will be input for the back prop\n",
    "        # add entropy of action probability to loss\n",
    "        self.optimizer = [self.actor_optimizer(), self.critic_optimizer()]\n",
    "\n",
    "        self.sess = tf.InteractiveSession()\n",
    "        K.set_session(self.sess)\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # make loss function for Policy Gradient\n",
    "    def actor_optimizer(self):\n",
    "        action = K.placeholder(shape=(None, self.action_size))\n",
    "        advantages = K.placeholder(shape=(None, ))\n",
    "\n",
    "        policy = self.actor.output\n",
    "\n",
    "        good_prob = K.sum(action * policy, axis=1)\n",
    "        eligibility = K.log(good_prob + 1e-10) * K.stop_gradient(advantages)\n",
    "        loss = -K.sum(eligibility)\n",
    "\n",
    "        entropy = K.sum(policy * K.log(policy + 1e-10), axis=1)\n",
    "\n",
    "        actor_loss = loss + 0.01*entropy\n",
    "\n",
    "        optimizer = Adam(lr=self.actor_lr)\n",
    "        updates = optimizer.get_updates(self.actor.trainable_weights, [], actor_loss)\n",
    "        train = K.function([self.actor.input, action, advantages], [], updates=updates)\n",
    "        return train\n",
    "\n",
    "    # make loss function for Value approximation\n",
    "    def critic_optimizer(self):\n",
    "        discounted_reward = K.placeholder(shape=(None, ))\n",
    "\n",
    "        value = self.critic.output\n",
    "\n",
    "        loss = K.mean(K.square(discounted_reward - value))\n",
    "\n",
    "        optimizer = Adam(lr=self.critic_lr)\n",
    "        updates = optimizer.get_updates(self.critic.trainable_weights, [], loss)\n",
    "        train = K.function([self.critic.input, discounted_reward], [], updates=updates)\n",
    "        return train\n",
    "\n",
    "    # make agents(local) and start training\n",
    "    def train(self):\n",
    "        agents = [Agent(i, self.actor, self.critic, self.optimizer, self.env_name, self.discount_factor,\n",
    "                           self.action_size, self.state_size) for i in range(self.threads)]\n",
    "\n",
    "        for agent in agents:\n",
    "            agent.start()\n",
    "\n",
    "        while True:\n",
    "            time.sleep(20)\n",
    "\n",
    "            plot = scores[:]\n",
    "            pylab.plot(range(len(plot)), plot, 'b')\n",
    "            pylab.savefig(\"./save_graph/cartpole_a3c.png\")\n",
    "\n",
    "# This is Agent(local) class for threading\n",
    "class Agent(threading.Thread):\n",
    "    def __init__(self, index, actor, critic, optimizer, env_name, discount_factor, action_size, state_size):\n",
    "        threading.Thread.__init__(self)\n",
    "\n",
    "        self.states = []\n",
    "        self.rewards = []\n",
    "        self.actions = []\n",
    "\n",
    "        self.index = index\n",
    "        self.actor = actor\n",
    "        self.critic = critic\n",
    "        self.optimizer = optimizer\n",
    "        self.env_name = env_name\n",
    "        self.discount_factor = discount_factor\n",
    "        self.action_size = action_size\n",
    "        self.state_size = state_size\n",
    "\n",
    "    # Thread interactive with environment\n",
    "    def run(self):\n",
    "        global episode\n",
    "        env = gym.make(self.env_name)\n",
    "        while episode < EPISODES:\n",
    "            state = env.reset()\n",
    "            score = 0\n",
    "            while True:\n",
    "                action = self.get_action(state)\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                score += reward\n",
    "\n",
    "                self.memory(state, action, reward)\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "                if done:\n",
    "                    episode += 1\n",
    "                    print(\"episode: \", episode, \"/ score : \", score)\n",
    "                    scores.append(score)\n",
    "                    self.train_episode(score != 500)\n",
    "                    break\n",
    "\n",
    "    # In Policy Gradient, Q function is not available.\n",
    "    # Instead agent uses sample returns for evaluating policy\n",
    "    def discount_rewards(self, rewards, done=True):\n",
    "        discounted_rewards = np.zeros_like(rewards)\n",
    "        running_add = 0\n",
    "        if not done:\n",
    "            running_add = self.critic.predict(np.reshape(self.states[-1], (1, self.state_size)))[0]\n",
    "        for t in reversed(range(0, len(rewards))):\n",
    "            running_add = running_add * self.discount_factor + rewards[t]\n",
    "            discounted_rewards[t] = running_add\n",
    "        return discounted_rewards\n",
    "\n",
    "    # save <s, a ,r> of each step\n",
    "    # this is used for calculating discounted rewards\n",
    "    def memory(self, state, action, reward):\n",
    "        self.states.append(state)\n",
    "        act = np.zeros(self.action_size)\n",
    "        act[action] = 1\n",
    "        self.actions.append(act)\n",
    "        self.rewards.append(reward)\n",
    "\n",
    "    # update policy network and value network every episode\n",
    "    def train_episode(self, done):\n",
    "        discounted_rewards = self.discount_rewards(self.rewards, done)\n",
    "\n",
    "        values = self.critic.predict(np.array(self.states))\n",
    "        values = np.reshape(values, len(values))\n",
    "\n",
    "        advantages = discounted_rewards - values\n",
    "\n",
    "        self.optimizer[0]([self.states, self.actions, advantages])\n",
    "        self.optimizer[1]([self.states, discounted_rewards])\n",
    "        self.states, self.actions, self.rewards = [], [], []\n",
    "\n",
    "    def get_action(self, state):\n",
    "        policy = self.actor.predict(np.reshape(state, [1, self.state_size]))[0]\n",
    "        return np.random.choice(self.action_size, 1, p=policy)[0]\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     env_name = 'CartPole-v1'\n",
    "#     env = gym.make(env_name)\n",
    "\n",
    "#     state_size = env.observation_space.shape[0]\n",
    "#     action_size = env.action_space.n\n",
    "\n",
    "#     env.close()\n",
    "\n",
    "#     global_agent = A3CAgent(state_size, action_size, env_name)\n",
    "#     global_agent.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Catch](https://gist.github.com/EderSantana/c7222daa328f0e885093) using raw pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Catch(object): # 1 game is 1 fruit dropped from top to bottom\n",
    "    def __init__(self, grid_size=10):\n",
    "        self.grid_size = grid_size\n",
    "        self.basketSize = 1\n",
    "        \n",
    "    def reset(self):\n",
    "        n = np.random.randint(0, self.grid_size-1)                # starting fruit_col\n",
    "        m = np.random.randint(0, self.grid_size-self.basketSize)  # starting basket col\n",
    "        self.state = np.asarray([0, n, m])                        # [fruit_row, fruit_col, basket's left end]\n",
    "        return self.observe()\n",
    "    \n",
    "    def _get_reward(self):   # inc/dec score only if fruit has dropped to bottom\n",
    "        fruit_row, fruit_col, basket_left = self.state\n",
    "        if fruit_row == self.grid_size-1 and basket_left <= fruit_col < basket_left+self.basketSize:\n",
    "            return 1.\n",
    "        else:\n",
    "            return 0.\n",
    "\n",
    "    def _is_over(self):    # game over if fruit dropped to bottom\n",
    "        return (self.state[0] == self.grid_size-1)\n",
    "    \n",
    "    def observe(self):\n",
    "        im_size = (self.grid_size, self.grid_size)\n",
    "        state = self.state\n",
    "        canvas = np.zeros(im_size)\n",
    "        canvas[self.state[0], self.state[1]] = 1                         # draw fruit\n",
    "        canvas[-1, self.state[2]:self.state[2] + self.basketSize+1] = 1  # draw basket\n",
    "        return canvas.flatten()\n",
    "    \n",
    "    def step(self, action):\n",
    "        if action == 0:   action = -1 # move left\n",
    "        elif action == 1: action =  0 # stay\n",
    "        else:             action =  1 # move right\n",
    "        f0, f1, basket_left = self.state\n",
    "        new_basket_left = min(max(0, basket_left + action), self.grid_size-self.basketSize)\n",
    "        f0 += 1                       # fruit dropped by one pixel\n",
    "        out = np.asarray([f0, f1, new_basket_left])\n",
    "        self.state = out\n",
    "        \n",
    "        return self.observe(), self._get_reward(), self._is_over(), None # returns whole canvas, R, done?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`gym`'s environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Catch(gym.Env):   # 1 game is 1 fruit dropped from top to bottom. agent at bottom row to catch it\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, grid_size=10,basket_size=1):\n",
    "        super(Catch, self).__init__()\n",
    "        \n",
    "        self.action_space = gym.spaces.Discrete(3)\n",
    "        self.observation_space = gym.spaces.MultiBinary(grid_size*grid_size)\n",
    "        \n",
    "        self.grid_size = grid_size\n",
    "        self.basket_size = basket_size\n",
    "\n",
    "    def reset(self):           # Reset the state of the environment to an initial state\n",
    "        n = np.random.randint(0, self.grid_size-1)                # starting fruit_col\n",
    "        m = np.random.randint(0, self.grid_size-self.basket_size)  # starting basket col\n",
    "        self.state = np.asarray([0, n, m])                        # [fruit_row, fruit_col, basket's left end]\n",
    "        return self._observe()\n",
    "    \n",
    "    def step(self, action):    # Execute one time step within the environment\n",
    "        if action == 0:   action = -1 # move left\n",
    "        elif action == 1: action =  0 # stay\n",
    "        else:             action =  1 # move right\n",
    "        f0, f1, basket_left = self.state\n",
    "        new_basket_left = min(max(0, basket_left + action), self.grid_size-self.basket_size)\n",
    "        f0 += 1                       # fruit dropped by one pixel\n",
    "        out = np.asarray([f0, f1, new_basket_left])\n",
    "        self.state = out\n",
    "        \n",
    "        return self._observe(), self._get_reward(), self._is_over(), None # returns whole canvas, R, done?\n",
    "    \n",
    "    def render(self, mode='human', close=False):        # Render the environment to the screen\n",
    "        print(state)\n",
    "    #############################helper methods---not required by gym.env##################################\n",
    "    def _get_reward(self):   # inc/dec score only if fruit has dropped to bottom\n",
    "        fruit_row, fruit_col, basket_left = self.state\n",
    "        return fruit_row == self.grid_size-1 and basket_left <= fruit_col < basket_left+self.basket_size\n",
    "\n",
    "    def _is_over(self):    # game over if fruit dropped to bottom\n",
    "        return (self.state[0] == self.grid_size-1)\n",
    "    \n",
    "    def _observe(self):\n",
    "        im_size = (self.grid_size, self.grid_size)\n",
    "        state = self.state\n",
    "        canvas = np.zeros(im_size)\n",
    "        canvas[self.state[0], self.state[1]] = 1                          # draw fruit\n",
    "        canvas[-1, self.state[2]:self.state[2] + self.basket_size+1] = 1  # draw basket\n",
    "        return canvas.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Various Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  200 | Average R  0.155 | Median R 0.0\n",
      "Episode  400 | Average R  0.245 | Median R 0.0\n",
      "Episode  600 | Average R   0.37 | Median R 0.0\n",
      "Episode  800 | Average R  0.525 | Median R 1.0\n",
      "Episode 1000 | Average R  0.555 | Median R 1.0\n",
      "Episode 1200 | Average R   0.58 | Median R 1.0\n",
      "Episode 1400 | Average R   0.58 | Median R 1.0\n",
      "Episode 1600 | Average R  0.565 | Median R 1.0\n",
      "Episode 1800 | Average R   0.73 | Median R 1.0\n",
      "Episode 2000 | Average R   0.76 | Median R 1.0\n"
     ]
    }
   ],
   "source": [
    "env = Catch(grid_size = 10)\n",
    "agent = DQN(env)\n",
    "agent.run(2000,fit_data='per_episode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  200 | Average R  0.115 | Median R 0.0\n",
      "Episode  400 | Average R  0.175 | Median R 0.0\n",
      "Episode  600 | Average R  0.275 | Median R 0.0\n",
      "Episode  800 | Average R  0.345 | Median R 0.0\n",
      "Episode 1000 | Average R  0.575 | Median R 1.0\n",
      "Episode 1200 | Average R   0.65 | Median R 1.0\n",
      "Episode 1400 | Average R  0.715 | Median R 1.0\n",
      "Episode 1600 | Average R  0.775 | Median R 1.0\n",
      "Episode 1800 | Average R   0.73 | Median R 1.0\n",
      "Episode 2000 | Average R   0.79 | Median R 1.0\n"
     ]
    }
   ],
   "source": [
    "env = Catch(grid_size = 10)\n",
    "agent = DRQN(env,nstep=4, ncell=16)\n",
    "agent.run(2000,fit_data='per_episode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0716 12:57:39.315320 4485518784 deprecation.py:323] From /Users/hoi/opt/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  100 | Average R   0.09 | Median R 0.0\n",
      "Episode  200 | Average R   0.11 | Median R 0.0\n",
      "Episode  300 | Average R   0.09 | Median R 0.0\n",
      "Episode  400 | Average R   0.09 | Median R 0.0\n",
      "Episode  500 | Average R   0.07 | Median R 0.0\n",
      "Episode  600 | Average R   0.12 | Median R 0.0\n",
      "Episode  700 | Average R   0.05 | Median R 0.0\n",
      "Episode  800 | Average R   0.07 | Median R 0.0\n",
      "Episode  900 | Average R   0.04 | Median R 0.0\n",
      "Episode 1000 | Average R   0.07 | Median R 0.0\n"
     ]
    }
   ],
   "source": [
    "env = Catch(grid_size = 10)\n",
    "agent = PolicyNet(env)\n",
    "agent.run(1000,fit_data='per_step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  200 | Average R  0.095 | Median R 0.0\n",
      "Episode  400 | Average R  0.115 | Median R 0.0\n",
      "Episode  600 | Average R   0.13 | Median R 0.0\n",
      "Episode  800 | Average R   0.17 | Median R 0.0\n",
      "Episode 1000 | Average R   0.18 | Median R 0.0\n",
      "Episode 1200 | Average R  0.195 | Median R 0.0\n",
      "Episode 1400 | Average R   0.16 | Median R 0.0\n",
      "Episode 1600 | Average R  0.255 | Median R 0.0\n",
      "Episode 1800 | Average R  0.205 | Median R 0.0\n",
      "Episode 2000 | Average R   0.42 | Median R 0.0\n"
     ]
    }
   ],
   "source": [
    "env = Catch(grid_size = 10)\n",
    "agent = ActorCritic(env, state_shape = (100,), action_dim = 3) ##separate networks\n",
    "agent.run(2000,fit_data='per_episode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  200 | Average R   0.07 | Median R 0.0\n",
      "Episode  400 | Average R  0.095 | Median R 0.0\n",
      "Episode  600 | Average R  0.095 | Median R 0.0\n",
      "Episode  800 | Average R    0.1 | Median R 0.0\n",
      "Episode 1000 | Average R  0.185 | Median R 0.0\n",
      "Episode 1200 | Average R   0.18 | Median R 0.0\n",
      "Episode 1400 | Average R   0.23 | Median R 0.0\n",
      "Episode 1600 | Average R   0.24 | Median R 0.0\n",
      "Episode 1800 | Average R   0.23 | Median R 0.0\n",
      "Episode 2000 | Average R  0.235 | Median R 0.0\n"
     ]
    }
   ],
   "source": [
    "env = Catch(grid_size = 10)\n",
    "agent = ActorCritic(env, state_shape = (100,), action_dim = 3) ##shared trunk\n",
    "agent.run(2000,fit_data='per_episode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode   10 | Average R      1 | Median R 1.0\n",
      "Episode   20 | Average R      1 | Median R 1.0\n",
      "Episode   30 | Average R      1 | Median R 1.0\n",
      "Episode   40 | Average R      1 | Median R 1.0\n",
      "Episode   50 | Average R      1 | Median R 1.0\n",
      "Episode   60 | Average R      1 | Median R 1.0\n",
      "Episode   70 | Average R      1 | Median R 1.0\n",
      "Episode   80 | Average R      1 | Median R 1.0\n",
      "Episode   90 | Average R      1 | Median R 1.0\n",
      "Episode  100 | Average R      1 | Median R 1.0\n"
     ]
    }
   ],
   "source": [
    "## Evaluation (epsilon=0)\n",
    "agent.run(100,fit_data=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib import animation, rc\n",
    "from IPython.display import HTML\n",
    "frames = []\n",
    "\n",
    "for e in range(100):\n",
    "    loss = 0.\n",
    "    env.reset()\n",
    "    done = False\n",
    "    state = env.observe()\n",
    "    frames.append(state.reshape(env.grid_size,env.grid_size))\n",
    "    while not done:\n",
    "        q = agent.model.predict(state[np.newaxis])  # q table at current state\n",
    "        action = np.argmax(q[0])\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        frames.append(next_state.reshape(env.grid_size,env.grid_size))\n",
    "        state = next_state\n",
    "# plt.imshow(frames[9],interpolation='none', cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#animation\n",
    "fig, ax = plt.subplots()\n",
    "im  = ax.imshow(np.random.random((env.grid_size,)*2),interpolation='none', cmap='gray')\n",
    "def init():\n",
    "#     im.set_array(np.random.random((grid_size,grid_size)))\n",
    "    return (im,)\n",
    "def animate(i):\n",
    "    im.set_array(frames[i])\n",
    "    return (im,)\n",
    "anim = animation.FuncAnimation(fig, animate, init_func=init,\n",
    "                               frames=len(frames), interval=50, blit=True)\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to files\n",
    "for i in range(len(frames)):\n",
    "    plt.imshow(frames[i],interpolation='none', cmap='gray')\n",
    "    plt.savefig(\"%03d.png\" % i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# [CartPole](https://gym.openai.com/envs/CartPole-v0/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network on default state representation\n",
    "* https://gym.openai.com/evaluations/eval_EIcM1ZBnQW2LBaFN6FY65g/\n",
    "* https://gym.openai.com/evaluations/eval_OeUSZwUcR2qSAqMmOE1UIw/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode   20 | Average R   30.2 | Median R 27.5\n",
      "Episode   40 | Average R     45 | Median R 39.0\n",
      "Episode   60 | Average R  75.35 | Median R 52.0\n",
      "Episode   80 | Average R  150.3 | Median R 139.5\n",
      "Episode  100 | Average R  176.4 | Median R 175.5\n",
      "Episode  120 | Average R  166.8 | Median R 167.5\n",
      "Episode  140 | Average R  167.3 | Median R 167.0\n",
      "Episode  160 | Average R  116.8 | Median R 119.5\n",
      "Episode  180 | Average R  173.4 | Median R 180.5\n",
      "Episode  200 | Average R  182.7 | Median R 200.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')#.unwrapped\n",
    "agent = DQN(env)\n",
    "agent.run(200,fit_data='per_step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode   20 | Average R   23.9 | Median R 19.0\n",
      "Episode   40 | Average R  29.45 | Median R 25.0\n",
      "Episode   60 | Average R  21.75 | Median R 20.5\n",
      "Episode   80 | Average R   40.2 | Median R 40.5\n",
      "Episode  100 | Average R  23.25 | Median R 17.0\n",
      "Episode  120 | Average R   47.3 | Median R 29.5\n",
      "Episode  140 | Average R  150.3 | Median R 161.0\n",
      "Episode  160 | Average R  154.5 | Median R 147.5\n",
      "Episode  180 | Average R  174.3 | Median R 178.5\n",
      "Episode  200 | Average R  151.6 | Median R 153.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')#.unwrapped\n",
    "agent = DRQN(env,nstep=4, ncell=16)\n",
    "agent.run(200,fit_data='per_step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode   20 | Average R   54.3 | Median R 28.0\n",
      "Episode   40 | Average R    197 | Median R 200.0\n",
      "Episode   60 | Average R  117.8 | Median R 122.0\n",
      "Episode   80 | Average R   48.3 | Median R 44.0\n",
      "Episode  100 | Average R  75.85 | Median R 84.0\n",
      "Episode  120 | Average R  129.3 | Median R 141.5\n",
      "Episode  140 | Average R  135.2 | Median R 136.0\n",
      "Episode  160 | Average R  105.7 | Median R 98.0\n",
      "Episode  180 | Average R   70.1 | Median R 71.5\n",
      "Episode  200 | Average R   60.6 | Median R 60.5\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "agent = PolicyNet(env)\n",
    "agent.run(200,fit_data='per_step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode   10 | Average R   13.1 | Median R 12.0\n",
      "Episode   20 | Average R   51.4 | Median R 44.5\n",
      "Episode   30 | Average R   44.4 | Median R 43.0\n",
      "Episode   40 | Average R   36.2 | Median R 38.0\n",
      "Episode   50 | Average R  156.9 | Median R 180.5\n",
      "Episode   60 | Average R    200 | Median R 200.0\n",
      "Episode   70 | Average R    200 | Median R 200.0\n",
      "Episode   80 | Average R  161.4 | Median R 146.0\n",
      "Episode   90 | Average R  120.1 | Median R 120.5\n",
      "Episode  100 | Average R  123.7 | Median R 120.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "agent = ActorCritic(env)          ##separate networks\n",
    "agent.run(100,fit_data='per_step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode   10 | Average R   17.3 | Median R 15.5\n",
      "Episode   20 | Average R   50.2 | Median R 41.0\n",
      "Episode   30 | Average R  145.4 | Median R 169.5\n",
      "Episode   40 | Average R  132.1 | Median R 148.5\n",
      "Episode   50 | Average R   98.7 | Median R 92.5\n",
      "Episode   60 | Average R   76.5 | Median R 73.0\n",
      "Episode   70 | Average R   89.8 | Median R 80.5\n",
      "Episode   80 | Average R   71.4 | Median R 64.5\n",
      "Episode   90 | Average R     96 | Median R 85.0\n",
      "Episode  100 | Average R   95.8 | Median R 104.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "agent = ActorCritic(env)         ##shared trunk\n",
    "agent.run(100,fit_data='per_step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode   10 | Average R  138.3 | Median R 138.5\n",
      "Episode   20 | Average R  137.4 | Median R 139.0\n",
      "Episode   30 | Average R  135.8 | Median R 137.5\n",
      "Episode   40 | Average R  137.1 | Median R 137.0\n",
      "Episode   50 | Average R  137.1 | Median R 138.0\n",
      "Episode   60 | Average R  138.1 | Median R 140.0\n",
      "Episode   70 | Average R  138.6 | Median R 137.5\n",
      "Episode   80 | Average R    139 | Median R 137.5\n",
      "Episode   90 | Average R  138.7 | Median R 138.5\n",
      "Episode  100 | Average R    135 | Median R 134.0\n"
     ]
    }
   ],
   "source": [
    "# evaluation\n",
    "agent.run(100,fit_data=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN value network on raw pixels\n",
    "https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "\n",
    "network input: `state (raw pixels)` output: `Q(a|state)`"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "        self.model = tf.keras.models.Sequential()\n",
    "        self.model.add(tf.keras.layers.Conv2D(16, kernel_size=5, strides=2, input_shape=screen_shape, activation='relu'))\n",
    "        self.model.add(tf.keras.layers.BatchNormalization())\n",
    "        self.model.add(tf.keras.layers.Conv2D(32, kernel_size=5, strides=2, input_shape=screen_shape, activation='relu'))\n",
    "        self.model.add(tf.keras.layers.BatchNormalization())\n",
    "        self.model.add(tf.keras.layers.Conv2D(32, kernel_size=5, strides=2, input_shape=screen_shape, activation='relu'))\n",
    "        self.model.add(tf.keras.layers.BatchNormalization())\n",
    "        self.model.add(tf.keras.layers.Flatten())\n",
    "#         self.model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "        self.model.add(tf.keras.layers.Dense(self.env.action_space.n, activation='linear'))\n",
    "        self.model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(.001)) #optimizer has its parameters\n",
    "        \n",
    "\n",
    "    def get_screen(self):\n",
    "        def get_cart_location(screen_width):\n",
    "            world_width = env.x_threshold * 2\n",
    "            scale = screen_width / world_width\n",
    "            return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
    "        screen = self.env.render(mode='rgb_array')\n",
    "        # Cart is in the lower half, so strip off the top and bottom of the screen\n",
    "        screen_height, screen_width, _ = screen.shape\n",
    "        screen = screen[int(screen_height*0.4):int(screen_height * 0.8)]\n",
    "        view_width = int(screen_width * 0.6)\n",
    "        cart_location = get_cart_location(screen_width)\n",
    "        # Strip off the edges, so that we have a square image centered on a cart\n",
    "        if cart_location < view_width // 2:\n",
    "            slice_range = slice(view_width)\n",
    "        elif cart_location > (screen_width - view_width // 2):\n",
    "            slice_range = slice(-view_width, None)\n",
    "        else:\n",
    "            slice_range = slice(cart_location - view_width // 2,\n",
    "                                cart_location + view_width // 2)\n",
    "        screen = screen[:, slice_range,:]\n",
    "        screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "        return screen\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "1141px",
    "left": "92px",
    "top": "111.133px",
    "width": "192.4px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
