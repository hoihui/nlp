{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.4'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "Y_train = np_utils.to_categorical(y_train, 10)# convert class vectors to binary class matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://keras.io/datasets/\">datasets</a> (all uint8):\n",
    "* mnist: X.shape is (N, 28, 28); y.shape is (N,) from 0 to 9\n",
    "* cifar10: X.shape is (N, 3, 32, 32); y.shape is (N,) from 0 to 9\n",
    "* cifar100: X.shape is (N, 3, 32, 32); y.shape is (N,) of string labels\n",
    " * label_mode: \"fine\" or \"coarse\".\n",
    "* imdb: \n",
    "* reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'img_rows' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-b1455b070056>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32melif\u001b[0m  \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_dim_ordering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tf'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mX_train_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mX_test_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'img_rows' is not defined"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "if K.image_dim_ordering() == 'th':\n",
    "    X_train_ = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n",
    "    X_test_ = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "elif  K.image_dim_ordering() == 'tf':\n",
    "    X_train_ = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "    X_test_ = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conform with Convolution2D's requirements for different backends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try: from sklearn.model_selection import train_test_split\n",
    "except: from sklearn.cross_validation import train_test_split\n",
    "X_train, X_valid, Y_train, Y_valid = train_test_split(X_train, Y_train, test_size=0.15, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in general not necessary as model.fit can specify validation ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "model.add(Dense(512, input_shape=(96,96)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://keras.io/layers/core/#dense\">Dense</a>(output_dim, input_shape, init, W_regularizer)\n",
    "* output_dim: Linear channel's length\n",
    "* input_shape: only necessary on first layer; otherwise is inferred internally\n",
    "* init: \"glorot_normal\"\n",
    "* W_regularizer=l2(0.1) [from keras.regularizers import l2]\n",
    "* activation: if specified, equiv to addling the Activation layer as the next item\n",
    "\n",
    "<a href=\"https://keras.io/layers/core/#activation\">Activation</a>(<a href=\"https://keras.io/activations/\">activation</a>)\n",
    "* 'relu', 'softplus', 'softsign', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear', 'softmax'(3dim or 2dim)\n",
    "* custom function with 1 input & 1 output\n",
    "\n",
    "<a href=\"https://keras.io/layers/convolutional/#convolution2d\">Convolution2D</a>(nb_filter, nb_row, nb_col, init='glorot_uniform', activation='linear', weights=None, border_mode='valid', subsample=(1, 1), dim_ordering='default', W_regularizer=None, b_regularizer=None, activity_regularizer=None, W_constraint=None, b_constraint=None, bias=True)\n",
    "* Input shape: 4D tensor with shape: (samples, channels, rows, cols) if dim_ordering='th' or 4D tensor with shape: (samples, rows, cols, channels) if dim_ordering='tf'.\n",
    "\n",
    "MaxPooling2D(pool_size=pool_size)\n",
    "\n",
    "merge([tower_1, tower_2, tower_3], mode='concat', concat_axis=1)\n",
    "\n",
    "Flatten()\n",
    "\n",
    "<a href=\"https://keras.io/layers/wrappers/#timedistributed\">TimeDistributed</a>(Dense(10,input_shape))\n",
    "* input must be at least 3D (batchSize x timeSteps x DenseInput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functional API ~nngraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://keras.io/getting-started/functional-api-guide/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, TimeDistributed\n",
    "from keras.layers import Dense, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = Input(shape=(100,), dtype='int32')#Sequential\n",
    "x = Dense(64, activation='relu')(inputs)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "predictions = Dense(10, activation='softmax')(x)\n",
    "model = Model(input=inputs, output=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = Input(shape=(dim1, dim2, dim3))\n",
    "a = TimeDistributed(LSTM(output_dim=10))(x) #apply \"LSTM\" model to each of dim1's elements\n",
    "b = LSTM(col_hidden)(a)\n",
    "prediction = Dense(nb_classes, activation='softmax')(b)\n",
    "model = Model(input=x, output=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Model(input=[main_input, auxiliary_input], output=[main_output, auxiliary_output]) #multi-input/outputs\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', loss_weights=[1., 0.2])\n",
    "model.fit([headline_data, additional_data], [labels, labels],nb_epoch=50, batch_size=32)\n",
    "# if names are provided for the inputs/outputs:\n",
    "model.compile(optimizer='rmsprop',loss={'main_output': 'binary_crossentropy', 'aux_output': 'binary_crossentropy'},\n",
    "                          loss_weights={'main_output': 1., 'aux_output': 0.2})\n",
    "model.fit({'main_input': headline_data, 'aux_input': additional_data},\n",
    "          {'main_output': labels, 'aux_output': labels},\n",
    "          nb_epoch=50, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Model Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.summary()  #human-readable\n",
    "model.output_shape # shape of output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "model.save_weights(\"model.h5\", overwrite=True)\n",
    "with open(\"model.json\", \"w\") as outfile:  #uncompiled\n",
    "    json.dump(model.to_json(), outfile)\n",
    "with open(\"model.json\", \"r\") as jfile:\n",
    "    model = model_from_json(json.load(jfile))\n",
    "model.load_weights(\"model.h5\")\n",
    "model.compile(\"sgd\", \"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Dense(64, activation='relu', name=\"dense_one\"))\n",
    "model.get_layer(\"dense_one\")  #get layer by name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Layer Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer = model.layers[0]  #extract the first layer from the list of model's layers\n",
    "layer.get_weights()[0] # For \"Dense\": 0 for weights, 1 for bias\n",
    "layer.set_weights(weights) # sets the weights of the layer from a list of Numpy arrays \n",
    "layer.get_config()# returns a dictionary containing the configuration of the layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a layer has a single node (i.e. if it isn't a shared layer), you can get its input tensor, output tensor, input shape and output shape via:\n",
    "\n",
    "    layer.input\n",
    "    layer.output\n",
    "    layer.input_shape\n",
    "    layer.output_shape\n",
    "\n",
    "If the layer has multiple nodes (see: the concept of layer node and shared layers), you can use the following methods:\n",
    "\n",
    "    layer.get_input_at(node_index)\n",
    "    layer.get_output_at(node_index)\n",
    "    layer.get_input_shape_at(node_index)\n",
    "    layer.get_output_shape_at(node_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD, Adam, RMSprop, ...\n",
    "model.compile(loss='categorical_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://keras.io/models/model/\">Parameters</a>:\n",
    "* loss: 'categorical_crossentropy' [target y is of shape (N,nclass), i.e. one-hot encoding], 'mse' (mean_squared_error), 'mae', 'mape', 'msle', 'sparse_categorical_crossentropy' (accept sparse labels), squared_hinge, hinge, binary_crossentropy, kld (kullback_leibler_divergence), 'poisson' [Mean of (predictions - targets * log(predictions))], 'cosine_proximity'\n",
    "* <a href=\"https://keras.io/optimizers/\">optimizer</a>: common parameters are clipnorm (max allowed gradient 2-norm) and clipvalue (max allowed gradient 1-norm)\n",
    " * 'sgd' or SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
    " * 'adagrad' or Adagrad(lr=0.01, epsilon=1e-08, decay=0.0)\n",
    " * 'rmsprop' or RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    " * 'adadelta' or Adadelta(lr=1.0, rho=0.95, epsilon=1e-08, decay=0.0)\n",
    " * 'adam' or Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    " * 'adamax' or Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    " * 'nadam' or Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004)\n",
    "* metrics: list of metrics to be evaluated by the model during training and testing\n",
    " * 'accuracy'\n",
    "* sample_weight_mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(x,y,batch_size=32, nb_epoch=10,verbose=1,callbacks=[], validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://keras.io/models/model/\">Parameters</a>:\n",
    "\n",
    "* x: Numpy array of training data, or list of Numpy arrays if the model has multiple inputs. If all inputs in the model are named, you can also pass a dictionary mapping input names to Numpy arrays.\n",
    "* y: Numpy array of target data, or list of Numpy arrays if the model has multiple outputs. If all outputs in the model are named, you can also pass a dictionary mapping output names to Numpy arrays.\n",
    "* batch_size: integer. Number of samples per gradient update.\n",
    "* nb_epoch: integer, the number of times to iterate over the training data arrays.\n",
    "* verbose: 0, 1, or 2. Verbosity mode. 0 = silent, 1 = verbose, 2 = one log line per epoch.\n",
    "* validation_split: float between 0 and 1: fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch.\n",
    "* validation_data: data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. This could be a tuple (x_val, y_val) or a tuple (val_x, val_y, val_sample_weights).\n",
    "* shuffle: boolean, whether to shuffle the training data before each epoch.\n",
    "* class_weight: optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class.\n",
    "* sample_weight: optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. In this case you should make sure to specify sample_weight_mode=\"temporal\" in compile().\n",
    "* callbacks: list of callbacks to be called during training. See callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# General model() class\n",
    "evaluate(self, x, y, batch_size=32, verbose=1, sample_weight=None)\n",
    "predict(self, x, batch_size=32, verbose=0)\n",
    "train_on_batch(self, x, y, sample_weight=None, class_weight=None)\n",
    "test_on_batch(self, x, y, sample_weight=None)\n",
    "predict_on_batch(self, x)\n",
    "fit_generator(self, generator, samples_per_epoch, nb_epoch, verbose=1, callbacks=[], validation_data=None, nb_val_samples=None, class_weight={}, max_q_size=10, nb_worker=1, pickle_safe=False)\n",
    "evaluate_generator(self, generator, val_samples, max_q_size=10, nb_worker=1, pickle_safe=False)\n",
    "predict_generator(self, generator, val_samples, max_q_size=10, nb_worker=1, pickle_safe=False)\n",
    "get_layer(self, name=None, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://keras.io/models/model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Sequential() only\n",
    "predict_classes(self, x, batch_size=32, verbose=1)\n",
    "predict_proba(self, x, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://keras.io/models/sequential/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_hat = model.predict_classes(x) #Sequential() only\n",
    "pandas.crosstab(y_hat, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Visualize wrong data\n",
    "test_wrong = [im for im in zip(X_test_,y_hat,y_test) if im[1] != im[2]]\n",
    "for ind, val in enumerate(test_wrong[:100]):\n",
    "    plt.subplot(10, 10, ind + 1)\n",
    "    im = 1 - val[0].reshape((img_rows, img_cols,))\n",
    "    axis(\"off\")\n",
    "    plt.imshow(im, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#visualize trained weights\n",
    "W1 = model.layers[0].get_weights()[0] # 0 for weights, 1 for bias\n",
    "print(\"weight matrix transposed:\",W1.T.shape)\n",
    "plt.figure(figsize=(5, 5))\n",
    "for i in range(100):\n",
    "    plt.subplot(10, 10, i + 1)\n",
    "    im = W1.T[i].reshape((img_rows, img_cols,))\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(im, cmap='seismic')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
